{
  "id": "68f812a8c22606ecfa5c727e",
  "title": "Python Essentials V1",
  "description": "No description available",
  "learningObjectives": [
    "No learning objectives"
  ],
  "finalProjectDetails": {
    "overview": "No final project specified.",
    "planningResources": [],
    "requirements": [
      "No requirements specified."
    ],
    "submissionInstructions": "No submission instructions provided."
  },
  "createdBy": {
    "name": "Hanna Akhramchuk",
    "githubUsername": "akhhanna20"
  },
  "contributors": [],
  "status": "pending",
  "originalGithubUrl": "https://github.com/Code-the-Dream-School/python-essentials-v1",
  "remoteGithubUrl": "https://github.com/akhhanna20/python-essentials-v1",
  "createdAt": "2025-10-21T23:09:28.697Z",
  "updatedAt": "2025-10-21T23:09:34.561Z",
  "lessonCount": 16,
  "lessonMetadata": [
    {
      "id": "68f812adc22606ecfa5c7298",
      "lessonNumber": 1,
      "title": "Lesson 1 ‚Äî Introduction to Python",
      "status": "pending",
      "assignment": {
        "title": "**Lesson 1 Assignment: Intro to Python**",
        "objective": "### Python Operators, Control Flow, and Calculator Implementation\n\n## **Objective and Overview**  \nIn this assignment, you will practice key Python concepts including:    \n‚úÖ Declaring functions  \n‚úÖ Formatted Strings  \n‚úÖ Type Conversion  \n‚úÖ Error Handling  \n‚úÖ For Loops with Ranges  \n‚úÖ Use of *args and **kwargs in Function Declarations  \n‚úÖ String Manipulation   \n\n### üéôÔ∏è Podcast: Lesson 1\n\nWe‚Äôre excited to introduce a new optional resource to support your learning‚Äî**short, mentor-style podcast episodes that break down key assignments and concepts in a friendly, approachable way.**\n\nIf you ever feel stuck, want a quick refresher before diving in, or just prefer listening over reading, these episodes are here to help! They‚Äôll cover **how to approach your assignment, common tricky spots, and why these skills matter‚Äîall in a quick, easy-to-digest format**.\n\n**These podcasts are generated by [NotebookLM](https://notebooklm.google/)** ‚Äî a new tool we're still exploring. Let us know if these are helpful, and happy coding!\n\n**[Listen to the assignment overview podcast here](https://youtu.be/azM0-ybstt4).**\n\nSince the podcast was created, the assignment itself has been updated to make it more comprehensive, so the podcast doesn't match up. You could check out the format/style of the podcast to see if it is the sort of thing that would help you.\n\n## **Instructions**\n\n### **Setup**\n\nYour homework for this and some future assignments will use a special python homework repository, so that it can be submitted using git.  That repository is [here.](https://github.com/Code-the-Dream-School/python_homework)  Click on the link, and carefully **follow the instructions in the README**.  You will then have your own copy of the repository, connected to your GitHub account.  You create the program files for the homework in the root of that repository.  For each assignment, you create a separate git branch (e.g. `git checkout -b lesson1`).  When you have completed your assignment, you add and commit your changes in that branch, push it to your GitHub, and create a pull request for that branch from your lesson branch (as the compare) to main branch (as the base).  You provide a link to that pull request when you submit your homework.  Your instructor will review it, approving or requesting changes.  Once your instructor has approved the pull request, you can merge it so that your lesson work gets updated to the main branch.  The reason for this workflow is to mimic what's done in the industry of making features and fixes on separate branches before testing/checking it.  Once the code is tested and checked by a supervisor/review process, the code gets merged to a production branch (in this case your main branch) so the users will see updates.\n\n### **Step 1: Complete the Coding Tasks**  \n\nAt the outset, you create python `.py` files.  In some future lessons, you will create Jupyter notebooks. Homework for this assignment is created within your `python_homework` folder.  Be sure to create an `assignment1` git branch.  Then, write Python code to complete the following tasks.  As you do, put in **comment lines to mark your code for Task 1, Task 2, and so on.** That will help your reviewer.\n\nThis assignment uses a Python tool for automated testing, called PyTest.  You installed it when you set up the python_homework folder.  You'll code a series of functions.  PyTest will validate whether your code is correct.  To run the test:  \n\n- Change to the python_homework/assignment1 folder.\n- Type `code .` to bring up VSCode for this folder. \n- Start a git bash terminal session within VSCode and enter the command (you can make git bash the default using ctl-shift-p and setting the `Terminal: Select Default Profile`)\n\n```bash\npytest -v -x assignment1-test.py # can use just -x, adding -v lists the passing tests\n```\n\nPyTest attempts to perform the tests, and as you haven't written the code yet, it quickly ends with an error -- in this case, because you haven't created the hello function.  You follow this pattern: \n\n- Add your code to the `./assignment1/assignment1.py file` which is provided (but it's empty.)\n- Write each function according to the instructions below.\n- Put a print() statement in the mainline of your code (not inside the function), and call the function from within the print() statement to see if it works.  You can run the file by typing `python assignment1.py`.\n- Then run the test using the command above.  \n- If it reports errors, change your code to fix them.  \n\nOnce the first test passes, you'll get an error for the second test, so you go on to write the second function, and so on until all 10 tests complete successfully.  You should have a look at assignment1-test.py to see how it works and what it tests for.  Keep going until all tests pass.\n\nThere are a number of new Python capabilities introduced during this assignment, beyond what is in the lesson, but they are explained below, so that you can learn and practice at the same time.\n\n**Help is Available**\n\nWe have covered quite a bit.  If you get stuck, 1:1 mentors are available to answer your questions.  Appointments are available in the [1:1 Mentor Table](https://airtable.com/appoSRJMlXH9KvE6w/shrQinGb1phZYwdiL)\n\n---",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [
          {
            "taskNumber": 1,
            "title": "Hello",
            "description": "Write a hello function that takes no arguments and returns `Hello!`.  Now, what matters here is what the function *returns*.  You can print() whatever you want for debugging purposes, but the tests ignore that, and only check the return value.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72a6"
          },
          {
            "taskNumber": 2,
            "title": "Greet with a Formatted String",
            "description": "Write a greet function.  It takes one argument, a name, and returns `Hello, Name!`.  Use a formatted string.  Note that you have to return exactly the right string or the test fails -- but PyTest tells you what didn't match.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72a7"
          },
          {
            "taskNumber": 3,
            "title": "Calculator**",
            "description": "- Write a calc function.  It takes three arguments.  The default value for the third argument is \"multiply\".  The first two arguments are values that are to be combined using the operation requested by the third argument, a string that is one of the following add, subtract, multiply, divide, modulo, int_divide (for integer division) and power.  The function returns the result.\n- Error handling: When the function is called, it could ask you to divide by 0. That will throw an exception: Which one?  You can find out by triggering the exception in your program or in the Python Interactive Shell.  Wrap the code within the calc function in a try block, and put in an except statement for this exception.  If the exception occurs, return the string \"You can't divide by 0!\".  \n- More error handling: When the function is called, the parameters that are passed might not work for the operation.  For example, you can't multiply two strings.  Find out which exception occurs, catch it, and return the string \"You can't multiply those values!\".\n- Here's a tip.  You have to do different things for add, multiply, divide and so on.  So you can do a conditional cascade, if/elif/elif/else.  That's perfectly valid.  But you might want to use the match-case Python statement instead.  Look it up!  It just improves code appearance.\n\nAgain, as you complete each function, you run the test to see whether everything is correct.\n\n---",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72a8"
          },
          {
            "taskNumber": 4,
            "title": "Data Type Conversion**",
            "description": "- Create a function called data_type_conversion.  It takes two parameters, the value and the name of the data type requested, one of float, str, or int.  Return the converted value.\n- Error handling: The function might be called with a bad parameter.  For example, the caller might try to convert the string \"nonsense\" to a float.  Catch the error that occurs in this case.  If this error occurs, return the string `You can't convert {value} into a {type}.`, except you use the value and data type that are passed as parameters -- so again you use a formatted string.\n\n---",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72a9"
          },
          {
            "taskNumber": 5,
            "title": "Grading System, Using `*args`**",
            "description": "- Create a grade function.  It should collect an arbitrary number of parameters, compute the average, and return the grade.\nbased on the following scale:  \n   - A: 90 and above  \n   - B: 80-89  \n   - C: 70-79  \n   - D: 60-69  \n   - F: Below 60  \n- When you use `*args` you get access to a variable named `args` in your function, which is a tuple, an ordered collection of values like a list.  You'll learn more about tuples and lists in the next lesson.  There are some helpful functions you can use at this point: `sum(args)`, `len(args)`, and so on.  One of the curiosities of Python is that these are not methods of any class.  They are just standalone functions.\n- Handle the error that occurs if the parameters are nonsense.  Return the string \"Invalid data was provided.\" in this case.  (Typically, you don't handle every possible exception in your error handling, except if the values in the parameters comes from the end user.)\n\n---",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72aa"
          },
          {
            "taskNumber": 6,
            "title": "Use a For Loop with a Range**",
            "description": "- Create a function called repeat.  It takes two parameters, a string and a count, and returns a new string that is the old one repeated count times.\n- You can get the test to pass by just returning `string * count`.  That would produce the correct return value.  But, for this task, do it using a for loop and a range.\n\n---",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72ab"
          },
          {
            "taskNumber": 7,
            "title": "Student Scores, Using `**kwargs`**",
            "description": "- Create a function called student_scores.  It takes one positional parameter and an arbitrary number of keyword parameters.  The positional parameter is either \"best\" or \"mean\".  If it is \"best\", the name of the student with the higest score is returned.  If it is \"mean\", the average score is returned.\n- As you are using `**kwargs`, your function can access a variable named `kwargs`, which is a dict.  The next lesson explains about dicts.  What you need to know now is the following:\n   - A dict is a collection of key value pairs.\n   - You can iterate through the dict as follows:\n   ```python\n   for key, value in kwargs.items():\n   ```\n   - You can also get `kwargs.keys()` and `kwargs.values()`.\n- The arbitrary list of keyword arguments uses the names of students as the keywords and their test score as the value for each.\n\n---",
            "codeExample": "```python\n   for key, value in kwargs.items():\n   ```",
            "_id": "68f812adc22606ecfa5c72ac"
          },
          {
            "taskNumber": 8,
            "title": "Titleize, with String and List Operations",
            "description": "- Create a function called titleize.  It accepts one parameter, a string.  The function returns a new string, where the parameter string is capitalized as if it were a book title.\n- The rules for title capitalization are: (1) The first word is always capitalized. (2) The last word is always capitalized. (3) All the other words are capitalized, except little words.  For the purposes of this task, the little words are \"a\", \"on\", \"an\", \"the\", \"of\", \"and\", \"is\", and \"in\".\n- The following string methods may be helpful: split(), join(), and capitalize().  Look 'em up.\n- The split() method returns a list. You might store this in the `words` variable.  `words[-1]` gives the last element in the list.\n- The `in` comparison operator: You have seen `in` used in loops.  But it can also be used for comparisons, for example to check to see if a substring occurs in a string, or a value occurs in a list.\n- A new trick: As you loop through the words in the `words` list, it is helpful to have the index of the word for each iteration.  You can access that index using the enumerate() function:\n```python\nfor i, word in enumerate(words):\n```\n\n---",
            "codeExample": "```python\nfor i, word in enumerate(words):\n```",
            "_id": "68f812adc22606ecfa5c72ad"
          },
          {
            "taskNumber": 9,
            "title": "Hangman, with more String Operations",
            "description": "- Create a function hangman.  It takes two parameters, both strings, the secret and the guess.\n- The secret is some word that the caller doesn't know.  So the caller guesses various letters, which are the ones in the guess string.\n- A string is returned.  Each letter in the returned string corresponds to a letter in the secret, except any letters that are not in the guess string are replaced with an underscore.  The others are returned in place.  Not everyone has played this kid's game, but it's common in the US.\n- Example: Suppose the secret is \"alphabet\" and the guess is \"ab\".  The returned string would be \"a___ab__\".\n- Note that Python strings are immutable.  That means that the following code would give an error:\n```python\nsecret = \"alphabet\"\nsecret[1] = \"_\"\n```\n- On the other hand, you can concatenate strings with the `+` operator.\n\n---",
            "codeExample": "```python\nsecret = \"alphabet\"\nsecret[1] = \"_\"\n```",
            "_id": "68f812adc22606ecfa5c72ae"
          },
          {
            "taskNumber": 10,
            "title": "Pig Latin, Another String Manipulation Exercise",
            "description": "- Pig Latin is a kid's trick language.  Each word is modified according to the following rules.  (1) If the string starts with a vowel (aeiou), \"ay\" is tacked onto the end. (2) If the string starts with one or several consonants, they are moved to the end and \"ay\" is tacked on after them. (3) \"qu\" is a special case, as both of them get moved to the end of the word, as if they were one consonant letter.\n- Create a function called pig_latin.  It takes an English string or sentence and converts it to Pig Latin, returning the result.  We will assume that there is no punctuation and that everything is lower case.\n\n### **Step 2: Submit Your Assignment on GitHub**  \n\n**Follow these steps to submit your work:**  \n\n#### **1Ô∏è‚É£ Add, Commit, and Push Your Changes**  \n- Within your python_homework folder, do a git add and a git commit for the files you have created, so that they are added to the `assignment1` branch.\n- Push that branch to GitHub. \n\n#### **2Ô∏è‚É£ Create a Pull Request**  \n- Log on to your GitHub account.\n- Open your `python_homework` repository.\n- Select your `assignment1` branch.  It should be one or several commits ahead of your main branch.\n- Create a pull request.\n\n#### **3Ô∏è‚É£ Submit Your GitHub Link**  \n- Your browser now has the link to your pull request.  Copy that link. \n- Paste the URL into the **assignment submission form**.  \n\n## **üéâ Well Done!**  \nGreat job completing Lesson 1! You're building a strong foundation in Python. üöÄ",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72af"
          }
        ],
        "submissionInstructions": "Please submit on time",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Introduction",
          "content": "# Lesson 1 ‚Äî Introduction to Python\n\nWelcome to **Python Essentials** with Code the Dream!",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7299"
        },
        {
          "subsectionOrder": 2,
          "title": "üëÄ How to Follow This Content",
          "content": "* Start by reading the lesson's **learning objective** in the `Lesson Overview` section. Each weekly assignment will measure your skill related to the learning objective.\n* Lessons are split into **subsections**, labeled like this: `1.1`, `1.2`, etc.\n* Several subsections also have a short **supplemental video** that will help you understand the content in that subsection.\n* At the end of each subsection, you'll find a multiple-choice **\"Check for Understanding\"** question. Complete the question and review the material if your answer is not correct!\n* After reading through the lesson content and correctly answering the \"Check for Understanding\" questions, complete the **Weekly Assignment**.\n\nIf you have questions at any point, ask a question in the `discussion` Slack channel or reach out to your mentor!",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c729a"
        },
        {
          "subsectionOrder": 3,
          "title": "Lesson Overview",
          "content": "**Learning objective:** Students will learn the basics of Python programming, including variables, data types, operators, control flow statements, and functions. They will also practice debugging their code.\n\nTopics:\n\n- **Setting Up Your Python Environment**:  \n  Installing Python, pip, and your IDE. We recommend the VS Code IDE. It's ok to use another IDE if you are comfortable with it. Verifying installations and creating `.py` files.\n- **Python Basics**:  \n  Variables, data types (integers, floats, strings, booleans), data conversion (explicit and implicit), and operators (arithmetic, comparison, logical).\n- **Block Structure and Indentation**:  \n  Understanding Python‚Äôs indentation-based syntax for defining blocks like functions, loops, and conditionals.\n- **Control Flow**:  \n  Conditional statements (`if`, `elif`, `else`), loops (`for`, `while`), and controlling loops with `break` and `continue`.\n- **Functions**:  \n  Defining and calling functions, parameters, return values, and handling dynamic arguments with `*args` and `**kwargs`.\n- **Debugging**:  \n  - **Error Handling**: Introduction to `try`, `except` for handling runtime errors.  \n  - **Basic Debugging**: Using print statements and the `logging` module to debug code effectively.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c729b"
        },
        {
          "subsectionOrder": 4,
          "title": "Setting up your environment",
          "content": "### Install Python\n\nYou can download Python from the official website: [python.org](https://www.python.org/downloads/).\n\nFollow the installation instructions for your operating system:\n\n1. **Install Python:**\n   Follow the installation instructions for your operating system.  For this class, we require Python 3.  The previous version (Python 2) is significantly different and is deprecated.\n\n   - **For Windows**\n       - If you are using Windows, it is common to use the Windows Subsystem for Linux for development. WSL is not recommended for this class.** Later lessons use `matplotlib for graphs`. It is difficult to do the configuration needed to get graphs to show in the WSL environment. Windows users should install in Windows native.\n       - Follow the instructions on the website. Make sure to check the option to **Add Python to PATH** during installation.\n       - Windows users should have Git for Windows installed, and should use Git Bash for all subsequent steps.  If python hangs when you run it in a git bash window, add the following line to ~/.bash_profile: `alias python='winpty python.exe'`\n\n   - **For macOS**:\n      - macOS typically comes with Python pre-installed. To ensure you are using Python 3, download the latest version of Python from [python.org](https://www.python.org/downloads/).\n      - Follow the installation instructions. You can also use **Homebrew** to install Python by running the following command in the terminal:\n          ```bash\n          brew install python\n          ```\n    - **For Linux**:\n       - Most Linux distributions come with Python pre-installed, but you need to have Python 3 for the class. To install or upgrade Python 3, you can use the package manager:\n       - For **Debian/Ubuntu** systems:\n          ```bash\n          sudo apt update\n          sudo apt install python3\n          ```\n        - For **Fedora**:\n          ```bash\n          sudo dnf install python3\n          ```\n        - For **Arch Linux**:\n          ```bash\n          sudo pacman -S python\n          ```\n\n\n2. **Verify Python Installation**:\n    After installation, you can verify that Python is installed correctly, and that you have Python 3, by opening a terminal or command prompt and running:\n   \n    ```bash\n    python --version\n    ```\n    Some systems install Python as python3 to differentiate it from a previous installation of Python 2, which is deprecated.:\n    ```bash\n    python3 --version\n    ```\n    This should display the installed version of Python. For example, you might see:\n    ```\n    Python 3.9.7\n    ```\n\n    At this point, you can try the Python Interactive Shell.  You type `python` or `python3` without arguments.  This brings up a command line into which you can type code, which is then executed as each statement is completed.  The code is not preserved, of course, but this is a good way to try many of the ideas below.  Ctrl-D exits the shell.\n\n3. **Install pip**\n\n    **Pip** is Python‚Äôs package installer, and it is included automatically with Python versions 3.4 and above. It allows you to easily install and manage Python libraries and packages from the Python Package Index (PyPI).\n\n    #### Verify if pip is installed:\n    To check if **pip** is installed, open a terminal or command prompt and type:\n\n    ```bash\n    pip --version\n    ```\n\n    Systems which install Python as `python3` will install pip as `pip3` instead:\n\n    ```bash\n    pip3 --version\n    ```\n\n    This should display the installed version of pip. If pip is not installed or you encounter an error, you may need to reinstall Python and ensure that the box for **Add Python to PATH** is checked.\n\n    #### Upgrading pip:\n    If you already have pip installed but want to make sure it‚Äôs up to date, run the following command:\n\n    ```bash\n    python -m pip install --upgrade pip\n    ```\n\n    Or, for Python 3:\n\n    ```bash\n    python3 -m pip install --upgrade pip\n    ```\n\n4. **Create a Working Folder**\n    - Your assignments will use a git repository, and the instructions for setting up that repository are included in the first lesson.  You should also have a separate folder to try the code samples from the lessons.  This working folder should be outside of the cloned repository on your computer.  For example, you could create a folder called `python_class`.  Inside that folder, create a folder called `working` to use for lesson code samples.  When you do the first assignment, you will also clone a repository called `python_homework` inside the `python_class` folder.  Your lessons and assignments will require some packages to be added to Python using pip.  These should be installed into a virtual environment -- a collection of packages specifically for your project.  (The JavaScript and Rails package managers set up a virtual environment automatically, but it requires several additional steps for Python.) \n    \n    Create a folder, cd to that folder, and then do the following:\n    - Install the virtualenv package: `pip install virtualenv` (or perhaps `pip3 install virtualenv`).\n    - Then, create the virtual environment.\n        - Windows users enter the following commands:\n            ```bash\n            python -m venv .venv\n            source .venv/Scripts/activate\n            code .\n            ```\n        - Mac and Linux users enter the following commands:\n            ```bash\n            python3 -m venv .venv\n            source .venv/bin/activate\n            code .\n            ```\n    Once your virtual environment is activated, you see `.venv` as part of your terminal prompt.  Be sure that is present for all subsequent work.  When you create a new terminal session, you have to activate the virtual environment again.  When the virtual environment is active, you can always use the commands `python` and `pip`, that is, you don't need `python3` or `pip3`.\n\n5. **Set Up VSCode for Python**\n    - Some developers may choose an alternate editor, such as PyCharm, but VSCode works well, and the instructions below describe what you need to do.\n    - Be sure to install the Python extension for VSCode.\n    - Windows developers: You should add the following lines to your `~/.bashrc` file (creating the file if it does not exist):\n    ```bash\n    if [ -f ./.venv/Scripts/activate ]; then\n        source ./.venv/Scripts/activate\n    fi\n    ```\n    - In the VSCode command palette (Ctrl-Shift-P) go to `Python: Select Interpreter` and choose the one that has `.venv` in it.\n    - When you open a VSCode terminal, you should see a `(.venv)` as part of the prompt.  This is how you know that the virtual environment is active.  You want it to be active before installing packages such as pandas or numpy.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c729c"
        },
        {
          "subsectionOrder": 5,
          "title": "Python Basics",
          "content": "### If you are a JavaScript Developer\n\nYou have a head start.  All the basic structures of programming (loops, conditional statements, variables, and so on) are in Python.  But, Python syntax is different.  You'll have to adjust to the differences.  You may want to read [this summary](https://www.freecodecamp.org/news/learn-python-for-javascript-developers-handbook/). (This is optional.)\n\n### A Cheat Sheet\n\nHere is [a one page summary of the Python syntax](https://quickref.me/python.html).  You may want to have it on hand.\n\n### Variables in Python\n\nA **variable** is like a labeled box where you store data. In Python, variables don‚Äôt need explicit declaration before assignment, and the type of data they hold can change dynamically.\n\nIn the example below,\n\n* `name` is assigned a **string** `\"Jazmine\"`\n* `age` is assigned an **integer** `28`\n* `height` is assigned a **float** `5.8`\n\n```python\nname = \"Jazmine\"   # A variable storing a string\nage = 28           # A variable storing an integer\nheight = 5.8       # A variable storing a float (decimal)\n```\n\nHere are some general rules for Python variable naming.\n- Lowercase: Use lowercase letters for variable names.\n- Underscores: Separate words in variable names with underscores (_).\n- Descriptive: Choose meaningful names that clearly indicate the variable's purpose.\n- Avoid single-character names: Except for simple loop counters (e.g., i, j, k).\n\n### Data Types in Python\n\n**Data types** specify the kind of data is stored in a variable. Common data types include:\n\n* Integer (`int`): Whole numbers (e.g., 42, -3)\n* Float (`float`): Decimal numbers (e.g., 3.14, -0.5)\n* String (`str`): Text (e.g., \"hello\", \"world\")\n* Boolean (`bool`): Represents True or False values\n\n```python\nis_student = True       # Boolean\nbalance = 1000.75       # Float\nfirst_name = \"Charlie\"  # String\nnumber_of_days = 7      # Integer\n```\n\nYou can check the type of a variable using the `type()` function:\n\n```python\nprint(type(balance))  # Output: <class 'float'>\n```\n\n### Data Conversion\n\nData conversion, or **type casting**, is the process of converting one data type to another. Python provides several built-in functions to make this easy. Common conversion functions include `int()`, `float()`, `str()`, and `bool()`. Notice that these conversion functions line up with the data types demonstrated in the previous section.\n\n```python\n# Convert a string to an integer\nnum_str = \"42\"\nnum_int = int(num_str)  # 42 (integer)\n\n# Convert an integer to a float\nnum_float = float(num_int)  # 42.0 (float)\n\n# Convert a number to a string\nnum_str_again = str(num_int)  # \"42\" (string)\n\n# Convert to a Boolean\nis_empty = not bool(\"\")  # True \nis_non_zero = bool(5)  # True (non-zero numbers are considered True)\n```\n\n#### Why Convert Data Types?\n\nData conversion is helpful when you need to perform operations between incompatible types or display values in specific formats. For instance, combining a number with text requires converting the number to a string.\n\n```python\n# Without type conversion\nage = 30\nmessage = \"I am \" + age + \" years old.\"  # \"TypeError: can only concatenate str (not \"int\") to str\"\n```\n\n```python\n# With type conversion\nage = 30\nmessage = \"I am \" + str(age) + \" years old.\"  # \"I am 30 years old.\"\n```\n\n#### Implicit vs. Explicit Conversion\n\nPython sometimes performs **implicit conversion** (automatic type conversion), such as when adding an integer and a float, the result is automatically a float. However, for more control, it‚Äôs usually better to use **explicit conversion** with the functions above.\n\n```python\n# Implicit conversion\nresult = 3 + 2.5  # 5.5 (float, because Python converts the integer to float)\n\n# Explicit conversion\nresult = int(2.8) + 3  # 5 (integer, because we explicitly converted the float to int)\n```\n\n### üé¨ Video 1.2: Data Types and Conversion\n\nLearn how to work with data types in Python in our first video, which covers essential type conversions with `int()`, `float()`, `str()`, and `bool()`, practical examples of when to use them, and tips to avoid common pitfalls.  In general, you are not required to view the videos for this class, as the lesson text covers the same information, but the videos may help you learn and remember.\n\n**[Watch the video here.](https://youtu.be/v5NBGGHKJtI)**",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c729d"
        },
        {
          "subsectionOrder": 6,
          "title": "Operators in Python",
          "content": "**Operators** are special symbols that perform operations on variables and values. Some of the most commonly used operators are:\n\n1. **Arithmetic Operators**: For mathematical calculations\n   * `+` (addition): `3 + 2` ‚Üí `5`\n   * `-` (subtraction): `5 - 3` ‚Üí `2`\n   * `*` (multiplication): `4 * 2` ‚Üí `8`\n   * `/` (division): `9 / 3` ‚Üí `3.0`\n   * `//` (integer division): `9 // 3` ‚Üí `3`\n   * `%` (modulus, remainder): `7 % 3` ‚Üí `1`\n   * `**` (exponentiation): `2 ** 3` ‚Üí 8\n\n2. **Comparison Operators**: Compare two values and return a Boolean (`True` or `False`)\n   * `==` (equal to): `5 == 5` ‚Üí `True`\n   * `!=` (not equal to): `5 != 4` ‚Üí `True`\n   * `<` (less than): `3 < 4` ‚Üí `True`\n   * `>` (greater than): `10 > 5` ‚Üí `True`\n   * `<=` (less than or equal to): `5 <= 5` ‚Üí `True`\n   * `>=` (greater than or equal to): `7 >= 3` ‚Üí `True`\n\n3. **Logical Operators**: Used to combine conditional statements\n   * `and`: `True and False` ‚Üí `False`\n   * `or`: `True or False` ‚Üí `True`\n   * `not`: `not True` ‚Üí `False`\n\nOperators Examples:\n\n```python\n# Arithmetic\nresult = 10 + 5  # 15\nremainder = 9 % 4  # 1\n\n# Comparison\nprint(5 > 3)  # True\n\n# Logical\nprint(True and False)  # False\n```\n### Check for Understanding\n\n**Question:** What type of data is stored in the variable `age` in the following code?\n\n```python\nage = 28\n```\n\n* A) String\n* B) Integer\n* C) Float\n* D) Boolean\n  \n<details>\n\n<summary>View answer</summary>\n\n**Answer**: B) Integer\n\n</details>\n\n**Question:** Which of the following data types would you use to store the value `\"Hello, World!\"`?\n\n* A) Integer\n* B) Float\n* C) String\n* D) Boolean\n\n<details>\n\n<summary>View answer</summary>\n\n**Answer**: C) String\n\n</details>\n\n**Question**: What will be the output of the following code?\n\n```python\nnum_str = \"42\"\nnum_int = int(num_str)\nprint(num_int)\n```\n\n* A) `\"42\"`\n* B) `42`\n* C) `<class 'str'>`\n* D) An error message\n\n<details>\n\n<summary>View answer</summary>\n\n**Answer**: B) `42`\n\n</details>\n\n**Question**: What will the following code output?\n\n``python\nprint(10 % 3)\n``\n\n* A) `3`\n* B) `1`\n* C) `10`\n* D) `0`\n\n<details>\n\n<summary>View answer</summary>\n\n**Answer:** B) `1`\n\n</details>",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c729e"
        },
        {
          "subsectionOrder": 7,
          "title": "Block Structure and Indentations",
          "content": "In Python, indentation plays a crucial role in the syntax of the language. Unlike many other programming languages, which use braces {} or other markers to denote code blocks, Python uses indentation to group statements and define the scope of loops, functions, classes, and conditional statements.\n\n**Why Indentation Matters in Python**\n\n* **Defining Code Blocks:** Indentation tells Python where a block of code begins and ends.\n* **Enforcing Readability:** The clean and readable structure makes Python code easier to follow.\n\n**Key Concepts:**\n\n* **Indentation in Control Structures:** \n    * All code under control structures (such as `if`, `else`, `for`, `while`, and function definitions) must be indented.  You always put a colon `:` before starting an indented block on the next line.\n\n* **Consistent Indentation:**\n    * Consistency is key. Python does not allow mixing tabs and spaces. Use either spaces or tabs but never both. \n    * The Python community‚Äôs standard is to use 4 spaces per indentation level.\n\n**Block Structure Example:**\n\n```python\ndef check_number(num):\n    if num > 0:\n        print(\"Positive number\")\n    elif num < 0:\n        print(\"Negative number\")\n    else:\n        print(\"Zero\")\n```\nIn the above example:\n\nThe function check_number defines the first level of indentation.\nThe if, elif, and else blocks define additional indentation levels for the code that falls under each condition.\nIndentation Error Example:\n\n\n```python\ndef check_number(num):if num > 0:  # This will raise an error because it's not indented properly\n    print(\"Positive number\") \n```\nIn the above case, Python will raise an error stating: IndentationError: expected an indented block.\n\nUsing Indentation with Loops:\n\n```python\nfor i in range(3):\n    print(\"Loop iteration:\", i)  # This line is inside the for loop\n```\nAny line that is indented under the for statement is part of the loop.\n\n\nIn many programming languages the format and structure makes code more easily readable.  Structure is even more critical in Python.  Read [this article from Geeks for Geeks](https://www.geeksforgeeks.org/indentation-in-python/) to gain an understanding of the importance of indentation, format, and structure when writing code blocks in Python.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c729f"
        },
        {
          "subsectionOrder": 8,
          "title": "Control Flow",
          "content": "Control flow structures allow us to direct the execution of code based on conditions or repeat code until a condition is met. The two main control flow structures in Python are **conditional statements** and **loops**.\n\n### Conditional Statements\n\nConditional statements enable code to execute only when specific conditions are met. Python uses `if`, `elif`, and `else` statements to handle different conditions.\n\n* `if`: Checks the initial condition. If `True`, it runs the code block.\n* `else`: Runs if none of the previous conditions were `True`.\n* `elif`: Stands for 'else if'; checks additional conditions if the previous ones were `False`.\n\n```python\nage = 20\n\nif age >= 18:\n    print(\"You're an adult!\")\nelif age >= 13:\n    print(\"You're a teenager.\")\nelse:\n    print(\"You're a child.\")\n```\n\n#### Nested Conditionals\n\nYou can also nest conditionals inside each other for more complex decision-making.\n\n```python\nscore = 85\n\nif score >= 90:\n    print(\"A\")\nelse:\n    if score >= 80:\n        print(\"B\")\n    else:\n        print(\"C\")\n```\n\n### Loops\n\n**Loops** allow us to repeat code multiple times, either for a specific range or while a condition is `True`\n\n#### `For` Loop\n\nThe `for` loop is commonly used to iterate over a sequence (like a list or range of numbers).\n\n```python\n# Looping through a list\nfruits = [\"apple\", \"banana\", \"cherry\"]\n\nfor fruit in fruits:\n    print(fruit)\n\n# Using range to loop a specific number of times\n# range(stop) where stop is greater than the last number generated\n# range(start, stop) starts with a number other 0\n# range(start, stop, step) uses the specified step size instead of 1 \nfor i in range(3):\n    print(\"Loop iteration:\", i)\n```\n\n#### `While` Loop\n\nA `while` loop runs as long as a condition remains `True`. Be careful to ensure the condition will eventually be `False` to avoid infinite loops.\n\n```python\ncount = 0\n\nwhile count < 3:\n    print(\"Count is:\", count)\n    count += 1\n```\n\n#### Breaking Out of Loops\n\nThe `break` statement can be used to exit a loop early.\n\n```python\nfor num in range(10):\n    if num == 5:\n        break\n    print(num)\n# Output: 0, 1, 2, 3, 4\n```\n\n#### Skipping Iterations\n\nThe `continue` statement allows you to skip the rest of the code in the current iteration and move to the next iteration.\n\n```python\nfor num in range(5):\n    if num == 2:\n        continue\n    print(num)\n# Output: 0, 1, 3, 4\n```\n\n### Check for Understanding\n\n**Question:** What will the following code output if `age = 16`?\n\n```python\nif age >= 18:\n    print(\"You're an adult!\")\nelif age >= 13:\n    print(\"You're a teenager.\")\nelse:\n    print(\"You're a child.\")\n```\n\n* A) \"You're an adult!\"\n* B) \"You're a teenager.\"\n* C) \"You're a child.\"\n* D) No output\n\n<details>\n\n<summary>View answer</summary>\n\n**Answer**: B) \"You're a teenager.\"\n\n</details>\n\n**Question**: What will the following code output?\n\n```python\nfor i in range(3):\n    print(i)\n```\n\n* A) `1 2 3`\n* B) `0 1 2`\n* C) `0 1 2 3`\n* D) `3`\n\n<details>\n\n<summary>View answer</summary>\n\n**Answer**: B) `0 1 2`\n\n</details>\n\n### Long Lines \n\nIf you need to split a line for readability, you put a backslash `\\` at the end of the line.  You don't need to indent the line after the backslash.  Python concatenates the two lines (or more, if the next line also has a backslash.)  If you do indent, Python includes the indentations.  Long strings can also be created another way.  You start and end them with three quotes. `\"\"\"`  Python concatenates them, including the line feeds, spaces, and indentations.\"\n\n## üé¨ Video 1.5: Loops and Conditionals\nOur next video is a breakdown of  common situations in which someone might use loops, how to control loop behavior with `break` and `continue`, and loop nesting.\n\n**[Watch the video here](https://youtu.be/VUwzi5TVMzM).**",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72a0"
        },
        {
          "subsectionOrder": 9,
          "title": "Functions",
          "content": "**Functions** are reusable blocks of code that perform specific tasks. They help keep your code organized, modular, and easy to understand.\n\n### Defining and Calling Functions\n\nA function is defined using the `def` keyword, followed by a function name, parentheses `()`, and a colon. The code inside the function is indented.\n\n```python\ndef greet():\n    print(\"Hello, world!\")\n```\n\nTo call a function, simply use its name followed by parentheses.\n\n```python\ngreet()  # Output: Hello, world!\n```\n\n### Parameters and Arguments\n\nFunctions can take **parameters** (variables defined within the parentheses in the function definition) to make them more versatile. When calling the function, you pass **arguments** (the actual values).\n\n```python\ndef greet(name):\n    print(\"Hello, \" + name + \"!\")\n    \ngreet(\"Jazmine\")  # Output: Hello, Jazmine!\n```\n\nYou can also define multiple parameters.\n\n```python\ndef add(a, b):\n    print(a + b)\n\nadd(3, 5)  # Output: 8\n```\n\n### Return Values\n\nA function can return a value to the caller using the `return` keyword. This makes the function's output available for use outside the function.\n\n```python\ndef square(number):\n    return number * number\n\nresult = square(4)  # result is 16\n```\n\nIf a function doesn‚Äôt explicitly return a value, it implicitly returns `None`\n\n### Default Parameters\n\nYou can set **default values** for parameters, making them optional when the function is called.\n\n```python\ndef greet(name=\"stranger\"):\n    print(\"Hello, \" + name + \"!\")\n\ngreet()            # Output: Hello, stranger!\ngreet(\"Luis\")   # Output: Hello, Luis!\n```\n\n### Using *args and **kwargs\n\nFunctions can be made more flexible by allowing them to handle an arbitrary number of arguments.\n\n**What are *args?**\n\n*args allows a function to accept any number of positional arguments, which are collected into a tuple.  A tuple is an immutable, ordered data structure.  Tuples are covered in detail in lesson2.\n\n**Example:**\n\n```python\ndef add_numbers(*args):\n    return sum(args)\n\nprint(add_numbers(1, 2, 3, 4))  # Output: 10\n```\n\n**Key Points:**\n\n* Use *args when the exact number of arguments isn't known beforehand.\n* The collected arguments are treated as a tuple.\n\n**What are **kwargs?**\n\n**kwargs allows a function to accept any number of keyword arguments, which are collected into a dictionary.  A dictionary is an associative array similar to a Hash in Ruby or an object in JavaScript.  Dictionaries are covered in detail in lesson 2.\n\n**Example:**\n\n```python\ndef print_info(**kwargs):\n    for key, value in kwargs.items():\n        print(f\"{key}: {value}\")\n\nprint_info(name=\"Janet\", role=\"Developer\", age=25)\n```\n\n**Key Points:**\n\n* Use **kwargs when you expect dynamic named parameters.\n* The collected arguments are treated as a dictionary.\n\n**Combining *args and **kwargs**\n\nYou can use both together to handle a mix of positional and keyword arguments.\n\n**Example:**\n\n```python\ndef mixed_function(*args, **kwargs):\n    print(\"Positional arguments:\", args)\n    print(\"Keyword arguments:\", kwargs)\n\nmixed_function(1, 2, 3, name=\"Janet\", role=\"Developer\")\n```\n\n**Output:**\n\n```\nPositional arguments: (1, 2, 3)\nKeyword arguments: {'name': 'Janet', 'role': 'Developer'}\n```\n\n**Another Example**\n\n```python\ndef mixed_function_2(*args, a_value=\"default\"):\n    print(f\"args are {args} and a_value is {a_value}\")\n\nmixed_function_2(1, 2, 3, a_value=\"override\")\n```\n\nIn this case, a_value is a keyword argument, because it comes after the `*args`.  In the example (this is not always so) a_value has a default value.  The keyword is given explicitly here, instead of using `**kwargs`.  When the keyword is given explicitly, the value is not delivered in a dictionary.  It is accessed using the keyword as a variable name. \n\nFunctions are always declared with named positional arguments (if any) first, then the `*args` (if this is used), then the explicitly named keyword arguments (if any), and then in last place `**kwargs` (if this is used).  When a function with keyword arguments is called, the order in which the keyword arguments is given doesn't matter (except they should come after the positional arguments.) \n\n### Variable Scope\n\nIf a variable is declared inside a function, it is local to that function.  For example, this code gives an error:\n\n```python\ndef set_name():\n    name=\"James\"\n\nset_name()\nprint(name)\n```\nThe name variable is declared inside the function, and is not defined outside.  Also note the results below:\n```python\nname = \"Hima\"\n\ndef set_name():\n    name=\"James\"\n\nset_name()\nprint(name) # Prints \"Hima\"\n\ndef set_name_2(name):\n    name = \"Nguyen\"\n\nset_name_2()\nprint(name) # Prints \"Hima\"\n```\n\nPython acts as if the `name=` statement inside each function is the declaration for a new local variable called `name`.  The first `name=` statement in the script above is for a global variable.\n\nA function *can* access global variables.  For example, suppose we add this code to the script above.\n\n```python\ndef print_global_name():\n    print(name) # Prints \"Hima\"\n```\n\nAnd, a function *can* change values stored in global variables (although this is typically bad practice).  As in this code:\n```python\nthis_list=[0,1] # a global\ndef change_list():\n    this_list[1]=17 # Does change this_list[1] for the this_list global\n```\nIndentation blocks in Python have no effect on variable scope.\n\n### üé¨ Video 1.6: Functions\n\nOur supplemental video for this section overviews functions, arguments, and parameters; along with two sets of example code.\n\n**[Watch the video here](https://youtu.be/89cGQjB5R4M?feature=shared).**\n\n### Check for Understanding\n\n**Question**: What is the purpose of the `return` statement in a function?\n\n* A) To stop the function\n* B) To send a value back to the caller\n* C) To print a message\n* D) To define a variable\n\n<details>\n\n<summary>View answer</summary>\n\n**Answer**: B) To send a value back to the caller.\n\n</details>\n\n**Question**: What will be the output of the following code?\n\n```python\ndef greet(name):\n    print(\"Hello, \" + name + \"!\")\n    \ngreet(\"Luis\")\n```\n\n* A) `\"Hello, stranger!\"`\n* B) `\"Hello, Luis!\"`\n* C) `\"Hello, name!\"`\n* D) `\"Luis\"`\n\n<details>\n\n<summary>View answer</summary>\n\n**Answer**: B) `\"Hello, Luis!\"`\n\n</details>",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72a1"
        },
        {
          "subsectionOrder": 10,
          "title": "Basic Debugging",
          "content": "**Debugging** is the process of finding and fixing errors in your code. Two popular methods for basic debugging in Python are using **print statements** and **logging**.\n\n### Debugging with Print Statements\n\nPrint statements are a simple way to check the values of variables and understand the flow of your program. This technique helps you see what‚Äôs happening at specific points in your code.\n\n```python\ndef multiply(a, b):\n    result = a * b\n    print(\"Result is: \", result)  # Print to check the result\n    return result\n\nmultiply(3, 5)  # Output: Result is: 15\n```\n\nTips for effective print debugging:\n\n* Use descriptive messages (e.g., `\"Starting loop at i=\" + str(i)`).\n* Print variable values and descriptions of the program state.\n* Remember to remove or comment out `print` statements when you‚Äôre done!\n\n### Debugging with Logging\n\nThe **logging** module provides more control over output and is useful for larger projects or tracking complex issues. Unlike **print**, logging allows you to set levels to distinguish between informational messages, warnings, errors, and more.\n\nLogging Levels\n\n* **DEBUG**: Detailed information, typically useful only for debugging.\n* **INFO**: Confirmation that things are working as expected.\n* **WARNING**: An indication that something unexpected happened, or indicative of future problems.\n* **ERROR**: A serious problem that prevented some part of the code from running.\n\nTo use logging:\n\n1. Import the `logging` module.\n2. Set up basic configuration with `logging.basicConfig()`.\n3. Use logging statements like `logging.debug()`, `logging.info()`, `logging.warning()`, and `logging.error()`.\n\nHere's an example of using logging for debugging. Notice how each of the three steps are incorporated.\n\n```python\n# Step 1\nimport logging\n\n# Step 2\nlogging.basicConfig(level=logging.DEBUG)\n\n# Step 3\ndef multiply(a, b):\n    logging.debug(f\"Multiplying {a} and {b}\")\n    result = a * b\n    logging.info(f\"Result is: {result}\")\n    return result\n\nmultiply(3, 5)\n```\n\n### For Further Investigation: Using the Debugger\n\nVSCode with the Python plugin provides a debugger.  You can set breakpoints, step in or over function calls, display or change the values of variables, and so on.  You will need to learn to use a debugger eventually, although it is not required for this course.  If you want to check this out, see [this link](https://code.visualstudio.com/docs/python/python-quick-start#_debug) for a description, and [here](https://www.youtube.com/watch?v=b4p-SBjHh28) is a video that shows the process.\n\n### üé¨ Video 1.7: Basic Debugging\n\nLet's wrap up this section with a short video on debugging.\n\n**[View the video here!](https://youtu.be/R4pCjyknKD0?feature=shared)**\n\n### Check for Understanding\n\n**Question**: What is the primary purpose of using `print` statements in debugging?\n\n* A) To find and correct errors in variable values and program flow\n* B) To slow down the program\n* C) To remove errors automatically\n* D) To show only the final output\n\n<details>\n<summary>View answer</summary>\n\n**Answer**: A) To find and correct errors in variable values and program flow\n</details>\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72a2"
        },
        {
          "subsectionOrder": 11,
          "title": "Error Handling",
          "content": "Error handling in Python is managed using the `try`, `except`, `else`, and `finally` blocks. This structure allows developers to gracefully handle errors that may occur during runtime, ensuring that the program can either recover from an issue or fail gracefully with useful feedback.\n\n### `try` and `except`\n\nThe `try` block contains code that might raise an error. If an error occurs, the `except` block is executed, and Python will not terminate the program abruptly. You can catch specific exceptions or handle all exceptions generally.\n\n```python\ntry:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Error: Division by zero is not allowed.\")\n```\n\n```python\ntry:\n    num = int(input(\"Enter a number: \"))\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n```\n\n### `else`\n\nThe `else` block is optional and runs if no exception was raised in the try block.\n\n```python\ntry:\n    result = 10 / 2\nexcept ZeroDivisionError:\n    print(\"Error: Division by zero is not allowed.\")\nelse:\n    print(f\"Success! The result is {result}.\")\n```\n\n### `finally`\n\nThe `finally` block runs regardless of whether an exception occurred or not. It‚Äôs often used for cleanup actions like closing files or database connections.\n\n```python\ntry:\n    file = open(\"example.txt\", \"r\")\n    content = file.read()\nexcept FileNotFoundError:\n    print(\"Error: File not found.\")\nfinally:\n    file.close()\n    print(\"File closed.\")\n```\n\n### Raising exceptions\n\nPython allows you to raise exceptions using the raise keyword, either with built-in exceptions or custom ones.\n\n```python\ndef check_age(age):\n    if age < 18:\n        raise ValueError(\"Age must be 18 or older.\")\n    return True\n\ntry:\n    check_age(16)\nexcept ValueError as e:\n    print(e)\n```\n\n### üé¨ Video 1.8 Error Handling\n\nOur final video of Lesson 1 covers error handling with `try` and  `except` blocks.\n\n**[View the video here](https://youtu.be/NIWwJbo-9_8?feature=shared).**\n\n\n\n### Check for Understanding\n\n**Question**: If the following code tries to divide by zero, which message will it print?\n\n```python\ntry:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Error: Division by zero is not allowed.\")\n```\n\n* A) It will print nothing\n* B) `10`\n* C) `Error: Division by zero is not allowed.`\n* D) `None`\n\n<details>\n<summary>View answer</summary>\n\n**Answer:** C) `Error: Division by zero is not allowed.`\n</details>",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72a3"
        },
        {
          "subsectionOrder": 12,
          "title": "String Operations",
          "content": "In Python, everything is an object, and each object is an instance of a class.  Each class provides methods for the object.  Try the following.  Open a `.py` file in VSCode, and declare a string, like:\n```\nmy_string = \"abc\"\n```\nThen, on the next line, type `my_string.`.  As you type the dot, VSCode prompts you with a pulldown that has many methods for this instance of the `str` class, such as lower(), upper(), split(), join(), strip(), and so on.  You can check out the reference [here.](https://docs.python.org/3/library/string.html)\n\nYou can also use formatted strings.  These do variable substitution to compose a string.  Each of the values is converted to a string and added to the result.  Formatted strings have an `f` just before the first double quote, as follows:\n```\nname = \"Ed\"\ncount = 6\nkind_of_object = \"apples\"\nprint(f\"{name} has {count} {kind_of_object}.\") # Prints \"Ed has 6 apples.\"\n```\nYou can also add format indications, for example to show two decimal places:\n```\ncost = 22/7\nprint(f\"The pie cost ${cost:.2f}.\")\n```",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72a4"
        },
        {
          "subsectionOrder": 13,
          "title": "üéâ Congratulations on finishing your first lesson in Python Essentials!",
          "content": "Your next step is to complete the coding assignment. As always, reach out to your mentor if you have questions, and take time to celebrate your hard work. \n\n---\nThis content was written by Janet Zulu, Reid Russom, and CTD volunteers‚Äîwith special thanks to the brain trust of John McGarvey, Rebecca Callari-Kaczmarczyk, Tom Arns, and Josh Sternfeld. To submit feedback, please fill out the **[CTD Curriculum Feedback Form](https://forms.gle/RZq5mav7wotFxyie6)**.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72a5"
        }
      ]
    },
    {
      "id": "68f812adc22606ecfa5c72b1",
      "lessonNumber": 2,
      "title": "Lesson 2 ‚Äî Data Structures and File Handling",
      "status": "pending",
      "assignment": {
        "title": "Assignment for Lesson 2",
        "objective": "**Objective:** In this assignment, you practice the use of the input() function.  You also practice file operations.  You use several methods of the list class, and you construct dictionaries from the contents of a CSV file.\n\n### **Step 1: Complete the Coding Tasks**  \n\nHomework for this assignment is created within your `python_homework` folder.  Be sure to create an `assignment2` branch.  Then, write Python code to complete the following tasks.  As you do, remember to put in **comment lines to mark your code for Task 1, Task 2, and so on.**\n\n**Help is Available**\n\nYou may find these tasks a little challenging.  If you get stuck, 1:1 mentors are available to answer your questions.  Appointments are available in the [1:1 Mentor Table](https://airtable.com/appoSRJMlXH9KvE6w/shrQinGb1phZYwdiL)\n\nOn Windows, if you find that the prompt in git bash is unexpected, you can fix it by running ``cd \\`pwd``.\n\nSince Python uses indentation to define blocks of code, it is often necessary to indent or outdent a whole block of code.  You can do this in vscode by selecting the code and then typing `ctl-]` to indent or `ctl-[` to outdent.\n\n---",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [
          {
            "taskNumber": 1,
            "title": "Diary",
            "description": "1. Change to the `assignment2` folder of your `python_homework` folder.  You create your programs for the assignment in this folder.\n\n2. Create a program called `diary.py`. Add code to do the following:\n   - Open a file called `diary.txt` for appending.\n   - In a loop, prompt the user for a line of input.  The first prompt should say, \"What happened today? \".  All subsequent prompts should say \"What else? \"\n   - As each line is received, write it to `diary.txt`, with a newline (`\\n`) at the end.\n   - When the special line \"done for now\" is received, write that to `diary.txt`.  Then close the file and exit the program (you just exit the loop).\n   - Wrap all of this in a try block. If an exception occurs, catch the exception and print out \"An exception occurred.\" followed by the name of the exception itself. Now, normally, you catch specific types of exceptions, and handle each according to program logic.  In this case, you can catch any non-fatal exceptions via an except for `Exception`, and then display the information from the exception and exit the program.  The `traceback` module provides a way to include function traceback information in your error message, which will make it easier to find the error.  You can use the following code to handle exceptions using the traceback module.\n   ```python\n   import traceback\n\n   ...\n\n   except Exception as e:\n      trace_back = traceback.extract_tb(e.__traceback__)\n      stack_trace = list()\n      for trace in trace_back:\n         stack_trace.append(f'File : {trace[0]} , Line : {trace[1]}, Func.Name : {trace[2]}, Message : {trace[3]}')\n      print(f\"Exception type: {type(e).__name__}\")\n      message = str(e)\n      if message:\n         print(f\"Exception message: {message}\")\n      print(f\"Stack trace: {stack_trace}\")\n   ```\n   - Open the file using a `with` statement (inside the try block), and rely on that statement to handle the file close.\n   - The input statement should be inside the loop inside the `with` block.\n\n3. Test the program.\n   - Run it a couple of times to create diary entries. (`python diary.py`)\n   - Have a look at `diary.txt` to make sure it appears correct.  **Warning:** `diary.txt` will end up in GitHub when you submit your homework, so don't put in anything personal.\n   - Trigger an exception while running the program:  When it prompts you for input, press Ctrl-D.  You may need to type Ctrl-C and newline to trigger an exception if Ctrl-D doesn't work.  Check to see that the exception is handled.",
            "codeExample": "```python\n   import traceback\n\n   ...\n\n   except Exception as e:\n      trace_back = traceback.extract_tb(e.__traceback__)\n      stack_trace = list()\n      for trace in trace_back:\n         stack_trace.append(f'File : {trace[0]} , Line : {trace[1]}, Func.Name : {trace[2]}, Message : {trace[3]}')\n      print(f\"Exception type: {type(e).__name__}\")\n      message = str(e)\n      if message:\n         print(f\"Exception message: {message}\")\n      print(f\"Stack trace: {stack_trace}\")\n   ```",
            "_id": "68f812adc22606ecfa5c72bc"
          },
          {
            "taskNumber": 2,
            "title": "Read a CSV File",
            "description": "This task, and the others that follow below, use the same pattern as for assignment1. This pattern is known as Test Driven Development (TDD).  It is a good practice which is often used in software industry.  You will need to create assignment2.py for the rest of the tasks.  Then type the following command:\n```bash\npytest -v -x assignment2-test.py\n```\nThis will give errors to report what you need to fix.  You run it repeatedly as you create the following functions, until all functions are working correctly.\n\nRemember to import the `csv` module for this task.\n\n2. Create a function called read_employees that has no arguments, and do the following within it. \n   - Declare an empty dict.  You'll add the key/value pairs to that.  Declare also an empty list to store the rows.\n   - You next read a csv file. Use a try block and a with statement, so that your code is robust and so that the file gets closed.\n   - Read `../csv/employees.csv` using csv.reader().  (This csv file is used in a later lesson to populate a database.)\n   - As you loop through the rows, store the first row in the dict using the key \"fields\".  These are the column headers.\n   - Add all the other rows (not the first) to your rows list.\n   - Add the list of rows (this is a list of lists) to the dict, using the key \"rows\".\n   - The function should return the dict.\n   - Add a line below the function that calls read_employees and stores the returned value in a global variable called employees. Then print out this value, to verify that the function works.\n   - In this case, it's not clear what to do if you get an exception.  You might get an exception because the filename is bad, or because the file couldn't be parsed as a CSV file.  For now, just use the same approach as described above: catch the exception, print out the information, and exit the program.  One likely exception in this case is an error in the syntax of your code.\n\n3. Run the test to see if you have this much right.\n\nA word about what's going on when the test runs: The test file imports your assignment2.py module.  When the import statement occurs, all the program statements in your module that are outside of functions do run.  That means the statement which sets your employees global variable is run.  As a result, the assignment2-test.py can reference this global variable too -- and it does.  If you forget to set this variable in your program, the test reports an error.",
            "codeExample": "```bash\npytest -v -x assignment2-test.py\n```",
            "_id": "68f812adc22606ecfa5c72bd"
          },
          {
            "taskNumber": 3,
            "title": "Find the Column Index",
            "description": "1. Create a function called column_index.  The input is a string.  The function looks in employees[\"fields\"] (an array of column headers) to find the index of the column header requested.  There won't be much to this function, because you just use the index() method of the list class, like so:\n```python\nemployees[\"fields\"].index(\"first_name\")\n```\nThe index() method returns the index of the matching value from the list.\n\n2. The column_index function should return this index.\n\n3. Run the test again to see if the test passes.\n\n4. Call the column_index function in your program, passing the parameter \"employee_id\".  Store the column you get back in a variable called employee_id_column.  This global value is used for subsequent steps.",
            "codeExample": "```python\nemployees[\"fields\"].index(\"first_name\")\n```",
            "_id": "68f812adc22606ecfa5c72be"
          },
          {
            "taskNumber": 4,
            "title": "Find the Employee First Name",
            "description": "1. Create a function called first_name.  It takes one argument, the row number.  The function should retrieve the value of first_name from a row as stored in the employees dict.\n\n2. You should first call your column_index function to find out what column index you want.\n\n3. Then you go to the requested row as stored in the employees dict, and get the value at that index in the row.\n\n4. Return the value.\n\n5. Try the test again.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72bf"
          },
          {
            "taskNumber": 5,
            "title": "Find the Employee: a Function in a Function",
            "description": "1. Create a function called employee_find.  This is passed one argument, an integer.  Just call it employee_id in your function declaration. We want it to return the rows with the matching employee_id.  There should only be one, but sometimes a CSV file has bad data.\n\n2. We could do this with a loop.  But we are going to use the filter() function.  Inside the employee_find function (yes, you do declare functions inside functions sometimes), create the following employee_match function:\n```python\ndef employee_match(row):\n   return int(row[employee_id_column]) == employee_id\n```\nThis function is referencing the employee_id value that is passed to the employee_find function.  It can access that value because the employee_match function is inside the employee_find function.  Note that we need to do type conversion here, because the CSV reader just returns strings as the values in the roows.  This inner function returns True if there is a match.  We are using the employee_id_column global value you set in Task 3.\n\n3. Now, still within the employee_find function, call the filter() function.  This is another one of those Python free standing functions.  (It is not a method of the list class.)  You call filter() as follows:\n```python\nmatches=list(filter(employee_match, employees[\"rows\"]))\n```\nThe filter() function needs to know how to filter, and the employee_match function provides that information.  The filter() function calls employee_match once per row, saying, Do we want this one?  When the filter function completes, we need to do type conversion to convert the result to a list.\n\n4. The employee_find function then returns the matches.\n\n5. Run the test and see if you got it right.",
            "codeExample": "```python\ndef employee_match(row):\n   return int(row[employee_id_column]) == employee_id\n```\n\n```python\nmatches=list(filter(employee_match, employees[\"rows\"]))\n```",
            "_id": "68f812adc22606ecfa5c72c0"
          },
          {
            "taskNumber": 6,
            "title": "Find the Employee with a Lambda",
            "description": "The employee_match function is a silly one-liner.  Lambdas allow us to give the logic inline.\n\n1. Create a function employee_find_2.  This function does exactly what employee_find does -- but it uses a lambda.\n```\ndef employee_find_2(employee_id):\n   matches = list(filter(lambda row : int(row[employee_id_column]) == employee_id , employees[\"rows\"]))\n   return matches\n```\n\nNote that there is no return statement in the lambda.  There is the parameter passed to the lambda (a row), followed by a colon, followed by the expression that gives the result.\n\n2. Run the test to make sure things still work.",
            "codeExample": "```\ndef employee_find_2(employee_id):\n   matches = list(filter(lambda row : int(row[employee_id_column]) == employee_id , employees[\"rows\"]))\n   return matches\n```",
            "_id": "68f812adc22606ecfa5c72c1"
          },
          {
            "taskNumber": 7,
            "title": "Sort the Rows by last_name Using a Lambda",
            "description": "We want to call the sort() method on the rows.  However, we need to tell it which column to use for the sort.\n\n1. Create a function sort_by_last_name.  It takes no parameters.  You sort the rows you have stored in the dict.\n\n2. Within the function, you call employees[\"rows\"].sort().  This sorts the list of rows in place. But, you need pass to the list.sort() method a keyword argument called key (so you pass a parameter with `key=` when you call it).  You set that keyword parameter equal to a lambda.  The lambda is passed the row, and the expression after the colon gives the value from the row to be used in the sort.  You might want to use your column_index function for last_name so you know which value from the row should be given in the lambda expression.  Remember that the `sort()` method sorts the list in place and does not return the sorted list.\n\n3. The sort_by_last_name function returns the sorted list of rows.\n\n4. Run the test until this works.\n\n5. Call the function in your program, and then print out the employees dict, to see it in sorted form.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72c2"
          },
          {
            "taskNumber": 8,
            "title": "Create a dict for an Employee",
            "description": "1. Create a function called employee_dict.  It is passed a row from the employees dict (not a row number).  It returns a dict.\n   - The keys in the dict are the column headers from employees[\"fields\"].\n   - The values in the dict are the corresponding values from the row.\n   - Do not include the employee_id in the dict.  You skip that field for now.\n\n2. Return the resulting dict for the employee.\n\n3. Add a line to your program that calls this function and prints the result.  Use a row from the rows stored in the employees dict to pass to the function for this test.\n\n4. Get the test working.\n\nIf you want to try something extra, look up the `zip()` function, which can be used to simplify the code for this problem.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72c3"
          },
          {
            "taskNumber": 9,
            "title": "A dict of dicts, for All Employees",
            "description": "1. Create a function called all_employees_dict.\n   - The keys in the dict are the employee_id values from the rows in the employees dict.\n   - For each key, the value is the employee dict created for that row.  (Use the employee_dict function you created in task 8.)\n\n2. The function should return the resulting dict of dicts.\n\n3. Add a line to your program that calls this function and prints the result.\n\n4. Get the test working.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72c4"
          },
          {
            "taskNumber": 10,
            "title": "Use the os Module",
            "description": "Sometimes the behavior of a program is to be modified without changing the program itself.  One way is to use environment variables.  Environment variables are also used to store secrets needed by the program, such as passwords.  Environment variables are accessed via the `os.getenv()` function.  Of course, there are many other functions in the os package.\n\n1. Within the terminal, enter the command `export THISVALUE=ABC`.\n\n2. Add a line to `assignment2.py` to import the os module.\n\n3. Create a function get_this_value().  This function takes no parameters and returns the value of the environment variable `THISVALUE`.\n\n4. Get the test working.  (Note that each time you want this test to pass, you have to have the `THISVALUE` environment variable set in your terminal session.)",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72c5"
          },
          {
            "taskNumber": 11,
            "title": "Creating Your Own Module",
            "description": "1. In the same folder, create a file called custom_module.py, with the following contents:\n\n```python\nsecret = \"shazam!\"\n\ndef set_secret(new_secret):\n   global secret\n   secret = new_secret\n```\n\n2. Add the line `import custom_module` to assignment2.py.\n\n3. Create a function called set_that_secret.  It should accept one parameter, which is the new secret to be set.  It should call custom_module.set_secret(), passing the parameter, so as to set the secret in custom_module.\n\n4. Add a line to your program to call set_that_secret, passing the new string of your choice.\n\n5. In another line, print out custom_module.secret.  Verify that it has the value you expect.\n\n6. Run the test until the next part passes.",
            "codeExample": "```python\nsecret = \"shazam!\"\n\ndef set_secret(new_secret):\n   global secret\n   secret = new_secret\n```",
            "_id": "68f812adc22606ecfa5c72c6"
          },
          {
            "taskNumber": 12,
            "title": "Read minutes1.csv and minutes2.csv",
            "description": "The \"story\" behind the following list of tasks is as follows.  A club meets, and for each meeting, there is a chairperson.  The club keeps several notebooks that record who whas the chairperson on a given date.  Some of the information is in one notebook, some in the other.  The club now wants to combine this information, to get the list of chairpersons sorted by date.  But the information in the csv files contains duplicates and is in no particular order.  (Yeah, the story is lame, but it is similar to other data analysis tasks.)\n\n1. Create a function called `read_minutes`.  It takes no parameters.  It creates two dicts, minutes1 and minutes2, by reading `../csv/minutes1.csv` and `../csv/minutes2.csv`.  Each dict has `fields` and `rows`, just as the employees dict had.  However! As you create the list of rows for both minutes1 and minutes2, convert each row to a tuple.  The function should return both minutes1 and minutes2.  **Note** You can return several values from a Python function, as follows: `return v1, v2`.  Don't worry about duplicates yet.  They will be dealt with in later tasks.  Think about the DRY (Don't repeat Yourself principal).  You may want to create a helper function to avoid duplicating code.\n\n2. Call the function within your assignment2.py script.  Store the values from the values it returns in the global variables minutes1 and minutes2. **Note** When a function returns several values, you get them as follows: `v1, v2 = function()`. Print out those dicts, so that you can see what's stored.\n\n3. Run the test until this part passes.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72c7"
          },
          {
            "taskNumber": 13,
            "title": "Create minutes_set",
            "description": "1. Create a function called `create_minutes_set`.  It takes no parameters. It creates two sets from the rows of minutes1 and minutes2 dicts.  (This is just type conversion.  However, to make it work, each row has to be hashable!  Sets only support hashable elements.  Lists aren't hashable, so that is why you stored the rows as tuples in Task 10.)  Combine the members of both sets into one single set.  (This operation is called a union.)  The function returns the resulting set.\n\n2. Call the function within your assignment2.py script.  Store the value returned in the global variable minutes_set.\n\n3. Run the test until the next part passes.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72c8"
          },
          {
            "taskNumber": 14,
            "title": "Convert to datetime",
            "description": "1. Add a statement, `from datetime import datetime`, to your program.  The datetime module has some nice capabilities for converting strings to dates.  You can look them up: strptime() and strftime().\n\n2. Create a function called create_minutes_list.  It takes no parameters, and does the following:\n   - Create a list from the minutes_set.  This is just type conversion.\n   - Use the `map()` function to convert each element of the list.  At present, each element is a list of strings, where the first element of that list is the name of the recorder and the second element is the date when they recorded.\n   - The map() should covert each of these into a tuple.  The first element of the tuple is the name (unchanged).  The second element of the tuple is the date string converted to a datetime object.\n   - You convert the date strings into datetime objects using `datetime.strptime(string, \"%B %d, %Y\")`.\n   - So, you could use the following lambda:\n   `lambda x: (x[0], datetime.strptime(x[1], \"%B %d, %Y\"))`\n   - The function should return the resulting list.\n\n3. Call the function from within your program.  Store the return value in the minutes_list global.  Print it out, so you can see what it looks like.\n\n4. Run the test until the next part passes.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72c9"
          },
          {
            "taskNumber": 15,
            "title": "Write Out Sorted List",
            "description": "1. Create a function called write_sorted_list.  It takes no parameters.  It should do the following:\n   - Sort minutes_list in ascending order of datetime.\n   - Call map again to convert the list.  In this case, for each tuple, you create a new tuple.  The first element of the tuple is the name (unchanged).  The second element of the tuple is the datetime converted back to a string, using `datetime.strftime(date, \"%B %d, %Y\")`\n   - Open a file called `./minutes.csv`.  Use a csv.writer to write out the resulting sorted data.  The first row you write should be the value of `fields` the from minutes1 dict.  The subsequent rows should be the elements from minutes_list.\n   - The function should return the converted list.\n\n2. Call this function from within your program.  Then check that the file is created, and that it contains appropriate content.\n\n3. Run the test again until the next test has passed.\n\n### Check for Understanding\n\n1. You created the minutes_set from several lists.  You then created a list from the set.  What's the point of the set, if you're going to end up with a list?\n\n2. Why did you subsequently need to create the list called minutes_list?  Couldn't you just keep working with the set?\n\n3. Why did you need to convert the date strings to datetime objects?\n\n4. Why did you convert them back to strings before writing out the CSV?\n\n\n### Answers\n\n1. The original lists had duplicates.  A set has only unique values, so by converting to a set, you get rid of duplicates.\n\n2. You need to call the map() function and sort() method.  Set objects don't support these, and in fact, the entries in a set are not in any particular order.  So, for these steps, you need lists.\n\n3. You needed to convert the date strings to datetime objects.  Otherwise, they'd just be sorted in alphabetical order, and \"April 1, 2023\" would come before \"September 2, 1980\".  You could have done this conversion in the lambda for `key=` in the sort() of the list, but that approach would make that lambda a little complicated.\n\n4. When a datetime object is printed, its appearance is not as friendly as the original date string, so you converted it back.\n\n\n### **Step 2: Submit Your Assignment on GitHub**  \n\n**Follow these steps to submit your work:**  \n\n#### **1Ô∏è‚É£ Add, Commit, and Push Your Changes**  \n- Within your python_homework folder, do a git add and a git commit for the files you have created, so that they are added to the `assignment2` branch.\n- Push that branch to GitHub. \n\n#### **2Ô∏è‚É£ Create a Pull Request**  \n- Log on to your GitHub account.\n- Open your `python_homework` repository.\n- Select your `assignment2` branch.  It should be one or several commits ahead of your main branch.\n- Create a pull request.\n\n#### **3Ô∏è‚É£ Submit Your GitHub Link**  \n- Your browser now has the link to your pull request.  Copy that link. \n- Paste the URL into the **assignment submission form**.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72ca"
          }
        ],
        "submissionInstructions": "Please submit on time",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Introduction",
          "content": "# Lesson 2 ‚Äî Data Structures and File Handling",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72b2"
        },
        {
          "subsectionOrder": 2,
          "title": "üëÄ Reminder: How to Follow This Content",
          "content": "* Start by reading the lesson's **learning objective** in the `Lesson Overview` section. Each weekly assignment will measure your skill related to the learning objective.\n* Lessons are split into **subsections**, labeled like this: `1.1`, `1.2`, etc.\n* Each subsection has a short **supplemental video** that will help you understand the content in that subsection.\n* At the end of each subsection, you'll find a multiple-choice **\"Check for Understanding\"** question. Complete the question and review the material if your answer is not correct!\n* After reading through the lesson content and correctly answering the \"Check for Understanding\" questions, complete the **Weekly Assignment**.\n\nIf you have questions at any point, ask a question in the `discussion` Slack channel or reach out to your mentor!",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72b3"
        },
        {
          "subsectionOrder": 3,
          "title": "Lesson Overview",
          "content": "Congrats, you've made it to Lesson 2! This week, we'll deepen our knowledge of core Python concepts, including dictionaries and lists. We'll also learn how to import **modules**, powerful tools that help your Python projects reach their full potential.\nAdditionally, we‚Äôll learn how to work with the operating system (OS) and virtual environments for more efficient project management.\n\n**Learning objective**: Students will explore various data structures, such as lists, tuples, dictionaries, and sets. They will learn how to read and write data to files and use external modules.  Additionally, they will be introduced to basic OS operations and using virtual environments.\n\nTopics:\n\n* Lists, Tuples, Dictionaries, and Sets: Creating, accessing, and modifying data structures.\n* File Handling: Reading from and writing to text and CSV files.\n* Introduction to Modules & packages: Importing and using Python libraries.\n* Keyboard Input\n* Working with the OS: Interacting with the file system and executing commands.\n* Virtual Environments: Managing dependencies for Python projects.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72b4"
        },
        {
          "subsectionOrder": 4,
          "title": "Lists, Tuples, Dictionaries, and Sets",
          "content": "Before we talk about lists and tuples, we need to define two new words: \"mutable\" and \"immutable.\"\n\nSomething that is **mutable** can be modified or changed after it is created. Something that is **immutable** cannot be modified or changed after it is created.\n\nImagine a piece of clay (mutable) versus a ceramic statue (immutable):\n\n* Clay can be reshaped, squished, or molded into different forms after it's first created. You can add or remove parts easily.\n* A ceramic statue, once it's fired in a kiln, cannot be changed without breaking it completely.\n\nIn Python programming, here's a concrete example using *mutable* lists and *immutable* tuples:\n\n``` python\n# Mutable example (list)\nfruits = ['apple', 'banana', 'cherry']\nfruits.append('date')  # We can add a new item\nfruits[0] = 'orange'  # We can change an existing item\nprint(fruits)  # Now ['orange', 'banana', 'cherry', 'date']\n\n# Immutable example (tuple)\ncolors = ('red', 'green', 'blue')\n# colors[0] = 'yellow'  # This would cause an error\n# You cannot change the tuple after creation\n```\nStrings are also immutable in Python.  This would cause an error:\n```\ncapital = \"raleigh\"\ncapital[0] = \"R\"\n```\n\n### Lists\n\nA **list** is a *mutable*, ordered collection of items. Lists allow duplicates and can hold items of different types, although typically you‚Äôll find lists containing items of the same type. You can add, remove, or modify elements in a list, making them versatile for storing data that may change.\n\n```python\nfruits = ['apple', 'banana', 'cherry']  # Define a list\nfruits.append('orange')  # Add an item\nprint(fruits)  # Output: ['apple', 'banana', 'cherry', 'orange']\n```\n\n#### Key List Methods\n\n* `append(item)`: Adds `item` to the end of the list.\n* `remove(item)`: Removes the first occurrence of `item`.\n* `sort()`: Sorts the list in place.\n\nThere lots of methods and operations which apply to lists including all of the [common](https://docs.python.org/3/library/stdtypes.html#typesseq-common) and [mutable](https://docs.python.org/3/library/stdtypes.html#typesseq-mutable) sequence operations.\n\nA list is an example of an iterable.  So you can iterate on it, as follows:\n\n```python\nfruits = ['apple', 'banana', 'cherry']\nfor fruit in fruits:\n    print(fruit)\n```\nThere are various other iterable collections in Python, such as tuples.\n\n#### The map() Function for Lists.\n\nThe map() function is not a method of the list class.  It is a Python built in function that operates on iterables. Suppose you have a list of numbers, and you want to increment all of them.  You can do it as follows:\n\n```python\nlist_one = [3,4,5]\ndef incrementor(x):\n    return x + 1\n\nlist_two = list(map(incrementor, list_one)) # [4,5,6]\n```\n\nYou pass the map() function two arguments, the function that changes the list item, and the iterable itself.  The function returns an iterable, and we can do type coversion to create a list.  Now, the incrementor() function above looks a little stupid.  You'd like to pass something in line, and for that, Python provides:\n\n### Lambdas\n\n```python\nlist_one = [3,4,5]\nlist_two = list(map(lambda x: x+1, list_one)) # [4,5,6]\n```\n\nThe lambda feature is a way to declare a simple function.  Lambdas are, in some respects, similar to arrow functions in JavaScript, but they are much more limited.  A lambda is a simple one liner.  The syntax is as follows: the word `lambda` followed by the arguments to be passed (the map function only passes one parameter, so in the case above there is only argument for the lambda), followed by a colon `:`, followed by an expression.  The value of the expression is what is returned by the lambda.  You can only give one expression in a lambda, so there is no room for multiple statements.\n\n### Slicing Lists\n\nYou can create a subset list from a list by slicing.  Here are examples:\n\n```python\nlist_1 = ['A','B','C','D','E']\nlist_2 = list_1[1:3] # Slicing from beginning to end. Gives ['B','C'].  list_1[3] is not included.\nlist_3 = list_1[2:]  # Gives ['C','D','E']\nlist_4 = list_1[:2]  # Gives ['A', 'B']\nlist_5 = list_1[-2:] # Gives ['D', 'E'] -- the last two elements\n\n### Tuples\n\nA **tuple** is an *immutable*, ordered collection of items. Once defined, the elements in a tuple cannot be modified, added, or removed. Tuples are useful for data that shouldn‚Äôt change, like fixed configuration values or coordinates.\n\n```python\ndimensions = (1920, 1080)  # Define a tuple\nprint(dimensions[0])  # Access the first element: Output: 1920\n```\n\n#### Why use tuples?\n\n* Tuples are memory-efficient compared to lists.\n* Useful when you want a constant, fixed-size collection of data.\n\n### Dictionaries\n\nA **dictionary** is an unordered collection of key-value pairs. Each key is unique and used to store and retrieve data efficiently. Dictionaries are ideal for mapping relationships, such as a user‚Äôs name to their profile data or an item to its price.\n\n```python\nperson = {'name': 'Jazmine', 'age': 30}  # Define a dictionary\nprint(person['name'])  # Output: Jazmine\nperson['email'] = 'jazmine@example.com'  # Add a new key-value pair\n```\n\n#### Key Dictionary Methods\n\n* `keys()`: Returns a list of keys in the dictionary.\n* `values()`: Returns a list of values.\n* `items()`: Returns an iterable over the key, value pairs in the dictionary\n* `get(key)`: Returns the value associated with `key`, or `None` if `key` is not found.\n\nThe operations and methods available for dictionaries are documented [here](https://docs.python.org/3/library/stdtypes.html#mapping-types-dict).\n\n### Sets\n\nA **set** is an unordered collection of unique elements. Sets are useful for removing duplicates from a list or performing mathematical set operations like union, intersection, and difference.\n\n```python\nunique_numbers = {1, 2, 3, 3, 4}  # Define a set (duplicates are ignored)\nunique_numbers.add(5)  # Add a new item\nprint(unique_numbers)  # Output: {1, 2, 3, 4, 5}\n```\n\n#### Common Set Operations\n\n* `union()`: Returns a set containing all unique elements from both sets.\n* `intersection()`: Returns elements common to both sets.\n* `difference()`: Returns elements present in one set but not the other.\n\nSet operations and methods are documented [here](https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset).\n\n### Review Table: Lists, Tuples, Dictionaries, and Sets\n\nOk, let's review! Here's a table demonstrating the key differences between the four new terms we just learned:\n\n| Data Structure | Ordered | Changeable (Mutable) | Allows Duplicates | Key-Value Pairs |\n|----------------|---------|----------------------|-------------------|-----------------|\n| **List**       | Yes     | Yes                 | Yes               | No              |\n| **Tuple**      | Yes     | No                  | Yes               | No              |\n| **Dictionary** | No      | Yes (Keys: No)      | Keys: No, Values: Yes | Yes           |\n| **Set**        | No      | Yes                 | No                | No              |\n\n### 2.1 Video: Lists, Tuples, Dictonaries, and Sets\n\nVideo 2.1 explains lists, sets, and tuples. \n\n**[View the video here](https://youtu.be/gOMW_n2-2Mw?feature=shared).**\n\n### 2.1 Check for Understanding\n\n**Question**: Which of the following statements about Python data structures is correct?\n\n* A) Lists are immutable, and tuples are mutable.\n* B) Sets allow duplicate elements.\n* C) Dictionaries store data in key-value pairs and do not allow duplicate keys.\n* D) Tuples are ordered and changeable.\n\n<details>\n\n<summary>Answer</summary>\n\n**Answer**: C) Dictionaries store data in key-value pairs and do not allow duplicate keys.\n\n</details>",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72b5"
        },
        {
          "subsectionOrder": 5,
          "title": "File Handling",
          "content": "In Python, reading from and writing to text files is handled with the `open()` function. Text files are simple, containing plain text data, making them ideal for storing simple logs, configuration data, or notes.\n\n#### Reading a Text File\n\nTo read a file, use `open()` with the `\"r\"` (read) mode and call `.read()`, `.readline()`, or .`readlines()` to get the content.\n\n```python\nwith open('example.txt', 'r') as file:\n    content = file.read()  # Read entire file\n    print(content)\n```\n\nThe python above uses the `with` statement.  This is a way to keep your code looking clean.  You always want to close the file when you are done.  The `with` statement closes it for you on exit from the block.  The file is closed even if there is an exception, but the exception is still passed on to you.  Later in the course, you will also use the `with` statement for a database connection, and it serves the same purpose.\n\nFile operations, including the open(), can raise exceptions.  To make your code robust, you put them in a try block, as follows:\n\n```python\ntry:\n    with open('example.txt', 'r') as file:\n        content = file.read()  # Read entire file\n        print(content)\nexcept Exception as e:\n    print(f\"An error occurred reading the file: {e}\")\nelse:\n    print(\"The file was read ok.\")\n```\nIf your `with` block is longer, you may have other try blocks inside it for more granular exception handling.\n\n#### Writing to a Text File\n\nTo write to a file, open it in `\"w\"` (write) or `\"a\"` (append) mode. Writing mode will overwrite the file if it exists, while append mode will add to the file.\n\n```python\nwith open('example.txt', 'w') as file:\n    file.write(\"Hello, World!\")  # Write to the file\n```\n\nThe `write()` method does not add a newline after the string which is provided.  The special character `'\\n'` can be added to the end of the string to add a newline.\n\n#### Additional Modes\n\n* `\"r+\"`: Read and write\n* `\"a\"`: Append to the file (keeps existing content).\n\n### Reading and Writing CSV Files\n\nCSV (Comma-Separated Values) files are commonly used to store tabular data, where each row is a new line, and each value is separated by a comma. Python‚Äôs built-in `csv` module makes it easy to work with these files.\n\n#### Reading a CSV File\n\nTo read a CSV file, use `csv.reader()` and iterate through the rows.\n\n```python\nimport csv\n\nwith open('example.csv', 'r') as file:\n    reader = csv.reader(file)\n    for row in reader:\n        print(row)\n```\n\n#### Writing to a CSV File\n\nTo write to a CSV file, use `csv.writer()`. Each row should be a list or tuple representing a row in the CSV.\n\n```python\nimport csv\n\nwith open('example.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow(['Name', 'Age', 'City'])  # Write header row\n    writer.writerow(['Jazmine', 30, 'New York'])  # Write a data row\n```\n\n#### Additional Options\n\n* `csv.DictReader()` and `csv.DictWriter()`: Use dictionaries to work with row data, which can make reading and writing more convenient when you have headers.\n\n### 2.2 Video: Reading and Writing a CSV File\n\nIn this video, we'll demonstrate reading and writing to a real CSV file. After the video, practice reading and writing to a CSV file using the W3 Resource tutorial [here](https://www.w3resource.com/python-exercises/csv/index.php).\n\n**[View the video here](https://youtu.be/MWYRGLKMzAQ?feature=shared).**\n\n### 2.2 Check for Understanding\n\n**Question**: What does the `\"w\"` mode do when opening a file in Python?\n\n* A) Reads the file without making changes.\n* B) Appends new data to the end of the file.\n* C) Overwrites the file with new data, creating it if it doesn‚Äôt exist.\n* D) Opens the file for reading and writing without overwriting.\n\n<details>\n\n<summary>Answer</summary>\n\n**Answer**: C) Overwrites the file with new data, creating it if it doesn‚Äôt exist.\n\n</details>",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72b6"
        },
        {
          "subsectionOrder": 6,
          "title": "Introduction to Modules",
          "content": "In Python, modules and packages are essential tools for organizing and reusing code. This section will cover everything you need to know about working with both modules and packages effectively.\n\n### Modules\n\nA module is a Python file containing code (functions, classes, and variables) that can be reused across different parts of a project. Modules help with:\n\n1. **Code Organization**: Breaking large codebases into manageable files\n2. **Reusability**: Using common code wherever needed\n3. **Namespace Management**: Avoiding naming conflicts\n4. **Built-in Functionality**: Accessing Python's standard library features\n\n#### Importing Modules\n\nThere are several ways to import modules:\n\n```python\n# Import entire module\nimport math\nprint(math.sqrt(16))  # Output: 4.0\n\n# Import specific functions\nfrom math import sqrt\nprint(sqrt(16))  # Output: 4.0\n\n# Use aliases for shorter names\nimport pandas as pd\ndf = pd.DataFrame({'Name': ['Jazmine', 'Luis'], 'Age': [30, 35]})\n```\n\n#### Creating Custom Modules\n\nYou can create your own modules by saving Python code in a .py file:\n\n```python\n# math_tools.py\ndef add(a, b):\n    return a + b\n\ndef multiply(a, b):\n    return a * b\n\n# main.py\nimport math_tools\nprint(math_tools.add(2, 3))  # Output: 5\n```\n\n### Packages\n\nA package is a collection of related modules organized in a directory structure. Packages require an `__init__.py` file to mark the directory as a Python package.  In this course, we don't create large projects with many modules, but as a Python professional, you will need to do this.  The contents of `__init__.py` are not described in this lesson, but you can view [this tutorial](https://packaging.python.org/en/latest/tutorials/packaging-projects/) to see how it is to be done (this is optional for this course).\n\n#### Package Structure Example\n```\nmy_package/\n    __init__.py         # Makes this directory a package\n    math_tools.py       # Module for math operations\n    string_tools.py     # Module for string operations\n```\n\n#### Using Packages\n\n```python\n# Import specific modules from a package\nfrom my_package import math_tools\nprint(math_tools.add(10, 20))\n\n# Import specific functions (if configured in __init__.py)\nfrom my_package import add, multiply\n```\n\n### Types of Modules\n\n1. **Standard Library**: Provides a vast library of modules and infrastructure.  It is documented [here](https://docs.python.org/3/library/index.html). Examples of built-in modules include:\n   - `datetime` for dates and times\n   - `json` for JSON data\n   - `os` for operating system operations\n\n2. **External Libraries**: Additional modules installed via pip:\n   ```bash\n   pip install requests\n   \n   # Then use in code:\n   import requests\n   response = requests.get('https://api.example.com/data')\n   ```\n\n### 2.3 Video: Python Modules and Libraries\n\nWhat are Python modules? How do you import and work with them? Check out vido 2.3 to learn more. \n\n**[View the video here](https://youtu.be/XcfxkHrHTVE?feature=shared).**\n\n### 2.3 Check for Understanding\n\n**Question**: Which of the following commands correctly imports only the sqrt function from the `math` module?\n\n* A) `import sqrt from math`\n* B) `from math import sqrt`\n* C) `import math.sqrt`\n* D) `import math as sqrt`\n\n<details>\n\n<summary>Answer</summary>\n\n**Answer**: B) `from math import sqrt`\n\n</details>",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72b7"
        },
        {
          "subsectionOrder": 7,
          "title": "Keyboard Input",
          "content": "In Python, handling keyboard input is straightforward, making it easy to interact with users. The input() function is the main way to capture user input from the keyboard, which you can then use directly or store in a variable for further processing.\n\n#### Using `input()`\n\nThe `input()` function displays a prompt (optional) and waits for the user to type something and press Enter. By default, `input()` captures the input as a string, so if you need it in another format (like an integer), you‚Äôll have to convert it.\n\n```python\nname = input(\"Enter your name: \")  # Displays a prompt and waits for input\nprint(f\"Hello, {name}!\")  # Greets the user with their input\n```\n\n#### Converting Input Types\n\nSince `input()` returns data as a string, you‚Äôll often want to convert it for calculations or comparisons.\n\n```python\nage = input(\"Enter your age: \")  # Gets input as a string\nage = int(age)  # Converts input to an integer\nprint(f\"Next year, you will be {age + 1} years old.\")\n```\n\nIf the user types something that can‚Äôt be converted (e.g., entering \"twenty\" instead of \"20\"), this will raise an error. To handle this gracefully, you can use `try-except`:\n\n```python\ntry:\n    age = int(input(\"Enter your age: \"))\n    print(f\"Next year, you will be {age + 1} years old.\")\nexcept ValueError:\n    print(\"Please enter a valid number.\")\n```\n\n### Example: Simple Calculator Using Keyboard Input\n\nHere‚Äôs a basic example that combines multiple `input()` calls to create a calculator. Pay attention to this code, because you'll be creating a calculator in this week's assignment!\n\n```python\n# Simple calculator\ntry:\n    num1 = float(input(\"Enter the first number: \"))\n    num2 = float(input(\"Enter the second number: \"))\n    operation = input(\"Enter an operation (+, -, *, /): \")\n\n    if operation == \"+\":\n        print(\"Result:\", num1 + num2)\n    elif operation == \"-\":\n        print(\"Result:\", num1 - num2)\n    elif operation == \"*\":\n        print(\"Result:\", num1 * num2)\n    elif operation == \"/\":\n        print(\"Result:\", num1 / num2)\n    else:\n        print(\"Invalid operation\")\nexcept ValueError:\n    print(\"Please enter valid numbers.\")\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero.\")\n```\n\n### 2.4 Check for Understanding\n\n**Question**: Which of the following statements about `input()` in Python is correct?\n\n* A) `input()` captures user input and automatically converts it to an integer.\n* B) `input()` displays a prompt and captures user input as a string by default.\n* C) `input()` captures user input but only works with numbers.\n* D) `input()` is used to output text to the console.\n\n<details>\n\n<summary>Answer</summary>\n\n**Answer**: B) input() displays a prompt and captures user input as a string by default.\n\n</details>",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72b8"
        },
        {
          "subsectionOrder": 8,
          "title": "Working with the OS",
          "content": "The `os` module in Python provides a powerful interface for interacting with the operating system. It enables you to perform various tasks, including:\n\n* File and directory manipulation\n* Environment variable handling\n* Running system commands\n\nThese functionalities are essential for automating processes and managing files within Python programs.\n\n### Common os Module Functions\n\n**Getting the Current Working Directory**\n\nThe `os.getcwd()` function retrieves the path of the current working directory (the directory from which your Python script is executing).\n\n```python\nimport os\ncurrent_directory = os.getcwd()\nprint(f\"Current working directory: {current_directory}\")\n```\n\n**Changing the Current Working Directory**\n\nYou can change the current working directory using `os.chdir(path)`, where `path` specifies the directory you want to switch to.\n\n```python\nos.chdir('/path/to/your/folder')\nprint(f\"New working directory: {os.getcwd()}\")\n```\n\n**Listing Files in a Directory**\n\nThe `os.listdir()` function returns a list containing all files and folders within the specified directory.\n\n```python\nfiles = os.listdir('/path/to/your/folder')\nprint(f\"Files in the directory: {files}\")\n```\n\n**Creating and Removing Directories**\n\nUse `os.mkdir()` to create a new directory and `os.rmdir()` to remove an empty directory.\n\n```python\n# Create a new directory\nos.mkdir('new_folder')\n\n# Remove the directory\nos.rmdir('new_folder')\n```\n\n*Note: If the directory isn't empty, you'll encounter an error. To remove a non-empty directory, use `shutil.rmtree()` from the `shutil` module.*\n\n**Checking if a Path Exists**\n\nUse `os.path.exists()` to verify if a file or directory exists at the specified path.\n\n```python\nif os.path.exists('some_file.txt'):\n    print(\"The file exists.\")\nelse:\n    print(\"The file does not exist.\")\n```\n\n**Getting File Information**\n\nThe `os.path` module provides functions to check file properties. For instance, use `os.path.isfile()` to determine if a path points to a file and `os.path.isdir()` to check if it's a directory.\n\n```python\nif os.path.isfile('some_file.txt'):\n    print(\"This is a file.\")\nelif os.path.isdir('some_folder'):\n    print(\"This is a folder.\")\n```\n\n**Executing Shell Commands**\n\nThe `os.system()` function allows you to execute shell commands from within your Python program. You can leverage this functionality to run system commands like `ls` or `dir` to list files or execute other commands provided by your operating system.\n\n```python\nos.system('ls')  # For Linux/Mac\nos.system('dir')  # For Windows\n```\n\n*Keep in mind that `os.system()` doesn't return the output of the command. If you need the output, consider using `subprocess.run()` instead.*\n\n**Environment Variables**\n\nYou can access environment variables using `os.environ`. For example, to retrieve the `PATH` environment variable:\n\n```python\npath_variable = os.environ.get('PATH')\nprint(path_variable)\n```\n\n**Addendum: The sys Package**\n\nIn many environments, Python is used as a scripting tool.  Python scripts are invoked with arguments.  The user might type:\n\n```bash\npython loadfile.py ./current.csv\n```\nand the script might load the contents of the file into a database.  The sys package enables this (among other things).  Consider the following code:\n\n```python\nimport sys\nfor arg in sys.argv:\n    print(arg)  # arg[0] is the program name.  The other arguments are what was passed on the command line.\n```\n\nThe [argparse](https://docs.python.org/3/library/argparse.html#module-argparse) module provides a powerful framework for handling command line arguments.\n\n### Video 2.5: Working with `os`\n\n**[Watch an overview of working with the `os` module here](https://youtu.be/tJxcKyFMTGo?feature=shared).**\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72b9"
        },
        {
          "subsectionOrder": 9,
          "title": "Virtual Environments",
          "content": "Virtual environments are a cornerstone of Python development, particularly when managing dependencies and isolating project environments. A virtual environment creates an isolated Python environment, enabling you to have project-specific dependencies that won't interfere with other projects or system-wide Python packages.\n\n### Why Use Virtual Environments?\n\n* **Isolation**: Virtual environments ensure that each project has its own set of dependencies, preventing version conflicts.\n* **Reproducibility**: Virtual environments make it simpler to reproduce the exact setup for a project on another machine.\n* **Dependency Management**: You can install and update packages without affecting other projects or your system's Python installation.\n\n### Setting Up a Virtual Environment\n\n1. **Install `virtualenv` (optional)**\n\n   While Python 3.3+ comes with the `venv` module for creating virtual environments, some users prefer using `virtualenv` for additional features. To install it:\n\n   ```bash\n   pip install virtualenv\n   ```\n\n2. **Create a Virtual Environment**\n\n   You can create a virtual environment using either `venv` (built-in module) or `virtualenv` (third-party package).\n\n   - **Using `venv`**:\n\n     ```bash\n     python3 -m venv myenv\n     ```\n\n     This command creates a directory named `myenv` that contains the virtual environment.  (Note: by convention, `.venv` is usually used as the name of the virtual environment, instead of `myenv` as in this example.)\n\n   - **Using `virtualenv`**:\n\n     ```bash\n     virtualenv myenv\n     ```\n\n   **In both cases, `myenv` is the directory where the isolated Python environment will live.**\n\n3. **Activating the Virtual Environment**\n\n   To activate the virtual environment, use the following commands:\n\n   * **Windows:**\n     The following command assumes that you are using Git Bash as your Windows development terminal environment.  This is strongly recommended.\n\n     ```bash\n     source myenv/Scripts/activate\n     ```\n\n\n\n   * **Mac/Linux:**\n\n     ```bash\n     source myenv/bin/activate\n     ```\n\n   Once activated, your terminal prompt will usually change to show the virtual environment's name, e.g., `(myenv)`.\n\n4. **Installing Packages in the Virtual Environment**\n\n   After activation, you can install packages as usual with `pip`. These packages will be installed inside the virtual environment, not globally.\n\n   ```bash\n   pip install requests\n   ```\n\n   This ensures that the `requests` package is available only in the current virtual environment.\n\n5. **Deactivating the Virtual Environment**\n\n   To exit the virtual environment and return to the global Python environment, simply run:\n\n   ```bash\n   deactivate\n   ```\n\n6. **Managing Dependencies with `requirements.txt`**\n\n   To record all the dependencies installed in your virtual environment, use the `pip freeze` command to generate a `requirements.txt` file:\n\n   ```bash\n   pip freeze > requirements.txt\n   ```\n\n   This file can be used to recreate the environment on another machine:\n\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n7. **Virtual Environment with IDEs**\n\n   Many IDEs, like VS Code and PyCharm, allow you to configure the Python interpreter to use the virtual environment. This ensures that the IDE uses the correct Python environment with all the necessary packages installed.\n\n### Video 2.6: The Virtual Environment\n\nCheck out Video 2.6 for a quick overview of setting up a virtual environment for Python in VS Code.  Note that you have already done this!  Your python_homework directory uses a virtual environment.\n\n**[Watch the video here](https://youtu.be/GZbeL5AcTgw?feature=shared).**",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72ba"
        },
        {
          "subsectionOrder": 10,
          "title": "ü•≥ That's it for Lesson 2!",
          "content": "Check out this week's coding assignment, and reach out to a mentor if you need help!\n\n---\nThis content was written by Janet Zulu, Reid Russom, and CTD volunteers‚Äîwith special thanks to the brain trust of John McGarvey, Rebecca Callari-Kaczmarczyk, Tom Arns, and Josh Sternfeld. To submit feedback, please fill out the **[CTD Curriculum Feedback Form](https://forms.gle/RZq5mav7wotFxyie6)**.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72bb"
        }
      ]
    },
    {
      "id": "68f812adc22606ecfa5c72cc",
      "lessonNumber": 3,
      "title": "Lesson 3 ‚Äî More Python Skills",
      "status": "pending",
      "assignment": {
        "title": "**Lesson 03: More Python Skills**",
        "objective": "## **Assignment 3 Objective**\n\nIn this assignment, you will get practice in using decorators, list comprehensions, closures, and custom classes.\n\n## **Assignment Instructions**\n\nYou create the code for this assignment in your python_homework/assignment3 folder.  Be sure to create an `assignment3` git branch before you start.  As usual, mark the code that completes each task with a comment line.\n\n---",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [
          {
            "taskNumber": 1,
            "title": "Writing and Testing a Decorator",
            "description": "1. Within the assignment3 folder, create a file called `log-decorator.py`.  It should contain the following.\n2. Declare a decorator called logger_decorator.  This should log the name of the called function (`func.__name__`), the input parameters of that were passed, and the value the function returns, to a file `./decorator.log`.  (Logging was described in lesson 1, so review this if you need to do so.)  Functions may have positional arguments, keyword arguments, both, or neither.  So for each invocation of a decorated function, the log would have:\n    ```\n    function: <the function name>\n    positional parameters: <a list of the positional parameters, or \"none\" if none are passed>\n    keyword parameters: <a dict of the keyword parameters, or \"none\" if none are passed>\n    return: <the return value>\n    ```\n    Here's a cookbook on logging:\n    ```python\n    # one time setup\n    import logging\n    logger = logging.getLogger(__name__ + \"_parameter_log\")\n    logger.setLevel(logging.INFO)\n    logger.addHandler(logging.FileHandler(\"./decorator.log\",\"a\"))\n    ...\n    # To write a log record:\n    logger.log(logging.INFO, \"this string would be logged\")\n    ```\n3. Declare a function that takes no parameters and returns nothing.  Maybe it just prints \"Hello, World!\".  Decorate this function with your decorator.\n4. Declare a function that takes a variable number of positional arguments and returns `True`.  Decorate this function with your decorator.\n5. Declare a function that takes no positional arguments and a variable number of keyword arguments, and that returns `logger_decorator`.  Decorate this function with your decorator.\n6. Within the mainline code, call each of these three functions, passing parameters for the functions that take positional or keyword arguments.  Run the program, and verify that the log file contains the information you want.\n\n---",
            "codeExample": "```\n    function: <the function name>\n    positional parameters: <a list of the positional parameters, or \"none\" if none are passed>\n    keyword parameters: <a dict of the keyword parameters, or \"none\" if none are passed>\n    return: <the return value>\n    ```\n\n```python\n    # one time setup\n    import logging\n    logger = logging.getLogger(__name__ + \"_parameter_log\")\n    logger.setLevel(logging.INFO)\n    logger.addHandler(logging.FileHandler(\"./decorator.log\",\"a\"))\n    ...\n    # To write a log record:\n    logger.log(logging.INFO, \"this string would be logged\")\n    ```",
            "_id": "68f812adc22606ecfa5c72d4"
          },
          {
            "taskNumber": 2,
            "title": "A Decorator that Takes an Argument",
            "description": "1. Within your assignment3 folder, write a script called `type-decorator.py`.\n2. Declare a decorator called type_converter.  It has one argument called `type_of_output`, which would be a type, like `str` or `int` or `float`.  It should convert the return from `func` to the corresponding type, viz:\n   ```python\n   x = func(*args, **kwargs)\n   return type_of_output(x)\n   ```\n3. Write a function `return_int()` that takes no arguments and returns the integer value 5.  Decorate that function with type-decorator.  In  the decoration, pass `str` as the parameter to type_decorator.\n4. Write a function `return_string()` that takes no arguments and returns the string value \"not a number\".  Decorate that function with type-decorator.  In the decoration, pass `int` as the parameter to type_decorator.  Think: What's going to happen?\n5. In the mainline of the program, add the following:\n   ```python\n   y = return_int()\n   print(type(y).__name__) # This should print \"str\"\n   try:\n      y = return_string()\n      print(\"shouldn't get here!\")\n   except ValueError:\n      print(\"can't convert that string to an integer!\") # This is what should happen\n   ```\n\n---",
            "codeExample": "```python\n   x = func(*args, **kwargs)\n   return type_of_output(x)\n   ```\n\n```python\n   y = return_int()\n   print(type(y).__name__) # This should print \"str\"\n   try:\n      y = return_string()\n      print(\"shouldn't get here!\")\n   except ValueError:\n      print(\"can't convert that string to an integer!\") # This is what should happen\n   ```",
            "_id": "68f812adc22606ecfa5c72d5"
          },
          {
            "taskNumber": 3,
            "title": "List Comprehensions Practice",
            "description": "1. Within the assignment3 folder, create a file called `list-comprehensions.py`. Add code that reads the contents of `../csv/employees.csv` into a list of lists using the csv module.\n2. Using a list comprehension, create a list of the employee names, first_name + space + last_name.  The list comprehension should iterate through the items in the list read from the csv file.  Print the resulting list.  Skip the item created for the heading of the csv file.\n3. Using a list comprehension, create another list from the previous list of names.  This list should include only those names that contain the letter \"e\".  Print this list.\n\n---",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72d6"
          },
          {
            "taskNumber": 4,
            "title": "Closure Practice",
            "description": "1. Within the assignment3 folder, create a file called `hangman-closure.py`.\n2. Declare a function called `make_hangman()` that has one argument called secret_word.  It should also declare an empty array called guesses.  Within the function declare a function called hangman_closure() that takes one argument, which should be a letter.  Within the inner function, each time it is called, the letter should be appended to the guesses array.  Then the word should be printed out, with underscores substituted for the letters that haven't been guessed.  So, if secret_word is \"alphabet\", and guesses is [\"a\", \"h\"], then \"a__ha__\" should be printed out.  The function should return `True` if all the letters have been guessed, and `False` otherwise.  `make_hangman()` should return `hangman_closure`.\n3. Within hangman-closure.py, implement a hangman game that uses make_hangman().  Use the input() function to prompt for the secret word.  Then use the input() function to prompt for each of the guesses, until the full word is guessed.\n4. Test your program by playing a few games.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72d7"
          },
          {
            "taskNumber": 5,
            "title": "Extending a Class",
            "description": "1. Within the assignment3 folder, create a file called `extend-point-to-vector.py`.\n2. Create a class called `Point`.  It represents a point in 2d space, with x and y values passed to the `__init__()` method.  It should include methods for equality, string representation, and Euclidian distance to another point.\n3. Create a class called `Vector` which is a subclass of `Point` and uses the same `__init__()` method.  Add a method in the vector class which overrides the string representation so `Vector`s print differently than `Point`s.  Override the `+` operator so that it implements vector addition, summing the `x` and `y` values and returning a new `Vector`.\n4. Print results which demonstrate all of the classes and methods which have been implemented.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72d8"
          },
          {
            "taskNumber": 6,
            "title": "More on Classes",
            "description": "1. Within the assignment3 folder, create a file called `tictactoe.py`.\n2. Within this file, declare a class called TictactoeException.  This should inherit from the Exception class.  Add an `__init__` method that stores an instance variable called `message` and then calls the `__init__` method of the superclass.  This is a common way of creating a new type of exception.\n3. Declare also a class called Board.  This should have an `__init__` function that only has the `self` argument.  It creates a list of lists, 3x3, all git containing \" \" as a value.  This is stored in the variable self.board_array.  Create instance variables self.turn, which is initialized to \"X\".  The Board class should have a class variable called valid_moves, with the value:\n```python\n   valid_moves=[\"upper left\", \"upper center\", \"upper right\", \"middle left\", \"center\", \"middle right\", \"lower left\", \"lower center\", \"lower right\"]\n```\n4. Add a `__str__()` method.  This converts the board into a displayable string.  You want it to show the current state of the game. The rows to be displayed are separated by newlines (\"\\n\") and you also want some \"|\" amd \"-\" characters.  Once you have created this method, you can display the board by doing a `print(board)`.\n4. Add a move() method.  This has two arguments, `self` and `move_string`.  The following strings are valid in TicTacToe: \"upper left\", \"upper center\", \"upper right\", \"middle left\", \"center\", \"middle right\", \"lower left\", \"lower center\", and \"lower right\".  When a string is passed, the move() method will check if it is one of these, and if not it will raise a TictactoeException with the message \"That's not a valid move.\".  Then the move() method will check to see if the space is taken.  If so, it will raise an exception with the message \"That spot is taken.\"  If neither is the case, the move is valid, the corresponding entry in board_array is updated with X or O, and the turn value is changed from X to O or from O to X.  It also updates last_move, which might make it easier to check for a win.\n5. Add a whats_next() method.  This will see if the game is over.  If there are 3 X's or 3 O's in a row, it returns a tuple, where the first value is True and the second value is either \"X has won\" or \"O has won\".  If the board is full but no one has won, it returns a tuple where the first value is True and the second value is \"Cat's Game\".  Otherwise, it returns a tuple where the first value is False and the second value is either \"X's turn\" or \"O's turn\".\n6. Implement the game within the mainline code of `tictactoe.py`.  At the start of the game, an instance of the board class is created, and then the methods of the board class are used to progress through the game.  Use the `input()` function to prompt for each move, indicating whose turn it is.  Note that you need to call board.move() within a try block, with an except block for TictactoeException.  Give appropriate information to the user.\n7. Test your program by playing a few games.\n\nOn assembling this program, the assignment author found that it was too time consuming to write some of the methods.  So, here are some pieces to reuse.  Please make sure you understand them.\n```python\n    def __str__(self):\n        lines=[]\n        lines.append(f\" {self.board_array[0][0]} | {self.board_array[0][1]} | {self.board_array[0][2]} \\n\")\n        lines.append(\"-----------\\n\")\n        lines.append(f\" {self.board_array[1][0]} | {self.board_array[1][1]} | {self.board_array[1][2]} \\n\")\n        lines.append(\"-----------\\n\")\n        lines.append(f\" {self.board_array[2][0]} | {self.board_array[2][1]} | {self.board_array[2][2]} \\n\")\n        return \"\".join(lines)\n    \n    def move(self, move_string):\n        if not move_string in Board.valid_moves:\n            raise TictactoeException(\"That's not a valid move.\")\n        move_index = Board.valid_moves.index(move_string)\n        row = move_index // 3 # row\n        column = move_index % 3 #column\n        if self.board_array[row][column] != \" \":\n            raise TictactoeException(\"That spot is taken.\")\n        self.board_array[row][column] = self.turn\n        if self.turn == \"X\":\n            self.turn = \"O\"\n        else:\n            self.turn = \"X\"\n    \n    def whats_next(self):\n        cat = True\n        for i in range(3):\n            for j in range(3):\n                if self.board_array[i][j] == \" \":\n                    cat = False\n                else:\n                    continue\n                break\n            else:\n                continue\n            break\n        if (cat):\n            return (True, \"Cat's Game.\")\n        win = False\n        for i in range(3): # check rows\n            if self.board_array[i][0] != \" \":\n                if self.board_array[i][0] == self.board_array[i][1] and self.board_array[i][1] == self.board_array[i][2]:\n                    win = True\n                    break\n        if not win:\n            for i in range(3): # check columns\n                if self.board_array[0][i] != \" \":\n                    if self.board_array[0][i] == self.board_array[1][i] and self.board_array[1][i] == self.board_array[2][i]:\n                        win = True\n                        break\n        if not win:\n            if self.board_array[1][1] != \" \": # check diagonals\n                if self.board_array[0][0] ==  self.board_array[1][1] and self.board_array[2][2] == self.board_array[1][1]:\n                    win = True\n                if self.board_array[0][2] ==  self.board_array[1][1] and self.board_array[2][0] == self.board_array[1][1]:\n                    win = True\n        if not win:\n            if self.turn == \"X\": \n                return (False, \"X's turn.\")\n            else:\n                return (False, \"O's turn.\")\n        else:\n            if self.turn == \"O\":\n                return (True, \"X wins!\")\n            else:\n                return (True, \"O wins!\")\n```\n\n---\n\n\n### Submit Your Assignment on GitHub**  \n\nüìå **Follow these steps to submit your work:**  \n\n#### **1Ô∏è‚É£ Add, Commit, and Push Your Changes**  \n- Within your python_homework folder, do a git add and a git commit for the files you have created, so that they are added to the `assignment3` branch.\n- Push that branch to GitHub. \n\n#### **2Ô∏è‚É£ Create a Pull Request**  \n- Log on to your GitHub account.\n- Open your `python_homework` repository.\n- Select your `assignment3` branch.  It should be one or several commits ahead of your main branch.\n- Create a pull request.\n\n#### **3Ô∏è‚É£ Submit Your GitHub Link**  \n- Your browser now has the link to your pull request.  Copy that link. \n- Paste the URL into the **assignment submission form**. \n\n---",
            "codeExample": "```python\n   valid_moves=[\"upper left\", \"upper center\", \"upper right\", \"middle left\", \"center\", \"middle right\", \"lower left\", \"lower center\", \"lower right\"]\n```\n\n```python\n    def __str__(self):\n        lines=[]\n        lines.append(f\" {self.board_array[0][0]} | {self.board_array[0][1]} | {self.board_array[0][2]} \\n\")\n        lines.append(\"-----------\\n\")\n        lines.append(f\" {self.board_array[1][0]} | {self.board_array[1][1]} | {self.board_array[1][2]} \\n\")\n        lines.append(\"-----------\\n\")\n        lines.append(f\" {self.board_array[2][0]} | {self.board_array[2][1]} | {self.board_array[2][2]} \\n\")\n        return \"\".join(lines)\n    \n    def move(self, move_string):\n        if not move_string in Board.valid_moves:\n            raise TictactoeException(\"That's not a valid move.\")\n        move_index = Board.valid_moves.index(move_string)\n        row = move_index // 3 # row\n        column = move_index % 3 #column\n        if self.board_array[row][column] != \" \":\n            raise TictactoeException(\"That spot is taken.\")\n        self.board_array[row][column] = self.turn\n        if self.turn == \"X\":\n            self.turn = \"O\"\n        else:\n            self.turn = \"X\"\n    \n    def whats_next(self):\n        cat = True\n        for i in range(3):\n            for j in range(3):\n                if self.board_array[i][j] == \" \":\n                    cat = False\n                else:\n                    continue\n                break\n            else:\n                continue\n            break\n        if (cat):\n            return (True, \"Cat's Game.\")\n        win = False\n        for i in range(3): # check rows\n            if self.board_array[i][0] != \" \":\n                if self.board_array[i][0] == self.board_array[i][1] and self.board_array[i][1] == self.board_array[i][2]:\n                    win = True\n                    break\n        if not win:\n            for i in range(3): # check columns\n                if self.board_array[0][i] != \" \":\n                    if self.board_array[0][i] == self.board_array[1][i] and self.board_array[1][i] == self.board_array[2][i]:\n                        win = True\n                        break\n        if not win:\n            if self.board_array[1][1] != \" \": # check diagonals\n                if self.board_array[0][0] ==  self.board_array[1][1] and self.board_array[2][2] == self.board_array[1][1]:\n                    win = True\n                if self.board_array[0][2] ==  self.board_array[1][1] and self.board_array[2][0] == self.board_array[1][1]:\n                    win = True\n        if not win:\n            if self.turn == \"X\": \n                return (False, \"X's turn.\")\n            else:\n                return (False, \"O's turn.\")\n        else:\n            if self.turn == \"O\":\n                return (True, \"X wins!\")\n            else:\n                return (True, \"O wins!\")\n```",
            "_id": "68f812adc22606ecfa5c72d9"
          }
        ],
        "submissionInstructions": "Please submit on time",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Introduction",
          "content": "# Lesson 3 ‚Äî More Python Skills",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72cd"
        },
        {
          "subsectionOrder": 2,
          "title": "Lesson Overview",
          "content": "**Learning objective:** In this lesson, students will learn and apply key advanced Python concepts including object-oriented programming, decorators, list comprehensions, and closures. They will learn how to write cleaner, more modular code using these features, and gain insight into how such patterns are used in real-world frameworks like Dash.\n\n### Topics\n1. Object-oriented programming (OOP)\n2. Decorators\n3. List comprehensions\n4. Closures\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72ce"
        },
        {
          "subsectionOrder": 3,
          "title": "Object-oriented programming in Python",
          "content": "Everything in Python is an object, and objects are instances of *classes*. That means when you create a variable like a string, list, or even a function ‚Äî you‚Äôre actually creating an object. Before finishing Python 100, it's worth understanding what this actually means.  \n\nUntil now we have been following principles of *functional programming*: writing standalone functions that take in inputs, return outputs, and typically don't remember anything between calls. But sometimes we want to bundle together data and the functions -- called *methods* -- that operate on that data. This is the core idea of object-oriented programming (OOP). This bundling is called *encapsulation*, and helps keep related code organized in one place. If you are creating a new data type, OOP lets you define not only what that data type is, but what you can do with it. \n\n> If you want to go a little deeper, there is a nice overview of OOP at [Real Python](https://realpython.com/python3-object-oriented-programming/). \n\n### Basic class definition\nLet‚Äôs look at a very simple example ‚Äî a class that represents dogs:\n\n```Python\nclass Dog:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def call_dog(self):\n        print(f\"Come here, {self.name}!\")\n\n    def speak(self):\n        print(\"bark bark bark\")\n\ndog1 = Dog(\"Spot\", 2)\ndog1.call_dog()\ndog1.speak()\nprint(f\"dog1's name is {dog1.name}.\")\n\ndog2 = Dog(\"Wally\", 4)\ndog2.call_dog()\ndog2.age += 1\nprint(dog2.age)\n```\n\nIn the above, we have:\n- The declaration of a class named `Dog`.  Unlike functions, class names are capitalized in Python.\n- An initialization method `__init__()` that runs automatically when you create a new dog. Here, the method is used to set the initial values of the object's attributes (`name` and `age`). \n- You can access an object's attributes using *dot notation*: this includes both instance variables and methods.\n- The object's instance variables `self.name` and `self.age` store data for each instance of the class.\n- The two methods, `call_dog()` and `speak()` are always passed the value  `self`, which gives them access to attributes like `name` or `age`. \n- Notice that we were able to modify `dog2`'s age -- there is nothing truly private about the data stored in an object's attributes. We will say more about this below. \n\n#### What is `self`?\nWithin a class definition, `self` refers to the current instance of the class ‚Äî the specific object the method is being called on. It can be a little confusing because when *defining* a method it is always the first parameter, but you don‚Äôt actually pass it in as a parameter when invoking the method; Python does that automatically behind the scenes. For instance, above we have `dog1.call_dog()`, which doesn't include the `self` argument explicitly. \n\n### Expanding the class: class attributes and class methods\nWe can add more bells and whistles to our simple Dog class. For example, suppose we want to count how many dogs have been created. Instead of storing that information in each individual dog, we can store it once at the class level:\n\n```Python\nclass Dog:\n    _count = 0  \n\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n        Dog._count += 1 \n\n    def call_dog(self):\n        print(f\"Come here, {self.name}!\")\n\n    def speak(self):\n        print(\"bark bark bark\")\n\n    @classmethod\n    def get_dog_count(cls):\n        return cls._count\n\n# Create a couple of dogs\ndog1 = Dog(\"Spot\", 2)\ndog1.call_dog()\ndog1.speak()\nprint(f\"dog1's name is {dog1.name}.\")\n\ndog2 = Dog(\"Wally\", 4)\ndog2.call_dog()\ndog2.age += 1\nprint(f\"dog2's new age: {dog2.age}\")\n\nprint(f\"Total dogs created: {Dog.get_dog_count()}\")  \n```\nThere are quite a few new things going on here:\n- The variable `_count` is a *class variable*. Unlike the instance variables like `name` and `age`, it belongs to the class itself ‚Äî not any individual dog. This means it is shared across all `Dog` objects. You can tell it is a class variables because it is defined outside the `__init__()` method.\n- The `_count` variable starts with a single underscore to signal that it‚Äôs intended for internal use. This is a common Python convention: it doesn‚Äôt make the variable truly private (it is still accessible), but it tells developers that it isn't meant to be directly accessed and modified. \n- The method `get_dog_count()` is a *class method*, declared using the `@classmethod` decorator (we will talk more about decorators below). This tells Python that the method operates on the `Dog` class rather than individual objects.\n- Instead of taking `self` as the first parameter,  `get_dog_count()` takes in `cls` (short for *class*). `cls` allows us to access the class-level variable `_count`.\n\n### Class inheritance\nSuppose you want to create a class that‚Äôs similar to `Dog`, but with a few differences ‚Äî maybe a bigger bark, or a new behavior like fetching. Instead of rewriting everything from scratch, Python (and OOP in general) lets you *inherit* from an existing class and customize only the parts you want. This is called *class inheritance*.\n\nHere's an example where we create a class `BigDog` that explicitly inherits from `Dog`:\n\n```Python\nclass BigDog(Dog): # inherits from Dog\n    def __init__(self, name, age): \n        # Call the parent class's __init__ to set name/age\n        super().__init__(name, age) \n\n    def fetch(self):\n        print(\"Got it.\")\n\n    def speak(self):\n        print(\"Woof Woof Woof\") # overrides Dog.speak()\n\n    def speak_verbose(self):\n        # call Dog.speak(), then BigDog.speak()\n        super().speak()\n        self.speak()\n\ndog3 = BigDog(\"Butch\", 3)\ndog3.call_dog()\ndog3.speak()\ndog3.speak_verbose()\n```\n\nHere, the `BigDog` class inherits from the `Dog` class, meaning it gets all of `Dog`‚Äôs methods and attributes unless explicitly changed:\n- The `call_dog()` method is inherited from `Dog`, because we didn‚Äôt override it.\n- The `speak()` method is overridden to make big dogs sound different.\n- The `speak_verbose()` method shows how to call both the original `Dog.speak()` (using `super()`) and `BigDog.speak()`.\n\nYou might try creating a `ShyDog` class that overrides `speak()` to say nothing unless prompted by giving it a treat with a method `give_treat()`.\n\n### A few other facts about classes\nA useful attribute of every class and instance is `__dict__`.\n\n```python\nprint(Dog.__dict__) # prints attributes and methods for the Dog class\nprint(dog1.__dict__) # prints the attributes of the instance and their values.\n```\n\nYou can also create classes from system classes:\n\n```Python\nclass Shout(str):\n   def __new__(cls, content):\n      return str.__new__(cls, content.upper())\n\nx = Shout(\"hello there\")\nprint(x) # prints HELLO THERE\n```\nIn this case, the subclass overrides the `__new__` method of the `str` class, and not the `__init__` method, because strings are immutable. \n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72cf"
        },
        {
          "subsectionOrder": 4,
          "title": "Introduction to Decorators",
          "content": "We‚Äôve already seen some decorators like `@classmethod` in our class definitions. But what are these things? A decorator is syntactic sugar that says, ‚ÄúTake this function or class, and pass it through another function to modify it.‚Äù You will probably *use* decorators more than you write them in Python, so let's see them in action to see how useful they can be.   \n\n#### Examples of decorators\nPython provides lots of built-in decorators. We've already seen one:\n\n```Python\n@classmethod\ndef get_dog_count(cls):\n```\nThis tells Python: ‚ÄúDon‚Äôt treat `get_dog_count()` like a normal method. Treat it as a method that applies to the class itself.‚Äù That‚Äôs all a decorator is doing: changing the behavior of a function or method, without you having to rewrite that function.\n\nThere are *many* useful built-in decorators. Sometimes, we want a method in a class to act like an attribute ‚Äî it does a calculation, but we want to access it without parentheses:\n\n```Python\nclass Circle:\n    def __init__(self, radius):\n        self.radius = radius\n\n    @property\n    def area(self):\n        return 3.14 * self.radius ** 2\n\n    @property\n    def diameter(self):\n        return 2 * self.radius\n```\n\nNow you can access those properties (area and diameter) as if they were attributes of the object, using dot notation: \n\n```Python\nc = Circle(3)\nprint(c.area)    \nprint(c.diameter)  \n```\nAnother nice feature is that while `area` and `diameter` are calculated using methods, if you change the `radius`, they will automatically repopulate with the correct values without you having to re-run those methods:\n\n```Python\nc.radios = 5\nprint(c.area)\n```\nYou didn't have to re-assign anything: `area` (and `diamater`) are always recalculated based on the current radius. \n\n`@property` and `@classmethod` are built-in Python decorators -- Python itself provides this functionality. Below, we will discuss how to write your own decorators. \n\nSo far we‚Äôve seen decorators used with functions and methods, like `@property` and `@classmethod`. However, decorators can also be used with *classes*. One useful example is the built-in `@dataclass` decorator, which lets you quickly and conveniently define simple classes for storing structured data, without needing an `__init__` method.\n\n```Python\nfrom dataclasses import dataclass\n\n@dataclass\nclass Book:\n    title: str\n    author: str\n\nbook1 = Book(\"Dune\", \"Frank Herbert\")\nbook2 = Book(\"Dune\", \"Frank Herbert\")\nbook3 = Book(\"Neuromancer\", \"William Gibson\")\n\nprint(book1)\nprint(book2.title)          \nprint(book1 == book2) # True ‚Äî same data, so considered equal\nprint(book1 == book3) # False ‚Äî different data\n```\nThere are a few things to notice about `dataclass` objects:\n- You didn‚Äôt have to write an `__init__` method or any other methods for the class: Python did it for you behind the scenes. You just declare the attribute names and their data types.\n- You can access the attributes of a book object using standard dot notation (`book2.title`). \n- You can check if two different books are the same with `book1 == book2` without having to define your own `__eq__()` operator.  \n\n### Writing your own decorators\nPython classifies functions as first-class objects, which means you can treat them just like any other variable or object. That is, you can pass them as arguments to other functions. For instance:\n\n```Python\ndef say_hello():\n    print(\"Hello!\")\n\ndef repeat_me(func, num_repeats):\n    for _ in range(num_repeats):\n        func()\n\nrepeat_me(say_hello, 5)  # Will print \"Hello!\" 5x\n```\nThis simple example demonstrates how you can pass around functions such as `say_hello()` and call them dynamically in your scripts. \n\nDecorators take advantage of this, and make such function application cleaner and easier to read. Let's look at a simple example, your first hand-made decorator: \n\n```python\ndef my_decorator(func):\n    def wrapper():\n        print (\"Hello!\")\n        func()\n        print (\"World!\")\n    return wrapper\n\n@my_decorator\ndef print_name():\n    print(\"John\")\n\nprint_name()\n```\n\nThe output of `print_name()`, which is modified with the decorator, will be:\n```\nHello!\nJohn\nWorld!\n```\nWhat's happening here?\n- The decorator function `my_decorator()` takes a function (`func`) as input. \n- Inside this function, you define a new function called `wrapper()` that does three things: it prints \"Hello!\", calls the original function `func()` (which prints the name \"John\"), and then prints \"World!\". \n- The decorator returns the new `wrapper()` function that includes this extra behavior that is wrapped around `func()`. \n- When you add the `@my_decorator` above the `print_name()` function declaration, Python runs the following behind the scenes:\n\n```Python\nprint_name = my_decorator(print_name)\n```\nDecorators let you add behavior to a function without modifying its original code. You didn‚Äôt have to touch `print_name()` -- you just wrapped it.\n\nIn this example the function is fairly trivial, however in the next section we can dig into some more useful examples. \n\n### Decorator that will work with any function\n\nHere is an example of a decorator that allows us to benchmark different sections of code. We need to allow all functions to go into this decorator, and it will print out the time it took for a function to complete.\n```python\nimport time\n\ndef timer(func):\n    ## Output the time the inner function takes\n    def wrapper_timer(*args, **kwargs):\n        start_time = time.perf_counter()\n        value = func(*args, **kwargs)\n        end_time = time.perf_counter()\n        run_time = end_time - start_time\n        print (f\"Finished in {run_time:.4f} secs\")\n        return value\n    return wrapper_timer\n\n@timer\ndef wait_half_second():\n    time.sleep(0.5)\n    return \"Done\"\n\nwait_half_second()\n```\nThis provides a useful way to wrap any function to see how long it takes to run (here we are wrapping a function that simply waits for a half second to demonstrate how the decorator works). \n\nIn more detail:\n- `@timer` decorates the function `wait_half_second`.\n- Inside the wrapper, we:\n  - Record the start time\n  - Call the original function (`func(*args, **kwargs)`)\n  - Record the end time and complute the elapsed time.\n  - Print the elapsed time.\n- Why did we use `*args` and `**kwargs`? Those let the decorator work with *any* function, no matter how many arguments it takes. `*args` captures positional arguments, while `**kwargs` captures keyword arguments. This flexibility makes the wrapper reusable for any function. Go ahead and try it for other functions.\n  \n\n### Decorator with arguments: decorator factories\nSometimes you want to pass *arguments* into a decorator ‚Äî like how many times to repeat something, or what prefix to add to a message.  In this case, you need two levels of wrapping. This is because decorators are called with just one argument: the function being decorated. If you want to pass extra arguments, you need a *decorator factory* ‚Äî a function that creates a customized decorator based on the parameters you provide. \n\nLet's look at an example where we allow users to specify how many times a message is repeated, and what prefix to add:\n\n```Python\ndef repeat_with_prefix(prefix, num_repeats): # decorator factory\n    def decorator(func):  # The decorator: takes the function\n        def wrapper(*args, **kwargs):  # The wrapper: runs the function with extra behavior\n            for _ in range(num_repeats):\n                result = func(*args, **kwargs)  # Call the original function\n                print(f\"{prefix} {result}\")\n        return wrapper\n    return decorator\n\n@repeat_with_prefix(\">>\", 3)  \ndef greet(name):\n    return f\"Hello, {name}!\"\n\ngreet(\"Amanda\")\n```\nBreaking down the above. When you run `greet(\"Amanda\")` you get:\n\n```Python\n>> Hello, Amanda!\n>> Hello, Amanda!\n>> Hello, Amanda!\n```\nHere's what happened behind the scenes:\n- The decorator factory is called: `@repeat_with_prefix(\">>\", 3)` creates and returns a decorator function customized with the appropriate arguments.\n- The decorator wraps the function: The `decorator` function (returned by the factory) is applied to `greet`.\n- The wrapper runs the function: When you call `greet(\"Amanda\")`,  the wrapper function uses the `prefix` and `num_repeats` provided by the factory to add the prefix to each output and print the result (`num_repeat` times)\n\nFeel free to modify the code or the parameters (e.g., change \">>\" to \"**\" or 3 to 5) to see how the decorator‚Äôs behavior changes. Parameterized decorators are a powerful but advanced concept, so take your time to experiment and build your intuition.\n\n### Callback to Dash application\nDecorators are often used in another way.  They register a function that is to be called by system code.  For example, in the lesson on Dash, you had the following lines:\n\n```Python\n@app.callback(\n    Output(\"stock-price\", \"figure\"),\n    [Input(\"stock-dropdown\", \"value\")]\n)\ndef update_graph(symbol):\n```\nThe `app.callback` method runs before `update_graph()` is ever called.  It records the fact that `update_graph()`, as wrappered by the `app.callback` wrapper function, is to be called whenever the stock dropdown changes.  You could do something like this, registering the functions to be wrapped, as follows:\n\n```Python\ncallback_dict={}\n\ndef wrap_output(before, after,greeting_type):\n    def decorator_wrap_output(func):\n        callback_dict[greeting_type] = func\n        def wrapper(*args, **kwargs):\n            result = func(*args, **kwargs)\n            return before + result + after\n        callback_dict[greeting_type]=wrapper\n        return wrapper\n    return decorator_wrap_output\n\n@wrap_output(\"begin:\", \":end\",\"for_hello\")\ndef hello():\n    return \"Hello, World!\"\n\n@wrap_output(\"begin:\", \":end\",\"for_goodbye\")\ndef goodbye():\n    return \"Goodbye, World!\"\n\nprint(callback_dict[\"for_hello\"]()) # Will print \"begin:Hello, World!:end\"\n\nprint(callback_dict[\"for_goodbye\"]()) # Will print \"begin:Goodbye, World!:end\"\n```\n\nHere, the decorated functions, in their wrappered form, get registered for callbacks in `callback_dict`.  This is like what Dash is doing when you use the decorator `@app.callback`.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72d0"
        },
        {
          "subsectionOrder": 5,
          "title": "Python List Comprehensions",
          "content": "### **Overview**  \nA list comprehension is a fast and Pythonic way to generate a list.  For example, suppose you want a list of the integers from 0 to 19.  You could do\n\n```Python\ninteger_list=[]\nfor x in range(20):\n    integer_list.append(x)\n```\nbut, with Python, you can use a list comprehension as a shorthand:\n```Python\ninteger_list = [x for x in range(20)]\n```\nOr, to get just the odd ones:\n```Python\nodd_list = [x for x in range(20) if x%2 != 0]\n```\nOr, to get the squares of the odd ones:\n```Python\nodd_squares_list = [x**2 for x in range(20) if x%2 !=0]\n# or\nodd_squares_list = [x**2 for x in odd_list]\n```\n\n### **Generator Expressions**\n\nA generator expression is just like a list comprehension, except that you use parentheses instead of square brackets.  A generator expression is an `iterable`, meaning you can use it in a `for` loop.  Like so:\n\n```python\nodd_squares_generator = (x**2 for x in range(20) if x%2 !=0)\n\nfor y in odd_squares_generator:\n    print(y)\n\n```\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72d1"
        },
        {
          "subsectionOrder": 6,
          "title": "Python Closures",
          "content": "A Python closure is a way of wrappering information by returning a function that has access to that information.  This provides some protection for the stuff you wrapper.  For example:\n\n```Python\ndef make_secret(secret):\n    def did_you_guess(guess):\n        if guess == secret:\n            print(\"You got it!\")\n        else:\n            print(\"Nope\")\n    return did_you_guess\n\ngame1 = make_secret(\"swordfish\")\ngame2 = make_secret(\"magic\")\n\ngame1(\"magic\") # Prints nope\ngame1(\"swordfish\") # Prints you got it\ngame2(\"magic\") # Prints you got it\n```\n\nOf course, the wrappered function could also store data, but you may need the `nonlocal` keyword.  This makes the variable still wrappered within the outer function, but accessible within the inner function:\n```Python\ndef make_secret(secret):\n    bad_guesses = 0\n    def did_you_guess(guess):\n        nonlocal bad_guesses\n        if guess == secret:\n            print(\"You got it!\")\n        else:\n            bad_guesses+=1\n            print(f\"Nope, bad guesses: {bad_guesses}\")\n    return did_you_guess\n\ngame1 = make_secret(\"swordfish\")\ngame1(\"magic\") # Prints nope, bad guesses 1\ngame1(\"magic\") # Prints nope, bad guesses 2\n```\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72d2"
        },
        {
          "subsectionOrder": 7,
          "title": "Summary",
          "content": "In this lesson, you learned:\n1. Declaring and Using Custom Python Classes\n2. How to use Python Decorators\n3. How to use Python List Comprehensions\n4. Closures in Python",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72d3"
        }
      ]
    },
    {
      "id": "68f812adc22606ecfa5c72db",
      "lessonNumber": 4,
      "title": "Lesson 4 ‚Äî Intro to Data Engineering",
      "status": "pending",
      "assignment": {
        "title": "Assignment for Lesson 4",
        "objective": "### Data Analysis and Manipulation with Pandas\n\n### **Objective:**\nIn this assignment, you will explore the basic functionalities of the Pandas library in Python, including creating, manipulating, inspecting, and analyzing data using various DataFrame methods. The goal is to understand how to handle data efficiently and perform essential operations to inspect and analyze datasets.\n\n### **Step 1: Complete the Coding Tasks**  \n\nHomework for this assignment is created within your `python_homework` folder.  Create an `assignment4` branch and change to the `assignment4` folder.  Write your code in `assignment4.py`.  As with the previous lessons, you will run unit tests on the assignment `pytest -v -x assignment4-test.py`.\n\n---\n\n### **Tasks:**",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [
          {
            "taskNumber": 1,
            "title": "Introduction to Pandas - Creating and Manipulating DataFrames",
            "description": "1. **Create a DataFrame from a dictionary:**\n   - Use a dictionary containing the following data:\n     - `Name`: ['Alice', 'Bob', 'Charlie']\n     - `Age`: [25, 30, 35]\n     - `City`: ['New York', 'Los Angeles', 'Chicago']\n   - Convert the dictionary into a DataFrame using Pandas.\n   - Print the DataFrame to verify its creation.\n   - save the DataFrame in a variable called `task1_data_frame` and run the tests.\n\n2. **Add a new column:**\n   - Make a copy of the dataFrame you created named `task1_with_salary` (use the `copy()` method)\n   - Add a column called `Salary` with values `[70000, 80000, 90000]`.\n   - Print the new DataFrame and run the tests.\n\n3. **Modify an existing column:**\n   - Make a copy of `task1_with_salary` in a variable named `task1_older`\n   - Increment the `Age` column by 1 for each entry.\n   - Print the modified DataFrame to verify the changes and run the tests.\n\n4. **Save the DataFrame as a CSV file:**\n   - Save the `task1_older` DataFrame to a file named `employees.csv` using ```to_csv()```, do not include an index in the csv file.\n   - Look at the contents of the CSV file to see how it's formatted.\n   - Run the tests.",
            "codeExample": "```to_csv()```",
            "_id": "68f812adc22606ecfa5c72e1"
          },
          {
            "taskNumber": 2,
            "title": "Loading Data from CSV and JSON",
            "description": "1. **Read data from a CSV file:**\n   - Load the CSV file from Task 1 into a new DataFrame saved to a variable `task2_employees`.\n   - Print it and run the tests to verify the contents.\n\n2. **Read data from a JSON file:**\n   - Create a JSON file (`additional_employees.json`).  The file adds two new employees.  Eve, who is 28, lives in Miami, and has a salary of 60000, and Frank, who is 40, lives in Seattle, and has a salary of 95000.\n   - Load this JSON file into a new DataFrame and assign it to the variable `json_employees`.\n   - Print the DataFrame to verify it loaded correctly and run the tests.\n\n3. **Combine DataFrames:**\n   - Combine the data from the JSON file into the DataFrame Loaded from the CSV file and save it in the variable `more_employees`.\n   - Print the combined Dataframe and run the tests.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72e2"
          },
          {
            "taskNumber": 3,
            "title": "Data Inspection - Using Head, Tail, and Info Methods",
            "description": "1. **Use the `head()` method:**\n   - Assign the first three rows of the `more_employees` DataFrame to the variable `first_three`\n   - Print the variable and run the tests.\n\n2. **Use the `tail()` method:**\n   - Assign the last two rows of the `more_employees` DataFrame to the variable `last_two`\n   - Print the variable and run the tests.\n\n3. **Get the `shape` of a DataFrame**\n   - Assign the shape of the `more_employees` DataFrame to the variable `employee_shape`\n   - Print the variable and run the tests \n\n4. **Use the `info()` method:**\n   - Print a concise summary of the DataFrame using the `info()` method to understand the data types and non-null counts.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72e3"
          },
          {
            "taskNumber": 4,
            "title": "Data Cleaning",
            "description": "1. Create a DataFrame from `dirty_data.csv` file and assign it to the variable `dirty_data`.\n   - Print it and run the tests.\n   - Create a copy of the dirty data in the varialble `clean_data` (use the `copy()` method).  You will use data cleaning methods to update `clean_data`.\n\n2. Remove any duplicate rows from the DataFrame\n   - Print it and run the tests.\n\n3. Convert `Age` to numeric and handle missing values\n   - Print it and run the tests.\n\n4. Convert `Salary` to numeric and replace known placeholders (`unknown`, `n/a`) with NaN\n   - print it and run the tests.\n\n5. Fill missing numeric values (use `fillna`).  Fill `Age` which the mean and `Salary` with the median\n   - Print it and run the tests\n\n6. Convert `Hire Date` to `datetime`\n   - Print it and run the tests\n\n7. Strip extra whitespace and standardize `Name` and `Department` as uppercase\n   - Print it and run the tests\n\n\n---\n\n### **Step 2: Submit Your Assignment on GitHub**  \n\n**Follow these steps to submit your work:**  \n\n#### **1Ô∏è‚É£ Add, Commit, and Push Your Changes**  \n- Within your python_homework folder, do a git add and a git commit for the files you have created, so that they are added to the `assignment4` branch.\n- Push that branch to GitHub. \n\n#### **2Ô∏è‚É£ Create a Pull Request**  \n- Log on to your GitHub account.\n- Open your `python_homework` repository.\n- Select your `assignment4` branch.  It should be one or several commits ahead of your main branch.\n- Create a pull request.\n\n#### **3Ô∏è‚É£ Submit Your GitHub Link**  \n- Your browser now has the link to your pull request.  Copy that link. \n- Paste the URL into the **assignment submission form**.  \n\n---",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72e4"
          }
        ],
        "submissionInstructions": "Please submit on time",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Introduction",
          "content": "# Lesson 4 ‚Äî Intro to Data Engineering",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72dc"
        },
        {
          "subsectionOrder": 2,
          "title": "Lesson Overview",
          "content": "**Learning objective:** Students will learn to load, preview, and inspect datasets in Pandas by reading data from common formats and summarizing data structure to facilitate efficient data analysis.\n\nTopics:\n\n  * Introduction to Pandas: Creating and manipulating DataFrames.\n  * Loading Data: Reading data from CSV, JSON, and dictionaries.\n  * Data Inspection: Using head(), tail(), info() to inspect datasets.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72dd"
        },
        {
          "subsectionOrder": 3,
          "title": "Intro to Pandas",
          "content": "**Pandas** is a very powerful, open-source library for data analysis and manipulation in Python. It's widely used for handling and analyzing data structures, particularly in tabular format. With Pandas, you can work easily with structured data, perform data cleaning, and conduct complex transformations.  You can read more about pandas [here](https://pandas.pydata.org/docs/index.html).\n\n### Why Use Pandas? \nPandas provides data structures like **DataFrames** and **Series** that make data manipulation in Python simpler and faster. It's well-suited for tasks that involve:\n\n  * Loading data from different file formats (CSV, Excel, SQL, etc.)\n  * Cleaning and transforming messy data\n  * Summarizing data for analysis\n  * Visualizing data with integration to other libraries like Matplotlib and Seaborn\n\n### Getting Started with Pandas\nTypically, to get started, you would do the following.\n\n```bash\npip install pandas\n```\n\n**But this is not necessary when you use your python_homework repository.** When you set up the folder, you installed all of the packages in `requirements.txt`, and that included Pandas.\n\nOnce the package is installed, you can import it in your Python code: \n\n```python\nimport pandas as pd\n```\n\n### The numpy library ###\n\nThe numpy library provides highly optimized datatypes and numerical operations for python.  It is written in c and compiled to binary code which is linked with python.  Numerical computation in python using numpy are competitive with compiled languages like c++ and much faster than native python.  The pandas library is built on top of numpy and numpy numbers are often used with pandas.  You can read more about numpy [here](https://numpy.org/).\n\n### Key Data Structures\n\n1. **Series**: A one-dimensional array-like structure, similar to a list or array, but with added features such as **customizable indexes**. Each element in a Series is associated with a label (the index), allowing for more intuitive data manipulation and access.\n2. **Data Frame:** A two-dimensional table where each column can hold different types of data. This is the most commonly used data structure in Pandas.\n\n#### Example: Creating a Series\n\n**You should run all of the following code examples within the Python interactive shell.**  Start VSCode from within your `python_homework` directory, start a terminal within VSCode, and enter the `python` command to start it. Then run the following code:\n\n```python\n# Creating a simple Series\nimport pandas as pd\n\ndata = [1, 3, 5, 7, 9]\ns = pd.Series(data, name=\"numbers\")\nprint(s)\n```\n\nThe output should be: \n\n```yaml\n0    1\n1    3\n2    5\n3    7\n4    9\nName: numbers, dtype: int64\n```\n\nA Series is a one dimensional data structure.  The column on the left is the index.  The column on the right is the data.\n\n#### Key Features of a Series:\n1. **Customizable Indexes**:\n   - Unlike standard lists or arrays, a Series can have user-defined labels for its indices, making it easier to differentiate and access data.\n   - For example:\n     ```python\n     import pandas as pd # The import only needs to be done once per interactive session\n     data = pd.Series([10, 20, 30], index=[\"a\", \"b\", \"c\"])\n     print(data)\n     # Output:\n     # a    10\n     # b    20\n     # c    30\n     ```\n   - To give a more difficult case, do this one:\n     ```python\n     data2 = pd.Series(['Tom', 'Li', 'Antonio', 'Mary'], index=[5, 2, 2, 3])\n     print(data2)\n     # Output:\n     # 5 Tom\n     # 2 Li\n     # 2 Antonio\n     # 3 Mary\n     print(data2[2])\n     # Output:\n     # 2 Li\n     # 2 Antonio\n     print(data2[1])\n     # This gives a key error!\n     ```\n    - **Notice the following:** Index labels are not necessarily numbers, nor are they in sequential order, and they may not even be unique.  They are **not** the same as the row number.  If some index label is not unique, and you request the value of the series for that index label, what is returned is another series.  This is called `levels` in Pandas.  Try the following:\n     ```python\n     data3 = data2.reset_index()\n     print(data3)\n     # output:\n     # 0 Tom\n     # 1 Li\n     # 2 Antonio\n     # 3 Mary\n     ```\n   - The order of entries in the Series does not change.  In Pandas, Series are value-mutable, meaning that you can change the value stored at a particular location, but are not size or order mutable.  Also, the index labels in a Series are immutable.\n2. Access entries by index label or position\n#### Example: Differentiating a Series from a List\n```python\n# List Example\nmy_list = [10, 20, 30]\nprint(my_list[1])  # Access by position\n# Output: 20\n\n# Series Example\nmy_series = pd.Series([10, 20, 30], index=[\"a\", \"b\", \"c\"])\nprint(my_series[\"b\"])  # Access by index label\n# Output: 20\n\nprint(my_series.iloc[2]) # Access by integer position\n# Output: 30\n```\n\n3. **Operations on an entire series:**\n```python\nmy_revised_series = my_series * 2\nprint(my_revised_series)\n# Output:\na 20\nb 40\nc 60\n```\nThis does not work for lists!  As we will see, there are other ways to change a series, including a map() method and numpy functions.\n\n#### Example: Creating a DataFrame\nDataFrames are like tables in a database or spreadsheet. To create a DataFrame from a dict, run the following code in Python: \n\n```python\n# Creating a DataFrame from a dict\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [24, 27, 22],\n    'City': ['New York', 'San Francisco', 'Chicago']\n}\ndf = pd.DataFrame(data)\nprint(df)\n```\n\nThe output should be: \n\n```markdown\n      Name  Age           City\n0    Alice   24       New York\n1      Bob   27  San Francisco\n2  Charlie   22        Chicago\n```\n\nOne can also create a dataframe from a list of dicts:\n\n```python\ndata_alice = {'Name': 'Alice', 'Age': 24, 'City': 'New York'}\ndata_bob = {'Name': 'Bob', 'Age': 27, 'City': 'San Francisco'}\ndata_charlie = {'Name': 'Charlie', 'Age': 22, 'City': 'Chicago'}\ndf = pd.DataFrame([data_alice, data_bob, data_charlie])\nprint(df)\n# output: same as before\n```\n\n#### Loading data from numpy objects\nIn addition to initialization from python Lists and Dictionaries demonstrated in the examples above, Pandas can be initialized from numpy objects.\n\n```python\nimport numpy as np # load the numpy library\n# Create a Pandas DataFrame using NumPy arrays\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndf = pd.DataFrame(data, columns=['A', 'B', 'C'])\n\nprint(df)\n\n```\n\n#### the DataFrame index and column labels\nDataFrames include an index.  The index provides a label for each row.  Again, this is **not** the same as the row number.  It can be useful for operations such as indexing, data alignment and subsetting.  For some of the operations we will discuss, we add an optional parameter to ignore the index since there is additional complexity involved in setting it up correctly.  For example, the index would need to be reset when combining two DataFrames.  DataFrames also have column labels.  These are typically descriptive strings.  You *can* have non-distinct column labels, but this is usually not helpful.\n\n\n### Common Operations in Pandas\n\n#### Reading Data\nPandas makes it easy to read data from files. For instance, to read data from a CSV file: \n\n```python\n# Read data from a CSV file\ndf = pd.read_csv('data.csv')\n```\n\n#### Combining two DataFrames\nThe `concat` method can be used to combine two DataFrames.\n\n```python\ndata = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [24, 27, 22],\n    'City': ['New York', 'San Francisco', 'Chicago']\n})\n\nmore_data = pd.DataFrame({\n  'Name': ['Fred', 'Barney'],\n  'Age': [57, 55],\n  'City': ['Bedrock', 'Bedrock']\n})\n\ndf = pd.concat([data, more_data], ignore_index=True)\n```\n\n#### Data Selection\nYou can select entries, rows and columns from a DataFrame in various ways. \n```python\n# Select an entry by index label and column\nprint(df.loc[1,'Name'])\n# Output: Bob\n\n# Select an entry by position\nprint(df.iloc[1, 1])\n# Output: 27\n```\n\nIn either case, **you specify the row first, and then the column.**\n\n```python\n# Select a single column\nprint(df['Age'])\n\n# Select multiple columns\nprint(df[['Name', 'City']])\n\n```\nWhen you select columns in this way, you obtain views or references to one or more columns in the DataFrame.\n- Each reference points to a Series.  Each Series has the same index labels as the DataFrame itself.\n- You should regard these views as read/only.  If you try to change the values in one of these series, you might get a warning, and it might not change the value in the original DataFrame.\n- The following is a bad practice:\n  ```python\n  df['Age'][1] = 35\n  ```\n  This changes the value in a view, which is a bad idea.  There are two correct ways to do this.\n  ```python\n  age_series = df['Age'].copy() # creates a new series\n  age_series[1] = 35 # changes the value in the series\n  df['Age'] = age_series # replaces the previous column with the new series\n  ```\n  This approach is long-winded if you are only changing one entry in a column.  More direct is:\n  ```python\n  df.loc[1,'Age'] = 35\n  ```\n\n```python\n# Select rows by index position\nprint(df.iloc[1])  # Select the second row\n\n# Select rows by condition\nprint(df[df['Age'] > 23])\n```\nWhen you select rows in this way, you get one or more Series.  These are copies of the data from the DataFrame.  If you change the values in these Series, that does not affect the original DataFrame.  The index labels for these series are the column labels from the original DataFrame.\n\nThe size, row order, and index labels for a Pandas DataFrame are immutable.  The values and column labels are mutable, and entire columns can be added, removed, or replaced.\n\n### Data Aggregation\nPandas also allows for powerful aggregations, such as finding the mean or sum of a column. \n\n```python\n# Find the average age\nprint(df['Age'].mean())\n\n# Count the number of unique cities\nprint(df['City'].nunique())\n```\n\n### Data Cleaning\nHandling missing data and cleaning data is essential in data analysis.  These are some examples of the data cleaning methods which are available. We will cover data cleaning in more detail in future lessons.\n\n#### Converting Columns to Numeric\n\n```python\nimport pandas as pd\n\ndata = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"Height\": [\"5.5\", \"unknown\", \"5.9\"],  # \"unknown\" is not numeric\n    \"Weight\": [\"60\", \"70\", \"NaN\"]        # \"NaN\" is a missing placeholder\n}\ndf = pd.DataFrame(data)\n\nprint(\"Before conversion:\")\nprint(df)\n\n# Replace placeholders with NaN and convert to numeric\ndf[\"Height\"] = df[\"Height\"].replace(\"unknown\", pd.NA)\ndf[\"Height\"] = pd.to_numeric(df[\"Height\"], errors=\"coerce\")\ndf[\"Weight\"] = pd.to_numeric(df[\"Weight\"], errors=\"coerce\")\n\nprint(\"\\nAfter conversion to numeric:\")\nprint(df)\n\n \n```\n\n#### Handling Missing Values with `fillna()`\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    \"Person\": [\"Alice\", \"Bob\", \"Charlie\", \"Dana\", \"Eve\"],\n    \"Score\": [10, np.nan, 20, None, 25],\n    \"City\": [\"New York\", \"Chicago\", None, \"Boston\", \"NaN\"]\n}\ndf = pd.DataFrame(data)\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Strategy 1: Fill numeric missing values with a fixed number\ndf[\"Score_filled_fixed\"] = df[\"Score\"].fillna(0)\n\n# Strategy 2: Fill numeric missing values with the column mean\nmean_score = df[\"Score\"].mean()  # ignoring NaNs\ndf[\"Score_filled_mean\"] = df[\"Score\"].fillna(mean_score)\n\n# Strategy 3: Fill textual missing values with \"Unknown\"\ndf[\"City_filled\"] = df[\"City\"].replace(\"NaN\", pd.NA).fillna(\"Unknown\")\n\nprint(\"\\nDataFrame after fillna strategies:\")\nprint(df)\n\n\n```\n\n#### Forward fill and backward fill\n\nHandle missing data in a time series or sequential data scenario.\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    \"Day\": [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"],\n    \"Sales\": [100, np.nan, 150, np.nan, 200]\n}\ndf = pd.DataFrame(data)\n\nprint(\"Original Sales Data:\")\nprint(df)\n\n# Forward fill (propagate last valid observation forward)\ndf_ffill = df.copy()\ndf_ffill[\"Sales\"] = df_ffill[\"Sales\"].fillna(method=\"ffill\")\n\n# Backward fill (use next valid observation to fill gaps)\ndf_bfill = df.copy()\ndf_bfill[\"Sales\"] = df_bfill[\"Sales\"].fillna(method=\"bfill\")\n\nprint(\"\\nForward Fill Result:\")\nprint(df_ffill)\n\nprint(\"\\nBackward Fill Result:\")\nprint(df_bfill)\n\n\n```\n\n#### Text Standardization (strip, upper, lower)\n\n```python\nimport pandas as pd\n\ndata = {\n    \"Department\": [\" SALES \", \"   HR\", \"FinanCe  \", \"Sales\", \"MARKETING \"],\n    \"Location\": [\" New York \", \" Boston\", \"Chicago   \", \"  Boston \", \"LOS ANGELES\"]\n}\ndf = pd.DataFrame(data)\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Strip whitespace\ndf[\"Department\"] = df[\"Department\"].str.strip()\ndf[\"Location\"] = df[\"Location\"].str.strip()\n\n# Convert columns to uppercase\ndf[\"Department_upper\"] = df[\"Department\"].str.upper()\ndf[\"Location_upper\"] = df[\"Location\"].str.upper()\n\n# Or lowercase, if you prefer\ndf[\"Department_lower\"] = df[\"Department\"].str.lower()\n\nprint(\"\\nAfter text standardization:\")\nprint(df)\n\n\n```\n\n#### Converting dates to `datetime`\n\n```python\nimport pandas as pd\n\n# Sample data with dates in various formats and some invalid values\ndata = {\n    \"Event\": [\"Project Start\", \"Client Meeting\", \"Beta Release\", \"Final Launch\"],\n    \"Date\": [\"2021/01/15\", \"2021-02-30\", \"03-15-2021\", \"April 31, 2021\"]  # Some invalid or unusual dates\n}\ndf = pd.DataFrame(data)\n\nprint(\"Before conversion:\")\nprint(df)\n\n# Convert 'Date' to datetime\n# errors=\"coerce\" will turn invalid dates into NaT (Not a Time)\ndf[\"Date_converted\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n\nprint(\"\\nAfter converting to datetime:\")\nprint(df)\n\n# You can check how many values became NaT (invalid dates)\nnum_invalid_dates = df[\"Date_converted\"].isna().sum()\nprint(f\"\\nNumber of invalid dates converted to NaT: {num_invalid_dates}\")\n\n\n```\n\n### Saving DataFrames to CSV Files\n\nPandas makes it easy to save DataFrames as CSV files, which is useful for sharing, storing, or exporting data for later use. You can use the to_csv() method to save your DataFrame to a CSV file\n```python\nDataFrame.to_csv(filepath, sep=',', index=True, header=True, encoding=None)\n```\n```filepath```: The name or path of the CSV file to save (e.g., ```\"output.csv\"```)   \n```sep```: The delimiter to use (default is a comma ```,```).   \n```index```: Whether to include the index as a column in the file (default is ```True```).   \n```header```: Whether to include column names as the first row (default is ```True```).\n\n\n### Save a DataFrame to a CSV File\n```python\nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [26, 31, 36],\n    'City': ['New York', 'Los Angeles', 'Chicago'],\n    'Salary': [70000, 80000, 90000]\n}\ndf = pd.DataFrame(data)\n\n# Save the DataFrame to a CSV file\ndf.to_csv(\"employees.csv\", index=False)\n\nprint(\"DataFrame saved to employees.csv\")\n```\nOutput File (employees.csv):\n\n```sql\nName,Age,City,Salary\nAlice,26,New York,70000\nBob,31,Los Angeles,80000\nCharlie,36,Chicago,90000\n\n```\n### 3.1 Videos: Installing and Using Pandas\n\nIn these two videos, we'll walk through installing and using Pandas in Python. This is an important step! If you're still feeling confused, contact a 1:1 mentor to walk through Pandas.\n\n**[Video: What is Pandas? Why and How to Use in Python](https://youtu.be/dcqPhpY7tWk?feature=shared).**\n\n*Note: The above video demonstrates using Pandas with Jupytr notebook. If you would like a look at using Pandas in VS Code, take time to look at the video below as well.*\n\n**[Video: Installing Pandas in VSCode](https://youtu.be/4WZK0eovQIA?feature=shared).**\n\n### 3.1 Check for Understanding\n\n1. Which data structure is used for storing a two-dimensional table in Pandas?\n  * A) List\n  * B) Array\n  * C) DataFrame\n  * D) Series\n\n<details>\n\n<summary>Answer</summary>\n1. C) DataFrame\n</details>\n\n\n2. How can you select rows in a DataFrame where a specific column value is greater than 10?\n  * A) `df.loc[df['column'] > 10]`\n  * B) `df.loc[df['column'] < 10]`\n  * C) `df['column'] > 10`\n  * D) `df.loc(column > 10)`\n  \n<details>\n\n<summary>Answer</summary>\n2. A) `df.loc[df['column'] > 10]`\n\n</details>\n\nGreat work! With these basics, you can now start using Pandas for various data manipulation and analysis tasks.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72de"
        },
        {
          "subsectionOrder": 4,
          "title": "Loading Data in Pandas",
          "content": "Pandas provides convenient methods for loading data from various sources, such as CSV and JSON files, as well as Python dictionaries. This makes it easy to get your data into a format where you can start analyzing it.\n\n### Reading Data from a CSV File\n\nAs you recall from Lesson 2, **CSV** (Comma-Separated Values) is one of the most common data formats, especially for tabular data. Pandas makes it simple to read and manipulate CSV files.\n\n#### Example: Reading a CSV File\n\nTo load a CSV file, use `pd.read_csv()`.\n\n```python\nimport pandas as pd\n\n# Load the data from a CSV file\ndf = pd.read_csv('data.csv')\nprint(df.head())\n```\n\nHere, `df.head()` will display the first five rows of the DataFrame by default. You can also customize `pd.read_csv()` with various parameters:\n\n```python\n# Loading a CSV file with custom parameters\ndf = pd.read_csv('data.csv', delimiter=';', header=0, index_col='ID')\n```\n\nIn the above example,\n  * `delimiter` defines a custom separator (e.g., `;`)\n  * `header` indicates the row for column headers (defaults to the first row)\n  * `index_col` sets a column to be used as the DataFrame index.\n\n### Reading Data from a JSON File\n\n**JSON** (JavaScript Object Notation) is another popular format, often used for data exchange on the web. Pandas can read JSON files directly into a DataFrame.\n\n#### Example: Reading a JSON File\n\nTo load a JSON file, use `pd.read_json()`. Note that this function looks the same as the one we used for a CSV file, we've just replaced `csv` with `json`.\n\n```python\n# Load the data from a JSON file\ndf = pd.read_json('data.json')\nprint(df.head())\n```\n\nThe JSON structure should be either a list of dictionaries (each representing a row) or a dictionary of lists (each representing a column). Here‚Äôs two examples of JSON data that can be read into a DataFrame:\n\n```json\n[\n    {\"Name\": \"Alice\", \"Age\": 24, \"City\": \"New York\"},\n    {\"Name\": \"Bob\", \"Age\": 27, \"City\": \"San Francisco\"},\n    {\"Name\": \"Charlie\", \"Age\": 22, \"City\": \"Chicago\"}\n]\n```\n\nThe same DataFrame can be read using this JSON structure.  It's a bit simpler since it doesn't repeat the column names.\n\n```json\n{ \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n  \"Age\": [25, 30, 35],\n  \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n}\n```\n\nFor complex JSON structures, you may need to specify additional parameters, or preprocess the JSON data before loading.\n\n\n\n### Using the `sep` Parameter in Pandas\n\nIn **Pandas**, the `sep` parameter is commonly used when reading or writing CSV (or similar) files. It specifies the **delimiter** (separator) used in the file to distinguish between columns.\n\n\n### Common Methods with `sep`\n\n1. **`pd.read_csv()`**: Reads a file and uses the `sep` parameter to define how columns are separated.\n\n2. **`DataFrame.to_csv()`**: Exports a DataFrame and uses `sep` to specify the delimiter in the output file.\n\n\n### Example 1: Reading a CSV File with a Custom Separator\n\nSome CSV files may use delimiters other than a comma, such as tabs (`\\t`) or pipes (`|`).\n\n```python\nimport pandas as pd\n\n# Read a CSV file with pipe as the separator\ndata = pd.read_csv(\"data.csv\", sep=\"|\")\nprint(data.head())\n```\n\nIf the file contains:\n\n```sql\nName|Age|City\nAlice|30|New York\nBob|25|Los Angeles\n```\n\nThe output will be:\n\n```markdown\n    Name  Age         City\n0  Alice   30     New York\n1    Bob   25  Los Angeles\n```\n\n### Example 2: Writing a CSV File with a Custom Separator\n\nYou can export a DataFrame using a custom delimiter instead of the default comma.\n\n```python\n# Create a sample DataFrame\ndf = pd.DataFrame({\n    \"Name\": [\"Alice\", \"Bob\"],\n    \"Age\": [30, 25],\n    \"City\": [\"New York\", \"Los Angeles\"]\n})\n\n# Save the DataFrame to a CSV file with a tab as the separator\ndf.to_csv(\"output.tsv\", sep=\"\\t\", index=False)\n```\n\nThe output file `output.tsv` will look like this:\n\n```sql\nName\tAge\tCity\nAlice\t30\tNew York\nBob\t25\tLos Angeles\n```\n\n#### Why Use `sep`?\n\n* **Flexibility:** Handle files with different delimiters (e.g., tabs, pipes, semicolons).\n* **Compatibility:** Work with non-standard or region-specific CSV formats.\n\n#### Loading Data from a Dictionary\n\nPandas also allows you to create DataFrames from Python dictionaries directly, which is useful when you already have data in a Python program.\n\n#### Example: Loading Data from a Dictionary\n\nIf you have a dictionary where keys represent column names and values represent the data, you can use `pd.DataFrame()` to create a DataFrame.\n\n```python\n# Create a dictionary\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [24, 27, 22],\n    'City': ['New York', 'San Francisco', 'Chicago']\n}\n\n# Convert the dictionary to a DataFrame\ndf = pd.DataFrame(data)\nprint(df)\n```\n\nThis will output: \n\n```markdown\n      Name  Age           City\n0    Alice   24       New York\n1      Bob   27  San Francisco\n2  Charlie   22        Chicago\n```\n\n### Summary of Methods\n\n| Format     | Method           | Example                          |\n|------------|-------------------|----------------------------------|\n| CSV        | `pd.read_csv()`   | `df = pd.read_csv('data.csv')`   |\n| JSON       | `pd.read_json()`  | `df = pd.read_json('data.json')` |\n| Dictionary | `pd.DataFrame()`  | `df = pd.DataFrame(data)`        |\n\n### 3.2 Check for Understanding\n\n1. Which function is used to load data from a JSON file into a DataFrame?\n\n  * A) pd.read_csv()\n  * B) pd.read_json()\n  * C) pd.DataFrame()\n  * D) pd.read_dict()\n<details>\n\n<summary>Answer</summary>\n1. B) `pd.read_json()`\n</details>\n\n2. If your CSV file uses semicolons (;) instead of commas, which parameter can you use to specify this in pd.read_csv()?\n\n  * A) separator\n  * B) delimiter\n  * C) sep\n  * D) Both b and c\n\n<details>\n\n<summary>Answer</summary>\n\n2. D) Both B and C\n\n</details>\n\nWith these functions, you‚Äôre equipped to load data from different formats into Pandas, ready for analysis!",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72df"
        },
        {
          "subsectionOrder": 5,
          "title": "Data Inspection",
          "content": "Once you've loaded data into a DataFrame, it‚Äôs essential to inspect it to understand its structure, spot any missing values, and identify the data types of each column. Pandas provides several convenient methods for quickly viewing and summarizing your dataset.\n\n### Using `head()` and `tail()` to Preview Data\n\nThe `head()` and `tail()` methods allow you to preview the first or last few rows of your DataFrame. By default, they show 5 rows, but you can specify any number you want.\n\n#### Example: Using `head()`\n\n`head()` is typically used to get a quick look at the beginning of the dataset.\n\n```python\nimport pandas as pd\n\n# Load some sample data\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank'],\n    'Age': [24, 27, 22, 32, 29, 41],\n    'City': ['New York', 'San Francisco', 'Chicago', 'Los Angeles', 'Houston', 'Seattle']\n})\n\n# Display the first 3 rows\nprint(df.head(3))\n```\n\nThis will output: \n\n```markdown\n      Name  Age           City\n0    Alice   24       New York\n1      Bob   27  San Francisco\n2  Charlie   22        Chicago\n```\n\n#### Example: Using `tail()`\n\nSimilarly, `tail()` allows you to view the last few rows of the DataFrame.\n\n```python\n# Display the last 2 rows\nprint(df.tail(2))\n```\n\nThis will output: \n\n```\n    Name  Age     City\n4    Eve   29   Houston\n5  Frank   41   Seattle\n```\n\n### Using `info()` to Get a Summary of the DataFrame\n\nThe `info()` method provides a summary of the DataFrame, including: \n  * The number of entries (rows)\n  * The data types of each column\n  * The number of non-null (non-missing) values in each column\n  * Memory usage of the DataFrame\n\n\n#### Example: Using `info()`\n\n```python\n# Get a summary of the DataFrame\ndf.info()\n```\nThis will output: \n\n```python\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6 entries, 0 to 5\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Name    6 non-null      object\n 1   Age     6 non-null      int64 \n 2   City    6 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 272.0 bytes\n```\n\nFrom this output, you can see the column names, data types, number of non-null entries per column, and how much memory the DataFrame consumes. This summary is helpful for understanding the dataset's overall structure.\n\n### Summary of Methods\n\n| Format     | Description           | \n|------------|-------------------|\n| `head()`        | Displays the first few rows of the DataFrame  | \n| `tail()`       | Displays the last five rows of the DataFrame  | \n| `info()` | Summarizes the DataFrame, including data types, null counts, and memory usage  | \n\n### 3.3 Check for Understanding\n\n1. What is the default number of rows displayed when using `df.head()`?\n\n     * A) 3\n     * B) 5\n     * C) 7\n     * D) 10\n <details>\n\n<summary>Answer</summary> \n1. B) 5\n</details>\n  \n2. Which method provides information about column data types and memory usage?\n\n     * A) `head()`\n     * B) `tail()`\n     * C) `summary()`\n     * D) `info()`\n\n<details>\n\n<summary>Answer</summary> \n\n2. D) `info()`\n\n</details>\n\n---\nThis content was written by Janet Zulu, Reid Russom, and CTD volunteers‚Äîwith special thanks to the brain trust of John McGarvey, Rebecca Callari-Kaczmarczyk, Tom Arns, and Josh Sternfeld. To submit feedback, please fill out the **[CTD Curriculum Feedback Form](https://forms.gle/RZq5mav7wotFxyie6)**.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72e0"
        }
      ]
    },
    {
      "id": "68f812adc22606ecfa5c72e6",
      "lessonNumber": 5,
      "title": "Lesson 05 ‚Äî Data Wrangling and Aggregation",
      "status": "pending",
      "assignment": {
        "title": "Assignment for Lesson 5",
        "objective": "### Advanced Data Wrangling and Aggregation with Pandas\n\n### **Objective:**\nThe purpose of this assignment is to deepen your understanding of data wrangling and aggregation using the Pandas library in Python. You will work with sample DataFrames to perform various operations such as filtering, handling missing values, merging, sorting, and transforming.\n\n### **Setup**\n\nThe assignments up to this one have required you to create `.py` files and to submit them by creating pull requests for your python_homework repository.  For this assignment, you create a Jupyter notebook file.  Jupyter notebooks are a way to do data presentation and analysis, using Python code.  A notebook is comprised of a sequence of cells, which come in two kinds: Markdown cells, for putting in the text you want to show, and code cells, where you put your Python code.\n\nWith a little setup, you can create Jupyter notebooks in VSCode, and submit them to GitHub.  However, GitHub is not a friendly environment for collaboration on notebooks.  Your reviewer wants to see the notebooks, to run them, and to give you comments in context.  For that purpose, we use [https://kaggle.com].  That site also has an interesting collection of data sets you can use for practice in data analysis and presentation.  Connect to the site now and register, so that you have an account.\n\n### **Tasks:**",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [
          {
            "taskNumber": 1,
            "title": "Data Selection",
            "description": "1. **On the Kaggle site, click on the plus button in the upper left, and create a notebook called CTD_Assignment_5.**  It comes up with a code cell already present.  Leave that one alone, and click on the plus markdown button to add a cell that says \"Task 1\". You do not have to use markdown formatting directives, however if you do choose to use formatting, it's worth noting that level two headings starting with '## ' are automatically added to the table of contents.  This is how you convey information about your code to your reviewer, Jupyter notebook style.  After adding a markdown cell, click on the plus code button to add another cell.  You add the code for this task to the cell.  As you complete each of the following tasks, run the cell to make sure your code works.  You run the cell by clicking on the arrow at the top left of the cell.\n\n**Note:** The various code cells in a Jupyter notebook are all part of the same program, so you have access to the variables and functions of one cell from each of the ones that follow.  You only need to import Pandas once, for example.  However, Kaggle sessions **time out** if you go to the kitchen for a sandwich or something.  When they time out, your variables go away.  So, if you then run cell 2, which is dependent on something in cell 1, you'll get an error.  To correct this, click on the Run All button at the top of your Kaggle notebook screen, and the entirety of the program runs in the order the cells appear.\n\n2. **Create DataFrames `df1`, `df2`, and `df3` using the provided sample data(feel free to change the values):**\n   - `df1` contains names, ages, and salaries of five employees.\n   - `df2` contains names, ages, and salaries of five other employees.\n   - Display each DataFrame.\n   \n```\ndata1 = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n    'Age': [25, 30, 35, 40, 30],\n    'Salary': [50000, 60000, 70000, 80000, 55000]\n}\n```\n\n```\ndata2 = {\n    'Name': ['Frank', 'Grace', 'Helen', 'Ian', 'Jack'],\n    'Age': [28, 33, 35, 29, 40],\n    'Salary': [52000, 58000, 72000, 61000, 85000]\n}\n\ndata3 = {\n    'Name': ['Frank', 'Helen', 'Ian', 'Hima', 'Chaka'],\n    'Age': [17, 93, 12, 57, 106],\n    'Favorite Color': ['blue', 'pink', 'burgundy', 'red', 'turquoise']\n}\n```\nPrint the resulting dataframes. \n\n3. **Perform the following selection operations on `df1`:**\n   - Select the 'Name' column, and print the result.\n   - Select both 'Name' and 'Salary' columns, and print the result.\n   - Slice the first three rows using integer-based indexing (`iloc`), and print the result.",
            "codeExample": "```\ndata1 = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n    'Age': [25, 30, 35, 40, 30],\n    'Salary': [50000, 60000, 70000, 80000, 55000]\n}\n```\n\n```\ndata2 = {\n    'Name': ['Frank', 'Grace', 'Helen', 'Ian', 'Jack'],\n    'Age': [28, 33, 35, 29, 40],\n    'Salary': [52000, 58000, 72000, 61000, 85000]\n}\n\ndata3 = {\n    'Name': ['Frank', 'Helen', 'Ian', 'Hima', 'Chaka'],\n    'Age': [17, 93, 12, 57, 106],\n    'Favorite Color': ['blue', 'pink', 'burgundy', 'red', 'turquoise']\n}\n```",
            "_id": "68f812adc22606ecfa5c72f1"
          },
          {
            "taskNumber": 2,
            "title": "Data Aggregation",
            "description": "Again, you create a markdown cell to describe this task, and a code cell containing the code for the task.  Do this throughout, whenever you are doing your homework in Jupyter notebooks.\n\n1. **Group `df1` by 'Age' and aggregate the 'Salary' column:**\n   - Calculate the mean, sum, and count of the salary for each age group.\n   - Display the aggregated results.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72f2"
          },
          {
            "taskNumber": 3,
            "title": "Merging and Joining DataFrames",
            "description": "1. **Merge `df1` and `df3` into `df_1_3_merged` on the 'Name' column:**\n   - Use an outer merge to combine the two DataFrames and handle any missing data.\n   - Use the suffixes `_left` and `_right` to differentiate columns from each DataFrame. (You specify `suffixes=['_left','_right']` on the call to merge.)\n   - Display the result with print(). When you do, you will see some annoying warning messages that contain words like \"Runtime Warning: invalid value encountered ...\".  This is because of the NaN values in the frame.  You could suppress these warnings by \n      ```python\n      np.warnings.filterwarnings('ignore')\n      ```\n      But, don't do this yet.  You might see some other warnings if you make a mistake, and you don't want to miss them.  Just ignore these particular warnings.\n   - We see that the 'Salary' column has `NaN` values.  Transform this column to replace `NaN` with the starting salary of 15000.  Hint: fillna() can be used on a Series.\n   - Also, transform the 'Favorite Color' column to replace `NaN` values with 'yellow'.\n   - Display the result.\n   - We have `NaN` for some of the entries in the 'Age_left' column.  For other rows, the `NaN` value is in the 'Age_right' column.  We want to create a new `Age` column, using the values from 'Age_left' if they are not `NaN`, and from 'Age_right' otherwise.  We'll use np.where().  As this is a new technique, the answer is as follows:\n       ```python\n       df_1_3_merged['Age'] = np.where(df_1_3_merged['Age_left'].notna(),df_1_3_merged['Age_left'], df_1_3_merged['Age_right'])\n       # This works like a ternary expression.  If the expression passed to np.where is True, you get the left value, otherwise the right value.\n       ```\n   - Display the result.\n   - Drop the 'Age_left' and 'Age_right' columns and display the result.\n\n2. **Use the Join Method:**\n   - Create new DataFrames df1_b and df3_b from df1 and df3.  In these new DataFrames, set 'Name' as the index.\n   - Join the DataFrames with outer join logic and display the result.  Do not use `inplace=True`.  Unlike the merge method, the join method does not provide default suffixes if there are overlapping columns.  Check the online documentation to find out how to specify them.",
            "codeExample": "```python\n      np.warnings.filterwarnings('ignore')\n      ```\n\n```python\n       df_1_3_merged['Age'] = np.where(df_1_3_merged['Age_left'].notna(),df_1_3_merged['Age_left'], df_1_3_merged['Age_right'])\n       # This works like a ternary expression.  If the expression passed to np.where is True, you get the left value, otherwise the right value.\n       ```",
            "_id": "68f812adc22606ecfa5c72f3"
          },
          {
            "taskNumber": 4,
            "title": "Filtering Rows Based on Conditions",
            "description": "1. **Filter rows in `df1` where 'Age' is greater than 30:**\n   - Display the filtered rows.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72f4"
          },
          {
            "taskNumber": 5,
            "title": "Sorting Data",
            "description": "1. **Sort `df1` by the 'Salary' column in descending order:**\n   - Display the sorted DataFrame.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72f5"
          },
          {
            "taskNumber": 6,
            "title": "Renaming Columns",
            "description": "1. **Rename columns in `df1`:**\n   - Rename 'Age' to 'Employee Age' and 'Salary' to 'Employee Salary'.  Do not use `inplace=True`, because then you wouldn't be able to do Task 9.\n   - Display the DataFrame with the renamed columns.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72f6"
          },
          {
            "taskNumber": 7,
            "title": "Data Transformation",
            "description": "1. **Apply a transformation to the 'Salary' column in `df1`:**\n   - Increase the salary by 10% for each employee.\n   - Display the updated DataFrame.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72f7"
          },
          {
            "taskNumber": 8,
            "title": "Concatenating DataFrames",
            "description": "1. **Concatenate `df1` and `df2` to add the rows of `df2` to the end of `df1`**\n   - Use `ignore_index=True` to reset the index.\n   - Display the result.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72f8"
          },
          {
            "taskNumber": 9,
            "title": "Data Wrangling a Kaggle Dataset",
            "description": "Kaggle has some nice datasets you can use in exercises.  These are `csv` files.  We are going to do some data wrangling on one of those provided files. For this task, we are going to find the international football teams that are especially bad on defense.\n- On the upper right of your notebook, click on 'Add Input'.  Click on the 'Datasets' button.  Then do a search on 'international football results'.  You should see one from Mart J√ºrisoo.  Click on the plus sign next to that one.  That adds the dataset to your notebook, so that you can read the CSV files.\n- Create a markdown cell for Task 9, and then create a new code cell for the following steps.\n- You need to find the available CSV file path names.  The first cell in your notebook, the one it started out with, has the following code:\n    ```python\n    import pandas as pd\n    import os\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n    ```\n   Click on this cell to make it active, and run the cell.  This will list, among others, the path `/kaggle/input/international-football-results-from-1872-to-2017/results.csv`.  This is the one you want.  Read it into a DataFrame called football_results.\n\n- Print the first 5 lines of this file.\n- All the entries have a home team and an away team.  This is kind of clumsy for us, because we want results for each team whether they were home or away.  So, we'll create a new DataFrame that organizes the in that way.  First, create a DataFrame called results_1.  You select the following columns from football_results: 'home_team','away_team','home_score','away_score',  and 'date'.  Print out the first 5 lines.\n- Next, create a DataFrame called results_2 from results_1.  You rename the column for 'home_team' to 'team', for 'away_team' to 'opponent', for 'home_score' to 'points_for', and for 'away_score' to 'points_against'.  Do not use `inplace=True`.  This dataset gives all the entries for the home teams.  Print out the first 5 lines.\n- Next, create a DataFrame called results_3 from results_1.  This also renames the columns, but now the rename is: 'away_team' becomes 'team', 'home_team' becomes 'opponent', 'away_score' becomes 'points_for', and 'home_score' becomes 'points_against'.  This dataset gives all the entries for the away teams.  Print out the first 5 lines.\n- Concatenate the results, resetting the index.  Store the result in football_results.  Print out the first 5 lines.  Now we have all the entries per team.\n- Do a `groupby()` on 'team'.  Get the mean() of the 'points_against' column.  Store the result (it is a Series) in the variable points_against.\n- Sort points_against so the values are descending.  Print out the first 10 lines.  These are the teams that are very bad on defense.",
            "codeExample": "```python\n    import pandas as pd\n    import os\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n    ```",
            "_id": "68f812adc22606ecfa5c72f9"
          },
          {
            "taskNumber": 10,
            "title": "More Data Wrangling for Football Results",
            "description": "This time, you'll have to figure out the steps.  Starting with the football_results DataFrame you created in Task 9, print out the most recent 10 games for Tunisia.  Remember to sort these so that you get the right games.  Avoid use of \"in_place=True\", as you may get annoying warnings.  It is often better to create a new DataFrame and store the result.\n\n\n### **Submit the Notebook for Your Assignment**  \n\nüìå **Follow these steps to submit your work:**  \n\n#### **1Ô∏è‚É£ Get a Sharing Link for Your Assignment**  \n- On the upper right of the Kaggle page, click on Save Version and save, accepting all defaults.  You can just do a quick save.\n- On the upper right, click on Share.  Choose Public, make sure that Allow Comments is on, and copy the public URL to your clipboard.\n\n#### **2Ô∏è‚É£ Submit Your Kaggle Link**  \n- Paste the URL into the **assignment submission form**.  \n\n---",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c72fa"
          }
        ],
        "submissionInstructions": "Please submit on time",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Introduction",
          "content": "# **Lesson 05 ‚Äî Data Wrangling and Aggregation**",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72e7"
        },
        {
          "subsectionOrder": 2,
          "title": "Lesson Overview",
          "content": "**Learning objective:** Students will learn to manipulate, summarize, and combine datasets in Pandas using selection, aggregation, merging, and transformation methods. They will practice accessing specific data, performing group-level calculations, and combining data from multiple sources.\n\n\n### **Topics:**\n1. Pandas Review (Optional): Recap of Series, DataFrames, and key methods from Lesson 3.\n2. Data Selection: Selecting subsets with `.loc[]`, `.iloc[]`, `.at[]`, and `.iat[]`; filtering rows by conditions; applying string methods.\n3. Data Aggregation: Grouping data with `groupby()`; applying functions like `sum()`, `mean()`, and `count()`; using `agg()` for multiple aggregations.\n4. Merging and Joining: Combining DataFrames with `merge()` and `join()`; inner, outer, left, and right joins; joining on one or multiple keys.\n5. Data Transformation: Adding, updating, and deleting columns; using operators, Series methods, `map()`, and NumPy functions for transformation.\n6. Utility Methods: Renaming columns, setting or resetting index, and sorting data.\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72e8"
        },
        {
          "subsectionOrder": 3,
          "title": "Pandas Review & Deep Dive** *(Optional)",
          "content": "Last week, we introduced Pandas, a powerful tool for data analysis. If you want a refresher on key Pandas concepts, **[listen to this NotebookLM-generated podcast](https://youtu.be/T46zVBxHrjc) reviewing what we learned last week**. \n\n*Note: The podcast references two sources, linked below.*\n\n  * (PDF) [Introduction to Pandas](https://github.com/Code-the-Dream-School/python-essentials/blob/ff583aac6befdb1e008b4d149527bc0dd5c437ef/lessons/resources/Pandas%201%20PDF.pdf)\n  * (Slide Deck) [Pandas for Dummies](https://www.slideshare.net/slideshow/numpy-and-pandas-introduction-for-beginners/281988048)",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72e9"
        },
        {
          "subsectionOrder": 4,
          "title": "Data Selection",
          "content": "### **Overview**\nIndexing and slicing allow you to extract specific rows or columns from a DataFrame, making it easier to analyze subsets of your data.\n\n### **Key Methods:**\n- `.loc[]`: Select rows and columns by labels.\n- `.iloc[]`: Select rows and columns by integer position.\n- `.at[]`: Access a specific cell by label.\n- `.iat[]`: Access a specific cell by position.\n\n### **Why Use Data Selection?**\nData selection helps you:\n- Filter relevant data for analysis.\n- Access specific values or ranges for visualization or calculation.\n- Preprocess data by selecting subsets of interest.\n\n**You should run all of the following code examples within the Python interactive shell.**  Start VSCode from within your `python_homework` directory, start a terminal within VSCode, and enter the `python` command to start the shell.\n\n### **Example: Using `.loc[]` and `.iloc[]`**\n```python\nimport pandas as pd\n\n# Sample DataFrame\ndata = {'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n        'Age': [24, 27, 22, 32],\n        'Score': [85, 92, 88, 76]}\ndf = pd.DataFrame(data)\n\n# Select the 'Name' column\nprint(df['Name'])\n\n# Select rows and specific columns\nprint(df.loc[0:2, ['Name', 'Age']])\n\n# Select rows by position\nprint(df.iloc[:2])  # First two rows\n```\n\n## **Explanation:**\n`.loc[]` is used for label-based indexing. Here, df.loc[0:2, ['Name', 'Age']] selects rows 0 to 2 and the 'Name' and 'Age' columns.\n`.iloc[]` is used for position-based indexing. df.iloc[:2] selects the first two rows.\n\nThis is slicing, similar to the slicing of lists described in lesson 2.  There are some differences however. The indices for df, in this case, are 0 through 3.\n\n```python\nprint(df.loc[0:2])  # This prints the first 3 rows! It starts with the row with index 0 and continues up to and including the row with index 2.\nprint(df.iloc[:2]) # This prints the first 2 rows only.  iloc[] works like list slicing.  It does not include the row with index 2.\nprint(df.loc[[0,2]]) # This prints row 0 and row 2.  You specify a list of the rows you want.  You can't do this with lists!\n```\nIn each of the cases above, what is returned is a new DataFrame that is a subset of the old one.\n\nOne can also specify a filter.  For example:\n\n```python\nprint(df[df['Age'] > 24])\n# print(df[df['Age'] > 24 and df['Score'] >=88])         Doesn't work!  'and' is not a valid operator for Series!\nprint(df[(df['Age'] > 24) & (df['Score'] >=88)])        # This one does work! It does the boolean AND of corresponding series elements.\n# print(df[\"a\" in df['Name']])                          Doesn't work!  The \"in\" operator doesn't work for Series!\nprint(df[df['Name'].str.contains(\"a\")])                 # This does work!  \n# There are a bunch of useful str functions for Series.  While we're at it:\n# df['Name'] = df['Name'].upper()                       Doesn't work!!\ndf['Name'] = df['Name'].str.upper()                     # Does work! \nprint(df)\n```",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72ea"
        },
        {
          "subsectionOrder": 5,
          "title": "Data Aggregation",
          "content": "### **Overview**\nAggregating data involves summarizing it by groups, enabling insights at a higher level (e.g., total sales by region, average score by category).\n\n### **Key Method:**\n- `groupby()`: Groups data by one or more columns.\n- Aggregation functions: `sum()`, `mean()`, `count()`, etc.\n\n### **Why Use Aggregation?**\nAggregation simplifies data analysis by:\n- Summarizing patterns across categories.\n- Providing key metrics like totals, averages, and counts.\n- Reducing data complexity.\n\n### **Example: Using `groupby()`**\n```python\n# Group data by a column and calculate the sum\ndata = {'Category': ['A', 'B', 'A', 'B', 'C'],\n        'Values': [10, 20, 30, 40, 50]}\ndf = pd.DataFrame(data)\n\n# Group by 'Category' and calculate the sum\ngrouped = df.groupby('Category').sum()\nprint(grouped) # grouped is another DataFrame with summary data\n\n# Calculate the mean for each group\nmean_values = df.groupby('Category')['Values'].mean()\nprint(mean_values)\n```\n\n## Explanation:\n- **`df.groupby('Category').sum()`** groups the data by the 'Category' column and calculates the sum of 'Values' within each category.\n- **`df.groupby('Category')['Values'].mean()`** calculates the mean of 'Values' for each category.\n\n---\n\n### **Advanced Aggregation**\n\nYou can apply multiple aggregation functions at once by using `agg()`. This allows you to calculate various summary statistics for each group.\n\n#### Example: Applying Multiple Aggregations\n\n```python\nimport pandas as pd\n\n# Sample DataFrame\ndata = {'Category': ['A', 'B', 'A', 'B', 'C'],\n        'Values': [10, 20, 30, 40, 50]}\ndf = pd.DataFrame(data)\n\n# Group by 'Category' and apply multiple aggregation functions\nresult = df.groupby('Category').agg({'Values': ['sum', 'mean', 'count']})\nprint(result)\n```\n\n## **Explanation:**\n`sum()` calculates the total sum of values for each category.\n`mean()` calculates the average value for each category.\n`count()` counts the number of non-null entries for each category\n\nNote: For a given agg() invocation on a column, you can specify one or several aggregation functions:\n\n```python\nresult1 = df.groupby('Category').agg({'Values': 'sum'})\n# or\nresult2 = df.groupby('Category').agg({'Values': ['sum', 'mean', 'count']})\n```\nIf you print out result1 and result2, you'll see that they look different.  The result2 DataFrame has column headers with several rows, because 3 columns are needed for Values aggregation, one for some, one for mean, and one for count.  The column names in this case are tuples, such as `(\"Values\",\"sum\").\n\nIn the result1 case, you just have one column for the sum of values, and the column name is \"Values\".  The difference hangs on whether you specify a single aggregation function or a list.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72eb"
        },
        {
          "subsectionOrder": 6,
          "title": "Merging and Joining",
          "content": "### **Overview**\nCombine multiple DataFrames using shared keys (columns or indices). \n\n### **Key Methods:**\n- `merge()`: Combines DataFrames on common columns.\n- `join()`: Combines DataFrames based on their indices.\n\n### **Why Use Merging and Joining?**\n- Combine related datasets for comprehensive analysis.\n- Handle data stored across multiple sources.\n- Maintain relationships between data records.\n\n### **Example: Using `merge()`**\n```python\n# Sample DataFrames\ndf1 = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['Alice', 'Bob', 'Charlie']})\ndf2 = pd.DataFrame({'ID': [1, 2, 4], 'Score': [85, 92, 88]})\n\n# Merge on the 'ID' column\nmerged_df = pd.merge(df1, df2, on='ID', how='inner')\nprint(merged_df)  # Inner merge\n```\n\nThe merged DataFrame has the columns from both DataFrames.  In this case the key is the 'ID' column.  If those match up, the rows of the merged DataFrame will have both 'Name' and 'Score' columns.\n\nSuppose that the two DataFrames also both have an 'Age' column.  You end up with an `Age_x` and an `Age_y` columns, where the renaming is done to prevent collision.  There are ways to choose which one you want to keep.\n\nThis is an inner merge.  That means you get only the rows where the ID values match.  One could specify a `how` value of 'left', 'right', or 'outer'.  'left' means include all rows from the left DataFrame, along with matching rows from the right DataFrame.  'right' means the converse.  And 'outer' means include all rows, matching up the ones for which the 'ID' is the same.  When using 'left', 'right', or 'outer', you get `NaN` values in the columns to be added for rows that don't match up.\n### Merging on Multiple Columns\n\nSometimes you need to merge two DataFrames based on multiple columns. This is useful when you have composite keys or want to match on more than one condition.\n\n```python\nimport pandas as pd\n\n# Sample DataFrames\n\ndf1 = pd.DataFrame({\n    'ID': [1, 2, 3],\n    'Date': ['2021-01-01', '2021-01-02', '2021-01-03'],\n    'Name': ['Alice', 'Bob', 'Charlie']\n})\n\ndf2 = pd.DataFrame({\n    'ID': [1, 2, 3],\n    'Date': ['2021-01-01', '2021-01-02', '2021-01-03'],\n    'Score': [85, 92, 88]\n})\n\n# Merge on both 'ID' and 'Date'\nmerged_df = pd.merge(df1, df2, on=['ID', 'Date'], how='inner')\nprint(merged_df)\n```\nIn this example:\n\nMerging on both 'ID' and 'Date': This allows you to ensure that the rows are only merged when both conditions match, i.e., the same ID and the same Date.\nhow='inner': This ensures that only the rows with matching values in both DataFrames are included in the result.\n\n## **Explanation:**\nThe merge() function combines two DataFrames based on a common column. Here, it's merging df1 and df2 on the 'ID' column using an inner join, meaning only rows with matching 'ID' values from both DataFrames will appear in the result.  You can also use the join() function.  This matches on index values, instead of the values in one or several key columns.\n\n### **Example: Using `join()`**\n```python\n# Join DataFrames by index\ndf1 = pd.DataFrame({'Name': ['Alice', 'Bob', 'Charlie']}, index=[1, 2, 3])\ndf2 = pd.DataFrame({'Score': [85, 92, 88]}, index=[1, 2, 4])\n\njoined_df = df1.join(df2, how='outer')\nprint(joined_df)\n```",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72ec"
        },
        {
          "subsectionOrder": 7,
          "title": "Data Transformation",
          "content": "While one can do transformation of the DataFrame as a whole, for the moment we will focus on approaches that do it one column at a time.  You can add, replace, or delete a column of a DataFrame at any time.\n\n```python\njoined_df['bogus']=['x','y','z','w'] # adds a column\nprint(joined_df)\njoined_df['bogus']=joined_df['bogus'] + \"_value\"  # replaces a column\nprint(joined_df)\njoined_df.drop('bogus', axis=1, inplace=True) # deletes the column.  You need axis=1 to identify that the drop is for a column, not a row\nprint(joined_df)\n```\n\nLook carefully at the case where the column is replaced.  `joined_df['bogus']` returns a view of the column, which is a Series.  You don't write to that directly.  You create a new column, transforming the existing value.  In this case, \"_value\" is concatenated to each of the values in the original view.  Then, you replace the 'bogus' column with the new column.\n\nYou can transform a Series in several ways.\n- You can use an operator, as in the above example.  Of course, you can't raise a string to a power of 2, or anything like that, so the type of the entry is important.\n- You can use a Series method.  One important example is the map() method, but there are others, such as astype().\n- You can use a NumPy function that can operate on a Series.  \nLet's look at an example of each:\n\n```python\nimport numpy\ndata = {'Name': ['A','B','C'],'Value':[1,2,3]}\nnew_df = pd.DataFrame(data)\nprint(new_df)\nnew_df['Value'] = new_df['Value'] ** 2  # using an operator\nprint(new_df)\nnew_df['Value'] = numpy.sqrt(new_df['Value']) # using a numpy function.  You can't use math.sqrt() on a Series.\nprint(new_df)\nnew_df['EvenOdd'] = new_df['Value'].map(lambda x : 'Even' if x % 2 == 0 else 'Odd') # the map method for a Series\nprint(new_df)\nnew_df['Value'] = new_df['Value'].astype(int) # type conversion method for a Series\nprint(new_df)\n```\nThe map() method takes one parameter, a function that does the conversion.  In the example above, a lambda is used to specify the function, and a ternary expression is used in the lambda.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72ed"
        },
        {
          "subsectionOrder": 8,
          "title": "Utility Methods",
          "content": "### **Changing Column Names**\n\nYou can rename one or more columns as follows:\n\n```python\njoined_df.rename(columns={'Score':'Test Score'}, inplace=True)\nprint(joined_df)\n```\n\n### **Converting a Column To An Index**\n\n```python\nrenamed_df=joined_df.set_index('Name')\nprint(renamed_df)\n```\n\n### **Sorting a DataFrame**\n\nYou can sort a DataFrame by column values: \n\n```python\njoined_df.sort_values(by='Score',ascending=False,inplace=True)\nprint(joined_df)\n```\n\n### **Resetting the Index***\n\nAfter you sort, the index values are no longer in order.  In this and other cases, you may want to reset the index:\n\n```python\njoined_df.reset_index(inplace=True, drop=True)\nprint(joined_df)\n```\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72ee"
        },
        {
          "subsectionOrder": 9,
          "title": "Check for Understanding",
          "content": "1. **How can you select rows where the \"Age\" column is greater than 25?**\n   - A) `df.loc[df['Age'] > 25]`\n   - B) `df.iloc[df['Age'] > 25]`\n   - C) `df[['Age'] > 25]`\n   - D) `df[df['Age'] > 25]`\n\n2. **Which method is used to combine two DataFrames on their indices?**\n   - A) `join()`\n   - B) `merge()`\n   - C) `groupby()`\n   - D) `concat()`\n\n\n<details>\n\n<summary>Answer</summary>\n\n**Answer Key:**\n1. A or D  \n2. A  \n\n</details>\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72ef"
        },
        {
          "subsectionOrder": 10,
          "title": "Summary",
          "content": "In this lesson, you‚Äôve learned:\n- How to select and slice subsets of data using `.loc[]` and `.iloc[]`.\n- How to use `groupby()` for aggregations like `sum()` and `mean()`.\n- How to combine DataFrames using `merge()` and `join()`.\n- How to transform the data.\n- Some utilities to rename columns, sort, convert a column to an index, and reset the index.\n\nUse these techniques to perform advanced data analysis in Pandas. For further exploration, refer to the [Pandas Documentation](https://pandas.pydata.org/docs/) and Python's [official documentation](https://docs.python.org/3/).\n```",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72f0"
        }
      ]
    },
    {
      "id": "68f812adc22606ecfa5c72fc",
      "lessonNumber": 6,
      "title": "# Lesson 06 ‚Äî Data Cleaning and Validation I",
      "status": "pending",
      "assignment": {
        "title": "Assignment for Lesson 6",
        "objective": "### Data Cleaning and Validation with Pandas\n\n### **Objective:**\nThis assignment gives you practice in several data manipulation techniques. In subsequent sections, the focus is on exploring fundamental data cleaning and validation techniques using the Pandas library in Python. You will learn how to handle missing data, transform data types, remove duplicates, manage outliers, standardize data, encode categorical variables, and validate data ranges.\n\nThis assignment is to be created in a Kaggle notebook, as you did for Assignment 5.  This time, create a notebook called CTD_Assignment_6 for code as described below.\n\n### **Tasks:**",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [
          {
            "taskNumber": 1,
            "title": "Practice Pivot Tables",
            "description": "1. In the upper right of your notebook page click on \"Add Input\" and then \"Datasets\".  Search on \"Ecommerce Consumer Behavior\".  You should find a dataset from Salahuddin Ahmed.  Click on the plus button to add this one to your notebook.\n\n2. Resolve the file you need to read, by running the first cell in your notebook. Then read the file into a DataFrame called `ecommerce`.  Print out the first 5 rows, so that you know what the data looks like.\n\n3. In the following step, you will want to sum the Purchase_Amount values.  Unfortunately, this column is stored as a string.  Replace the column with one that converts the string to a float.  You will need to take the dollar sign off before conversion.\n\n4. Create a \"buying_patterns\" pivot table from the data.  The index should be the 'Purchase_Category'.  The columns should be 'Gender' and 'Income_Level'.  The value you sum should be 'Purchase_Amount'.  Print out the resulting DataFrame.\n\n5. Create a \"demographics\" pivot table from the ecommerce DataFrame.  You want two levels of index, on 'Income_Level' and 'Education_Level'.  For columns you want 'Marital_Status'.  You want to count the 'Customer_ID' values.  Print out the resulting DataFrame.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c7306"
          },
          {
            "taskNumber": 2,
            "title": "Practice apply()",
            "description": "1. Add another input.  This time, you search for \"AI-Powered Job Recommendations\".  You should find a dataset from Samay Ashar.  \n\n2. Create a new code cell.  Add code to resolve the name of the file you need to read.  For this part, you use the job recommendations file.  Read it into a DataFrame called \"jobs\".  Print out the first 10 lines, so that you know what the column names are.\n\n3. Use the `apply()` method to create an additional column in the jobs DataFrame called 'Check These Out'.  Give that a value of \"Yes\" if the job is entry level, has a salary greater than or equal to 70000, and requires both SQL and Python.  Give the column a value of \"No\" otherwise.\n\n4. Create a DataFrame called \"my_jobs\".  Select the rows from \"jobs\" that have a 'Check These Out' column value of 'Yes'.  Print out the first 10 lines.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c7307"
          },
          {
            "taskNumber": 3,
            "title": "Handling Missing Data",
            "description": "1. **Create a DataFrame using the provided data:**\n\n   - Add another input dataset.  This time, search on \"code the dream lesson 5\"\n   - Create another code block.\n   - Resolve the name of the file as usual.\n   - Read it into a DataFrame called df.\n   - The DataFrame contains columns for 'Name', 'Age', 'Salary', 'Join Date', and 'City', with some missing values.\n   - Print the original DataFrame.\n  \nHere's a summary of what the DataFrame looks like to help you get started (but don't use this code):\n\n```python\ndata = {\n    'Name': ['Alice', 'Bob', None, 'David', 'Eva'],\n    'Age': [25, None, 35, 40, 30],\n    'Salary': [50000, 60000, None, 80000, 55000],\n    'Join Date': ['2020-01-01', None, '2020-03-15', '2020-04-20', None],\n    'City': ['New York', 'Los Angeles', 'Chicago', None, 'Miami']\n}\ndf = pd.DataFrame(data)\n```\n\n2. **Perform the following operations on new DataFrames:**\n     - Create df1 by using `dropna()` on the df DataFrame created above.  Print the `info()` for df and df1 to see how many lines have missing values.\n     - **Replace missing values** in df using the `fillna()` method:\n     - Replace missing 'Name' values with `'Unknown'`.\n     - Replace missing 'Age' values with the **mean** of the 'Age' column.\n     - Replace missing 'Salary' values with the **median** of the 'Salary' column.\n     - Replace missing 'Join Date' values with `'2020-01-01'`.\n     - **Remove rows with missing values** using the `dropna()` method and save the result in df2.  Only the 'City' column should have missing values at this point.  Reset the index.\n     - Convert the 'Age' column in df2 to **integer** type using `astype(int)`.\n     - Print the updated df2 DataFrame.",
            "codeExample": "```python\ndata = {\n    'Name': ['Alice', 'Bob', None, 'David', 'Eva'],\n    'Age': [25, None, 35, 40, 30],\n    'Salary': [50000, 60000, None, 80000, 55000],\n    'Join Date': ['2020-01-01', None, '2020-03-15', '2020-04-20', None],\n    'City': ['New York', 'Los Angeles', 'Chicago', None, 'Miami']\n}\ndf = pd.DataFrame(data)\n```",
            "_id": "68f812adc22606ecfa5c7308"
          },
          {
            "taskNumber": 4,
            "title": "Data Transformation",
            "description": "1. **Convert Data Types:**\n   - Add another input.  This time search for \"Code The Dream Eclipses\".  This is a list of eclipses that were or will be observed in Arkansas.\n   - Load the CSV file into a DataFrame called df3.  **Note:** The separator is \"|\" for this CSV file.\n   - Print df3.info() and the first 5 rows of df3.\n   - Attempt to convert the 'Date' column to **datetime** format using `pd.to_datetime()`.  You will see that an error is thrown for an invalid date.\n   - Add `errors='coerce` to your `pd.to_datetime()` statement and try the conversion again.\n   - Print the first 20 lines of the revised df3.  Examine what is stored for the dates that could not be converted.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c7309"
          },
          {
            "taskNumber": 5,
            "title": "Removing Duplicates",
            "description": "1. **Identify and remove duplicate records:**\n   - Print `df3.info()`.\n   - Use the `duplicated()` method to identify duplicate rows in the DataFrame, and save the result in duplicate_series.  This Series has `True` for each duplicate entry.\n   - Print `duplicate_series[duplicate_series == True].head(10)` to see the first 10 duplicated entries.\n   - Print `duplicate_series.value_counts()` to see how many duplicates you have.\n   - Use the `drop_duplicates()` method to remove duplicate rows.\n   - Print `info()` for the updated DataFrame.\n  \nBy default, `drop_duplicates()` keeps the first occurrence of each duplicate row. You could use the `keep` parameter to change this behavior, but the default is ok for now.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c730a"
          },
          {
            "taskNumber": 6,
            "title": "Handling Outliers",
            "description": "1. **Identify and replace outliers in the 'Age' column:**\n   - For this and the following tasks, use df2 again.\n   - Consider outliers as values greater than 100 or less than 0.\n   - Replace outliers with the **median** of the 'Age' column.\n   - Print the updated DataFrame after handling outliers.\n\nOutliers can also be identified using statistical methods like the Interquartile Range (IQR) or Z-scores -- but we'll just keep it simple for now",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c730b"
          },
          {
            "taskNumber": 7,
            "title": "Standardizing Data",
            "description": "1. **Standardize the 'Name' column:**\n   - Convert all names to lowercase and trim any leading or trailing whitespace using `str.lower()` and `str.strip()`.\n   - Print the updated DataFrame.\n\n2. **Standardize inconsistent entries in the 'City' column:**\n   - Find variations in the City name:\n   ```python\n   print(df.groupby('City').agg({'Name': 'count'})) # This will show all city names, so you can see variations\n   ```\n   - Replace variations like 'NYC' with 'New York' and 'LA' with 'Los Angeles'.\n   - Print the updated DataFrame.",
            "codeExample": "```python\n   print(df.groupby('City').agg({'Name': 'count'})) # This will show all city names, so you can see variations\n   ```",
            "_id": "68f812adc22606ecfa5c730c"
          },
          {
            "taskNumber": 8,
            "title": "Validating Data Ranges",
            "description": "1. **Ensure the 'Age' column contains values within the valid range (18 to 65):**\n   - Replace invalid ages (less than 18 or greater than 65) with a NaN value.  (NaN is actually part of numpy: `np.nan` is the value you should use.  Don't use the string 'NaN'!)\n   - Print the updated Dataframe.\n   - Fill the NaN values with the **median** of the 'Age' column.\n   - Print the updated DataFrame.\n\n**Explanation:** Validating data ranges ensures that your data is consistent and suitable for analysis or modeling.\n\n### **Submit the Notebook for Your Assignment**  \n\nüìå **Follow these steps to submit your work:**  \n\n#### **1Ô∏è‚É£ Get a Sharing Link for Your Assignment**  \n- On the upper right of the Kaggle page, click on Save Version and save, accepting all defaults.  You can just do a quick save.\n- On the upper right, click on Share.  Choose Public, make sure that Allow Comments is on, and copy the public URL to your clipboard.\n\n#### **2Ô∏è‚É£ Submit Your Kaggle Link**  \n- Paste the URL into the **assignment submission form**.  \n\n---",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c730d"
          }
        ],
        "submissionInstructions": "Please submit on time",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Lesson 06 ‚Äî Data Cleaning and Validation I",
          "content": "**Lesson Overview**\n\n**Learning objective:** Students will learn essential data cleaning and transformation techniques in Pandas, including handling missing values, outliers, and duplicates. They will also use pivot tables, the `apply()` method, and column-wise operations to reshape and enrich datasets.\n\n\n### Topics\n\n1. **Pivot Tables**: A way to present summary data.\n2. **Transforming DataFrames with apply()**: A flexible way to create new columns by combining column entries from existing columns.\n3. **Intro to Data Cleaning**: Identifying dirty data; understanding standardization, outliers, deduplication, and validation. \n4. **Handling Missing Data**: Removing rows with `dropna()`, replacing values with `fillna()`. \n5. **Data Transformation**: Converting data types, reformatting dates, **feature engineering**, **data discretization**. \n6. **Removing Duplicates**: Identifying and removing duplicate records.\n7. **Removing Outliers**: Identify and removing outlying records\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72fd"
        },
        {
          "subsectionOrder": 2,
          "title": "Pivot Tables.",
          "content": "As usual, run these examples in the Python interactive shell of your `python_homework` terminal session.\n\nConsider the following case. Acme Inc. has 3 sales employees and 2 regions.  The employees each report the revenue they got from sales of each of the products in each of the regions.  These reports are probably kept in a database, but for our purposes, they look like this:\n```python\ninclude pandas as pd\ndata = [{'Employee': 'Jones', 'Product': 'Widget', 'Region': 'West', 'Revenue': 9000}, \\\n{'Employee': 'Jones', 'Product': 'Gizmo', 'Region': 'West', 'Revenue': 4000}, \\\n{'Employee': 'Jones', 'Product': 'Doohickey', 'Region': 'West', 'Revenue': 11000}, \\\n{'Employee': 'Jones', 'Product': 'Widget', 'Region': 'East', 'Revenue': 4000}, \\\n{'Employee': 'Jones', 'Product': 'Gizmo', 'Region': 'East', 'Revenue': 5500}, \\\n{'Employee': 'Jones', 'Product': 'Doohickey', 'Region': 'East', 'Revenue': 2345}, \\\n{'Employee': 'Smith', 'Product': 'Widget', 'Region': 'West', 'Revenue': 9007}, \\\n{'Employee': 'Smith', 'Product': 'Gizmo', 'Region': 'West', 'Revenue': 40003}, \\\n{'Employee': 'Smith', 'Product': 'Doohickey', 'Region': 'West', 'Revenue': 110012}, \\\n{'Employee': 'Smith', 'Product': 'Widget', 'Region': 'East', 'Revenue': 9002}, \\\n{'Employee': 'Smith', 'Product': 'Gizmo', 'Region': 'East', 'Revenue': 15500}, \\\n{'Employee': 'Garcia', 'Product': 'Widget', 'Region': 'West', 'Revenue': 6007}, \\\n{'Employee': 'Garcia', 'Product': 'Gizmo', 'Region': 'West', 'Revenue': 42003}, \\\n{'Employee': 'Garcia', 'Product': 'Doohickey', 'Region': 'West', 'Revenue': 160012}, \\\n{'Employee': 'Garcia', 'Product': 'Gizmo', 'Region': 'East', 'Revenue': 16500}, \\\n{'Employee': 'Garcia', 'Product': 'Doohickey', 'Region': 'East', 'Revenue': 2458}]\nsales = pd.DataFrame(data)\nprint(sales)\n```\nOk, all the information is here, but it is not organized as the CFO would like.  You have already learned one way to summarize the data, using groupby().  Another is to use pivot tables.  Here are three examples:\n\n```python\nsales_pivot1 = pd.pivot_table(sales,index=['Product','Region'],values=['Revenue'],aggfunc='sum',fill_value=0)\nprint(sales_pivot1)\n# This creates a two level index to show sales by product and region. The revenue values are summed for each product and region.\nsales_pivot2 = pd.pivot_table(sales,index='Product',values='Revenue',columns='Region', aggfunc='sum',fill_value=0)\nprint(sales_pivot2)\n# The result here is similar, but instead of a two level index, you have columns to give sales by region.\nsales_pivot3 = pd.pivot_table(sales,index='Product',values='Revenue',columns=['Region','Employee'], aggfunc='sum',fill_value=0)\nprint(sales_pivot3)\n# By adding the employee column, you get these revenue numbers broken down by employee.  The fill value is used when there is no corresponding entry.\n```\nBy gaining familiarity with pivot tables, you can present data in various ways that make it easy to show the business picture.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72fe"
        },
        {
          "subsectionOrder": 3,
          "title": "Using apply()",
          "content": "You have already learned several ways to generate a new column from an existing one.  Suppose, however, that you want to combine information from several columns.  For example, you could do the following:\n```python\nsales_pivot2['Total'] = sales_pivot2['East'] + sales_pivot2['West'] # adding two columns to make a new one\nprint(sales_pivot2)\nper_employee_sales=sales.groupby('Employee').agg({'Revenue':'sum'})\nper_employee_sales['Commission Percentage'] = [0.12, 0.09, 0.1]\nper_employee_sales['Commission'] = per_employee_sales['Revenue'] * per_employee_sales['Commission Percentage']\nprint(per_employee_sales)\n```\nOk, so far so good.  But suppose the combination rules are a little more complicated.  Consider this case:\n```python\nper_employee_sales=sales.groupby('Employee').agg({'Revenue':'sum'})\nper_employee_sales['Commission Plan'] = ['A', 'A', 'B']\n```\nSales employees on commission plan A get $1000 for the first 10000 in revenue, and 5% for revenue over 10000.  Everyone else gets $1400 for the first 10000 in revenue, but 4% for revenue over 10000.  All employees get zip if they don't get at least 10000 in revenue. You use apply(), and you specify `axis=1` to request the entire row.  You pass a function to apply(), and the function is called once per row.  As follows:\n```python\ndef calculate_commission(row):\n    if row['Revenue'] < 10000:\n        return 0\n    if row['Commission Plan'] == 'A':\n        return 1000 + 0.05 * (row['Revenue'] - 10000)\n    else:\n        return 1400 + 0.04 * (row['Revenue'] - 10000)\n\nper_employee_sales['Commission'] = per_employee_sales.apply(calculate_commission, axis=1)\nprint(per_employee_sales)\n```",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c72ff"
        },
        {
          "subsectionOrder": 4,
          "title": "What is Data Cleaning?",
          "content": "Data is often dirty.  Values may be missing, or duplicated, or incorrectly formatted, or have values that are not plausible or manageable by data analysis.  The following optional [video](https://www.youtube.com/watch?v=WpX2F2BS3Qc) expains the concepts.  The main idea is garbage-in, garbage-out.  Any analysis you do on data that is partly incorrect may be invalid.  Data cleaning involves the following procedures:\n\n- Standardization.  For example, you want to represent dates and phone numbers in a standard way.\n- Addressing outliers.  These are values that appear improbable and were likely entered in error.\n- Deduplication.\n- Addressing missing values.\n- Validation.\n\nYou will learn various technical approaches to each of these.\n\n### Standardization\n\nYou can check, for example, that email addresses are of a valid format.  Suppose some are not.  You can either flag the rows for manual correction, or discard the rowss, or attempt to reformat the addresses to correct the problem, or derive the correct addresses from other information in the row or perhaps in a separate dataset.  For US phone numbers, you could check that each is a string of 10 numeric characters.\n\n### Addressing Outliers\n\nSometimes outliers are easy to identify.  If a person's age is negative, or greater than 120, this is an outlier.  But in other cases, it might not be so clear.  For example, if you discarded outliers for daily rainfall in a region, you could end up discarding all flood events, which are important records that are needed for the analysis.\n\n### Deduplication\n\nDeduplication is pretty easy if the rows are exactly the same.  But, if one of the values in the row is a little bit different, it may not be as clear.  Perhaps you have access to a reliable key, such as a known phone number or email address.\n\n### Addressing missing values\n\nYou can throw away the rows with missing information ... but other parts of the record may have valuable information.\n\nYou can substitute a plausible value ... but this distorts the data.  For example, if the data contains a person's net worth, you could substitute the mean value for that column, but actually most people don't have nearly the 'average' net worth.  You could substitute the median value instead, but again, most people have incomes that differ from the median by quite a bit.\n\nEach automated change to clean the data involves a tradeoff.  You should always archive the original data, in case cleaning introduces distortions.\n\n### Validation\n\nThis is a process to check that the cleaned data is in fact correct.  For example, for information about a person, you might want to ask the person if the stored information is correct.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7300"
        },
        {
          "subsectionOrder": 5,
          "title": "Handling Missing Data",
          "content": "### Overview\n\nMissing data is common in datasets and can significantly affect the outcome of analyses if not handled properly. In Pandas, there are methods to identify and handle missing data, either by removing rows or by filling in the missing values. Correctly managing missing data is crucial for ensuring accurate results.\n\n### Key Methods\n\n- `isnull()`: Find the rows where data is missing.\n- `dropna()`: Removes rows or columns with missing data.\n- `fillna()`: Replaces missing values with specified values.\n\n### Why Handle Missing Data?\n\n- Ensures accurate calculations and visualizations.\n- Prevents runtime errors during analysis.\n- Allows consistent dataset formatting.\n\n### Example: Using `isnull()`, `dropna()` and `fillna()`\n\n**For this and all examples below, you should run the code within the Python interactive shell of your python_homework VSCode terminal.**\n\n```python\nimport pandas as pd\n\n# Sample DataFrame with missing values\ndata = {'Name': ['Alice', 'Bob', None, 'David'],\n        'Age': [24, 27, 22, None],\n        'Score': [85, None, 88, 76]}\ndf = pd.DataFrame(data)\n\n# Find rows with missing data\ndf_missing = df[df.isnull().any(axis=1)]\nprint(df_missing)\n# Remove rows with missing data\ndf_dropped = df.dropna()\nprint(df_dropped)\n\n# Replace missing data with default values\ndf_filled = df.fillna({'Age': 0, 'Score': df['Score'].mean()})\nprint(df_filled)\n```\n\n**Explanation:**\n`df.isnull().any(axis=1)` finds the rows that have null or NaN values.  The `axis=1` is needed to specify rows.\n\n`dropna()` removes any row that contains a `None` (missing) value. This can remove quite a lot of data, especially if you have a lot of columns.\n\n`fillna()` is used to replace missing values. In this case, the `Age` column's missing values are replaced with 0, and the `Score` column's missing values are filled with the mean of the existing scores. This can cause issues if the values you are replacing become outliers.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7301"
        },
        {
          "subsectionOrder": 6,
          "title": "Data Transformation",
          "content": "### Overview\n\nData transformation is essential for ensuring that all data conforms to the correct format and type, which makes it easier to manipulate and analyze. This includes tasks like converting strings to numeric values, reformatting date strings into datetime objects, **creating new features** (feature engineering), and **discretizing continuous variables**.\n\n### Key Tasks\n\n- Converting data types (e.g., strings to integers).\n- Reformatting date strings into datetime objects.\n- **Creating new features:** \n    - Combining existing features (e.g., `Age` and `YearsOfExperience` to create `AgeGroup`).\n    - Extracting features from existing ones (e.g., extracting `Year` and `Month` from a `Date` column).\n    - Generating interaction features (e.g., multiplying two features).\n- **Data Discretization:** \n    - Binning continuous variables into discrete categories (e.g., age ranges, income brackets).\n    - Using techniques like `pd.cut()` or `pd.qcut()`.\n\n### Why Transform Data?\n\n- Prevents errors due to mismatched data types.\n- Simplifies operations such as comparisons and calculations.\n- Ensures uniformity in dataset representation.\n- Improves model performance in machine learning tasks.\n\n### Example: Converting Data Types and Reformatting Dates\n\n```python\nimport pandas as pd\n\n# Sample DataFrame with mixed data types\ndata = {'Name': ['Alice', 'Bob', 'Charlie'],\n        'Age': ['24', '27', '22'],\n        'JoinDate': ['2023-01-15', '2022-12-20', '2023-03-01']}\ndf = pd.DataFrame(data)\n\n# Convert 'Age' column to integers\ndf['Age'] = df['Age'].astype(int)\n\n# Convert 'JoinDate' column to datetime\ndf['JoinDate'] = pd.to_datetime(df['JoinDate'])\n\nprint(df.dtypes)  # Verify data types\nprint(df)\n```\nIn addition you can use the Series map() method to change items in a column.\n\n```python\nimport pandas as pd\n\n# Sample DataFrame\n\ndata = {'Name': ['Alice', 'Bob', 'Charlie'],\n        'Location': ['LA', 'LA', 'NY'],\n        'JoinDate': ['2023-01-15', '2022-12-20', '2023-03-01']}\ndf = pd.DataFrame(data)\n\n# Convert 'Location' abbreviations into full names\n\ndf['Location'] = df['Location'].map({'LA': 'Los Angeles', 'NY': \"New York\"})\nprint(df)\n```\n\nThe problem with the code above is that if the value in the 'Location' column is not either 'LA' or 'NY', it is converted to `NaN`.  Suppose you want to preserve the existing value in this case. You'd use the replace() method instead:\n\n```python\ndf['Location'] = df['Location'].replace({'LA': 'Los Angeles', 'NY': \"New York\"})\n```\n\nHere is another case.  Suppose we have a list of people's phone numbers, but they are not in standard format.  We can attempt to clean this up in this way:\n\n```python\nimport pandas as pd\ndata = {'Name': ['Tom', 'Dick', 'Harry', 'Mary'], 'Phone': [3212347890, '(212)555-8888', '752-9103','8659134568']}\ndf = pd.DataFrame(data)\ndf['Correct Phone'] = df['Phone'].astype(str)\n\ndef fix_phone(phone):\n    if phone.isnumeric():\n        out_string = phone\n    else:\n        out_string = ''\n        for c in phone:\n            if c in '0123456789':\n                out_string += c\n    if len(out_string) == 10:\n        return out_string\n    return None\n    \ndf['Correct Phone'] = df['Correct Phone'].map(fix_phone)\nprint(df)\n```\nIn the code above, the 'Phone' column is preserved, in case there is useful information, but the 'Correct Phone' column is created, with a best effort at reformatting the phone numbers.  Note that the logic did not fit in a lambda, so a function was declared to pass to map().\n\nFinally we can use built in numpy functions in order to change all of the data in a data frame by following a function\n```python\n\nimport pandas as pd\n\ndata = {'Name': ['Alice', 'Bob', 'Charlie'],\n\t'Age': [20, 22, 43]}\n\ndf = pd.DataFrame(data)\n\n# Increase the age by 1 as a new year has passed\ndf['Age'] = df['Age'] + 1\nprint(df)\n```\n\n\nFor **Data Discretization** we have to use the more complicated pandas.cut() function. This will allow us to automatically split data into a series of equal sized bins.\n\n```python\nimport pandas as pd\ndata = {'Name': ['Alice', 'Bob', 'Charlie'],\n        'Location': ['LA', 'LA', 'NY'],\n        'Grade': [78, 40, 85]}\ndf = pd.DataFrame(data)\n\n# Convert grade into three catagories, \"bad\", \"okay\", \"great\"\n\ndf['Grade'] = pd.cut(df['Grade'], 3, labels = [\"bad\", \"okay\", \"great\"])\nprint(df)\n```\n\n**Explanation:**\n\n`astype(int)` converts the `Age` column, originally stored as strings, into integers.\n`pd.to_datetime()` converts the `JoinDate` column into Python‚Äôs datetime objects for easier date manipulation and comparison.\n`pd.cut()` allows us to create bins for data and provide data discretization\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7302"
        },
        {
          "subsectionOrder": 7,
          "title": "Removing Duplicates",
          "content": "### Overview\n\nDuplicate records can inflate metrics and skew results. Removing duplicates ensures that each record is unique, which improves the reliability of analyses. Pandas provides the `drop_duplicates()` method to identify and remove duplicate rows.\n\n### Key Method\n\n- `drop_duplicates()`: Removes duplicate rows based on one or more columns.\n\n### Why Remove Duplicates?\n\n- Prevents redundant information in analysis.\n- Improves data quality and storage efficiency.\n- Ensures unique data for accurate insights.\n\n### Example: Using `drop_duplicates()`\n\n```python\nimport pandas as pd\n\n# Sample DataFrame with duplicates\ndata = {'Name': ['Alice', 'Bob', 'Alice', 'David'],\n        'Age': [24, 27, 24, 32],\n        'Score': [85, 92, 85, 76]}\ndf = pd.DataFrame(data)\n\n# Identify and remove duplicates\ndf_cleaned = df.drop_duplicates()\nprint(df_cleaned)\n\n# Remove duplicates based on 'Name' column\ndf_cleaned_by_name = df.drop_duplicates(subset='Name')\nprint(df_cleaned_by_name)\n```\n\n**Explanation:**\n\n`drop_duplicates()` removes rows where the entire record is a duplicate of another.\n`drop_duplicates(subset='Name')` removes rows where the `Name` column is duplicated, keeping only the first occurrence of each name.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7303"
        },
        {
          "subsectionOrder": 8,
          "title": "Handling Outliers",
          "content": "### **Overview**\nOutliers are extreme values that deviate significantly from other observations and can bias statistical calculations.\n\n### **Common Approach:**\n- Replace outliers with statistical measures like the median.\n\n### **Code Example:**\n\n(You don't need to run this one, as the DataFrame is not provided.)\n\n```python\n# Replace outliers in 'Age' (e.g., Age > 100 or Age < 0)\ndf['Age'] = df['Age'].apply(lambda x: df['Age'].median() if x > 100 or x < 0 else x)\n\nprint(\"DataFrame after handling outliers:\")\nprint(df)\n```\n\n### **Explanation:**\n- Outliers in the `Age` column that are greater than 100 or less than 0 are replaced by the median value of the `Age` column.\n\n---\n\n\n\n### Example\n\n**Check for Understanding**\n\nWhich method removes rows with missing values?\n\nA) `fillna()`\nB) `dropna()`\nC) `remove_missing()`\nD) `replace_na()`\n\n<details>\n<summary>Answer</summary>\nAnswer: B\n</details>\n\nHow do you convert a column of strings to integers?\n\nA) `pd.to_int(column)`\nB) `df['column'].convert(int)`\nC) `df['column'].astype(int)`\nD) `df['column'].to_int()`\n\n<details>\n\n<summary>Answer</summary>\n\n**Answer**: C\n\n</details>\n\n\nWhich method is used to remove duplicate rows in Pandas?\n\nA) `remove_duplicates()`\nB) `drop_duplicates()`\nC) `find_duplicates()`\nD) `remove_redundant()`\n\n<details>\n\n<summary>Answer</summary>\n\n**Answer**: B\n\n</details>",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7304"
        },
        {
          "subsectionOrder": 9,
          "title": "Summary",
          "content": "In this lesson, you‚Äôve learned:\n\n* How to use pivot tables for improved data presentation.\n* How to use apply() on a DataFrame to generate a new column.\n* Concepts in data cleaning.\n* How to handle missing data using `isnull()`, `dropna()` and `fillna()`.\n* How to transform data by converting data types, reformatting dates, and discretizing continuous variables.\n* How to identify and remove duplicate records with `drop_duplicates()`.\n* How to remove outliers using statistical methods.\n\nBy applying these techniques, you can clean and validate your datasets for accurate and effective analysis. For further exploration, refer to the Pandas Documentation and Python's official documentation.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7305"
        }
      ]
    },
    {
      "id": "68f812adc22606ecfa5c730f",
      "lessonNumber": 7,
      "title": "Lesson 07 ‚Äî Data Cleaning and Validation with Pandas",
      "status": "pending",
      "assignment": {
        "title": "Advanced Data Cleaning and Validation Assignment",
        "objective": "### Data Cleaning and Validation with Pandas\n\n### **Objective:**\n\nThis assignment is an approximation of a real world data cleaning case.  You have four copies of a dataset, each with errors.  You want to create a single authoritative dataset that cleans all the errors out.  In addition, you'll get practice in using regular expressions for data transformation.\n\nThis assignment is to be created in a Kaggle notebook, as you did for Assignments 5 and 6.  This time, create a notebook called CTD_Assignment_7 for code as described below.  As for all assignments that use notebooks, you should create a markdown cell above your code that marks the location where you completed each coding task.\n\nComplete the tasks below to demonstrate your understanding of data cleaning and validation techniques. Submit your code and outputs for each task.\n\n---\nThis assignment asks you to do some fairly serious data cleaning.  You will start from 4 CSV files.  Each of the files describes 400 people, with a name, address, zip, and phone number for each.  However:\n1. Some of the people have the same name.\n\n2. There are only 200 different addresses.  Some people share a house.\n\n3. Of the people that share addresses, 20% also share a phone number.\n\n4. Each of the CSV files record this information, **but there are errors in each of the files.** The errors are different in each of the files.\n\n5. In 15% of the records, the phone number is missing.\n\n6. In another 15% of the records, the phone number is incorrect.\n\n7. In 20% of the records, the zipcode is missing.\n\n8. In 15% of the records, the name is misspelled.\n\n9. In 15% of the records, the address is misspelled.\n\n10. Some rows may have more than one error.\n\nYour task is to create a dataset with 400 rows, one for each of the people, that corrects these errors by comparing the versions in each of the files.",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [
          {
            "taskNumber": 1,
            "title": "How Would You Solve This?",
            "description": "1. Think: How would you do this, using the power of Pandas?\n\n2. Create a markdown cell in your notebook.  Write down a list of 5 or so ideas about how you could do this.  You do not need to use markdown -- plain text will suffice.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c731d"
          },
          {
            "taskNumber": 2,
            "title": "Load Dataset ##",
            "description": "1. In your notebook, click on the \"Add Input\" button in the upper left, and then on \"Datasets\".  Do a search on \"Code The Dream Assignment 6\", and then on the plus sign next to that dataset to add it to your notebook.  Run the first cell of the notebook to resolve the file names.\n\n2. Create a code cell.  Within it, import pandas and load the four CSV files into four DataFrames.  Print out the first 5 lines of one of these DataFrames, so that you understand the data shape.  Each of the datasets is similar.\n\n3. You need an additional package, called \"thefuzz\".  This package does approximate matching of text strings.  However, it is not part of the standard library delivered with Kaggle, so you need to add this code:\n\n```python\ntry: \n    from thefuzz import process \nexcept ImportError: \n    !pip install thefuzz \n    from thefuzz import process\n```\n4. Concatenate all 4 DataFrames into one, call it `df`.  Print out the info for df.  You should have 1600 rows, but there are NaN values and other problems.",
            "codeExample": "```python\ntry: \n    from thefuzz import process \nexcept ImportError: \n    !pip install thefuzz \n    from thefuzz import process\n```",
            "_id": "68f812adc22606ecfa5c731e"
          },
          {
            "taskNumber": 3,
            "title": "Clean Up Spelling Errors",
            "description": "We know that some of the names are misspelled.  We are going to assume that 3 of the spellings are correct, and that the remainder is bad.  Of course, several people have the same name, and some people have the same address, so that for a given name, or a given address, there may be more than 4 rows.  But, we'll assume that if a name or address is spelled the same way 3 or more times, that's the correct spelling.  (This assumption may well be false -- but, as we'll see, it still helps to fix up the majority of cases)\n\n1. Find the list of probably correct names.  You use the value_counts() method, which returns a Series.  In the Series, the index is the list of names, and the values are the count for each.  As follows:\n   ```python\n   df_names = df.value_counts('Name')\n   names = list(df_names[df_names > 2].index)\n   ```\n   Print out the first 10 entries of the list of names.\n\n2. Ok, now we have the list of probably good names.  What do we do about the bad ones?  We want to replace each with the good name that is most similar to it, using the `thefuzz` package. As follows:\n   ```python\n   df['Name'] = df['Name'].map(lambda x : x if x in names else process.extractOne(x, names)[0])\n   ```\n   Here, process.extractOne() goes through the list of good names to find the best match.  It returns a tuple, where the first element is the match, and the second is a number that indicates how good the match is.\n\n3. Fix the addresses the same way.",
            "codeExample": "```python\n   df_names = df.value_counts('Name')\n   names = list(df_names[df_names > 2].index)\n   ```\n\n```python\n   df['Name'] = df['Name'].map(lambda x : x if x in names else process.extractOne(x, names)[0])\n   ```",
            "_id": "68f812adc22606ecfa5c731f"
          },
          {
            "taskNumber": 4,
            "title": "Clean Up the Zip and Phone Columns",
            "description": "We now have four versions of the information for each person.  We can compare these and, one would hope, we can find the right values, basically by collecting the votes from each.  This is a new technique, so the answer is as follows:\n\n```python\ndef fix_anomaly(group): \n    group_na = group.dropna() \n    if group_na.empty: \n        return group.values \n    mode = group_na.mode() \n    if mode.empty: \n        return group.values \n    return mode.iloc[0]\n\ndf['Zip'] = df.groupby(['Name', 'Address'], as_index=False)['Zip'].transform(fix_anomaly).reset_index(drop=True)\n```\nLet's explain this code.  \n- We do a groupby() for name and address.  The records where the both the name and address are the same are for the same person.  \n- We use as_index=False because we don't want to change the indexing.  \n- Then we do a transform() on the Zip column.  This works kind of like the map() function, but you are passed each group in turn.  \n- Each time we get a group of values, we collect the votes.  First we do a dropna(), because a NaN value doesn't get a vote.  Then we use the mode() function to find the most common value, and change the value for the entire group to match.  \n- There may be several modes, for example if there are 2 of one value and 2 of another, so we just take the first mode. (Actually this is a little dangerous, because one of the other modes might be the correct one.  The correct approach might be to leave the values unchanged if there is more than one mode.)  \n- We have to handle a couple of edge cases.  First, all the values may be NaN, in which case we have nothing to do.  Second, there may be no mode.  This happens if each of the non-null values occurs once.  So then we just leave the values unchanged, because we don't know which one to use. After the transform() completes, we are done with the groupby(), so we reset the index.\n\n1. Fix the phone number column the same way.  Hint: You can reuse fix_anomaly().",
            "codeExample": "```python\ndef fix_anomaly(group): \n    group_na = group.dropna() \n    if group_na.empty: \n        return group.values \n    mode = group_na.mode() \n    if mode.empty: \n        return group.values \n    return mode.iloc[0]\n\ndf['Zip'] = df.groupby(['Name', 'Address'], as_index=False)['Zip'].transform(fix_anomaly).reset_index(drop=True)\n```",
            "_id": "68f812adc22606ecfa5c7320"
          },
          {
            "taskNumber": 5,
            "title": "The Final Consolidation",
            "description": "1. Eliminate duplicate records.\n\n2. Print out the first 10 records of the resulting DataFrame.\n\n3. Print out the info for the end DataFrame.\n\nHmm.  413 records reported by info().  We were hoping for 400 authoritative records.  And, we still have some records with null values.  So, next step:",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c7321"
          },
          {
            "taskNumber": 6,
            "title": "Failure Analysis",
            "description": "Validation of the final result, and failure analysis for whatever isn't working right, is a critical part of data cleaning.\n\n1. Print out all the rows that have null values.  (Note: This only finds some of the errors, as there are still erroneous records that have no null values.)\n\n2. We can now subset the original data to figure out what went wrong.  So, add a line to save a copy of the df dataframe, as it was right after the concatenation.  Call the copy df_save.\n\n3. In the records that have null values, you see one for \"Tammie ThoXXmas\".  This looks like a misspelling that should have been corrected.  Print out all the rows from df_save that have the name \"Tammie ThoXXmas\".  Can you see what is wrong?\n\n4. Here's the answer: There are two people named Tammie Thomas, one in Minnesota, one in Massachusetts.  So the original dataset had 8 entries for the two people with that name.  And, in three of them, the name is misspelled in exactly the same way.  Can you see why this would cause trouble for our code?  This is exactly the sort of thing that happens with real-world data.\n\n5. We see that there is also a problem for \"Charles Smith\".  Print out all the rows from df_save that have this name.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c7322"
          },
          {
            "taskNumber": 7,
            "title": "How to Improve",
            "description": "1. Create another markdown cell.  Explain what happened for \"Tammie ThoXXmas\", so that this name was not corrected.\n\n2. Also, in the markdown cell, explain why the approach failed for \"Charles Smith\".\n\n3. What could be done to make the data valid?  Put your ideas into the markdown cell.  Hint: The solution is often not more code.\n\n4. The assumption that a name is not correct unless it is spelled the same way 3 or more times has introduced problems.  \"Mrs. Gail Perez\" was misspelled twice, so all 4 instances were replaced by \"Mrs. Taylor Johnson DDS\".  That's dead wrong. The name and address fixing algorithm is really not very good. How could we avoid errors like this?  Also, since this assignment was created, someone thought of a check you can perform to find nearly all errors, but it must be done before you remove duplicates.  What is that check?\n\nYou see that data cleaning often involves assumptions.  Those require careful thought. Such assumptions are necessary, but they might be wrong.",
            "codeExample": "",
            "_id": "68f812adc22606ecfa5c7323"
          },
          {
            "taskNumber": 8,
            "title": "Regular Expressions for Data Cleaning",
            "description": "1. Given the following data, use the `Series.str.extract` method to create a `DataFrame` with columns for 'timestamp', 'level', and 'message'.  Assign it to the variable `extracted_logs` and print it.  Note that each capture group will create a separate column in the `DataFrame` returned by the `extract` method.\n\n```python\nlog_entries = pd.Series([\n    \"[2023-10-26 10:00:00] INFO: User logged in\",\n    \"[2023-10-26 10:05:30] WARNING: Invalid input\",\n    \"[2023-10-26 10:10:15] ERROR: Database connection failed\",\n    \"[2023-10-26 10:12:45] DEBUG: Processing request\"\n])\n```\n\n2. Given the following `Series` change all of the placeholders to the string `<VALUE>` using the `replace` method.  Save the result in the variable `standardized_text` and print it.  Special characters in the pattern will need to be escaped.\n\n```python\ntext_data = pd.Series([\n    \"Value is {amount}.\",\n    \"The price is [value].\",\n    \"Cost: (number)\",\n    \"Quantity = <qty>\"\n])\n```\n\n3. Given the data provided below, use the `DataFrame.filter` method to select columns ending in `_at`.  Save the result in a variable called `time_columns` and print it.\n\n```python\ndf = pd.DataFrame({\n    \"order_id\": [123, 124, 125, 126],\n    \"customer_name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"],\n    \"order_status\": [\"shipped\", \"cancelled\", \"shipped\", \"delivered\"],\n    \"created_at\": [\"2021-01-05\", \"2021-01-06\", \"2021-01-06\", \"2021-01-07\"],\n    \"updated_at\": [\"2021-01-07\", \"2021-01-07\", \"2021-01-08\", \"2021-01-08\"]\n})\n```\n\n4. Given the provided data, use the `Series.str.contains` method to create a subset for orders which have shipped.  You can us `case=False` to make matches case-insensitive.  Save it in the variable `shipped_orders` and print it.\n\n```python\norder_data = [\n    \"Order #123 has been shipped on 2021-01-05 (Tuesday)\",\n    \"Order #124 has been cancelled\",\n    \"shipment confirmation #125 on 02/06/2021\",\n    \"Order #126 delivered on 01 07 2021\",\n    \"Canceled order #127, refund pending\",\n    \"order #128 - Shipped 2021/03/10\"\n]\norders = pd.Series(order_data)\n```\n\n5. Using the same `orders` `Series`, create a `DataFrame` with columns for `order number`, `date`, and `shipped`.  `shipped` is a `boolean` value. \n Convert the dates to `datetime` which can convert mixed formats.  Note that the modifier `{n}` can be used to specify an exact number of matched characters.  Don't include lines which don't contain a date.  Save the `DataFrame` in a variable called `order_table` and print it.\n\n\n\n---\n\n### **Submit the Notebook for Your Assignment**  \n\nüìå **Follow these steps to submit your work:**  \n\n#### **1Ô∏è‚É£ Get a Sharing Link for Your Assignment**  \n- On the upper right of the Kaggle page, click on Save Version and save, accepting all defaults.  You can just do a quick save.\n- On the upper right, click on Share.  Choose Public, make sure that Allow Comments is on, and copy the public URL to your clipboard.\n\n#### **2Ô∏è‚É£ Submit Your Kaggle Link**  \n- Paste the URL into the **assignment submission form**.  \n\n---",
            "codeExample": "```python\nlog_entries = pd.Series([\n    \"[2023-10-26 10:00:00] INFO: User logged in\",\n    \"[2023-10-26 10:05:30] WARNING: Invalid input\",\n    \"[2023-10-26 10:10:15] ERROR: Database connection failed\",\n    \"[2023-10-26 10:12:45] DEBUG: Processing request\"\n])\n```\n\n```python\ntext_data = pd.Series([\n    \"Value is {amount}.\",\n    \"The price is [value].\",\n    \"Cost: (number)\",\n    \"Quantity = <qty>\"\n])\n```\n\n```python\ndf = pd.DataFrame({\n    \"order_id\": [123, 124, 125, 126],\n    \"customer_name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"],\n    \"order_status\": [\"shipped\", \"cancelled\", \"shipped\", \"delivered\"],\n    \"created_at\": [\"2021-01-05\", \"2021-01-06\", \"2021-01-06\", \"2021-01-07\"],\n    \"updated_at\": [\"2021-01-07\", \"2021-01-07\", \"2021-01-08\", \"2021-01-08\"]\n})\n```\n\n```python\norder_data = [\n    \"Order #123 has been shipped on 2021-01-05 (Tuesday)\",\n    \"Order #124 has been cancelled\",\n    \"shipment confirmation #125 on 02/06/2021\",\n    \"Order #126 delivered on 01 07 2021\",\n    \"Canceled order #127, refund pending\",\n    \"order #128 - Shipped 2021/03/10\"\n]\norders = pd.Series(order_data)\n```",
            "_id": "68f812adc22606ecfa5c7324"
          }
        ],
        "submissionInstructions": "Please submit on time",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Introduction",
          "content": "# **Lesson 07 ‚Äî Data Cleaning and Validation with Pandas**",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7310"
        },
        {
          "subsectionOrder": 2,
          "title": "Lesson Overview",
          "content": "**Learning objective:** Students will learn to clean and standardize real-world datasets using Pandas. They will handle missing data, outliers, duplicates, inconsistent formatting, and categorical variables, while also applying transformations and basic feature engineering techniques to prepare data for analysis or modeling.\n\n\n### **Topics:**\n1. **Handling Missing Data**: Using `dropna()` and `fillna()` to manage null values.\n2. **Data Transformation**: Changing data types and formatting dates for analysis.\n3. **Using Regular Expressions**: Using regex with `str.replace()`, `extract()`, and `contains()` in Pandas.\n4. **Removing Duplicates**: Identifying and removing repeated rows with `drop_duplicates()`.\n5. **Handling Outliers**: Replacing extreme values with statistical measures like the median.\n6. **Standardizing Data**: Lowercasing, trimming whitespace, and mapping values for consistency.\n7. **Validating Data Ranges**: Ensuring numeric values fall within expected bounds.\n8. **Handling Categorical Data**: Encoding categories with label encoding and one-hot encoding using `get_dummies()`.\n9. **Handling Inconsistent Data**: Using regex and string methods to normalize inconsistent entries.\n10. **Feature Engineering**: Binning continuous variables into categories with `pd.cut()`.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7311"
        },
        {
          "subsectionOrder": 3,
          "title": "Handling Missing Data",
          "content": "### **Overview**\nMissing data can affect the accuracy and reliability of analysis. Common approaches include removing or replacing missing values.\n\n### **Key Methods:**\n- `dropna()`: Removes rows or columns with missing values.\n- `fillna()`: Replaces missing values with specified replacements like the mean, median, or a default value.\n\n### **Why Handle Missing Data?**\n- Ensures consistent dataset formatting.\n- Avoids errors during calculations.\n- Preserves data integrity.\n\n### **Code Example:**\n```python\nimport pandas as pd\n\n# Sample DataFrame\ndata = {'Name': ['Alice', None, 'Charlie', 'David'],\n        'Age': [24, None, 22, 35],\n        'Salary': [50000, 60000, None, None]}\ndf = pd.DataFrame(data)\n\n# Drop rows with missing data\ndf_dropped = df.dropna()\n\n# Replace missing values\ndf_filled = df.fillna({'Name': 'Unknown', 'Age': df['Age'].mean(), 'Salary': 0})\n\nprint(\"DataFrame with missing data handled:\")\nprint(df_filled)\n```\n\n### **Explanation:**\n- `dropna()` removes any row that contains a `None` (missing) value.\n- `fillna()` is used to replace missing values. In this case, the `Age` column's missing values are replaced with the mean, and the `Salary` column's missing values are filled with 0.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7312"
        },
        {
          "subsectionOrder": 4,
          "title": "Data Transformation",
          "content": "### **Overview**\nData transformation converts data into consistent formats and types for easier analysis and calculations.\n\n### **Key Tasks:**\n- Converting data types.\n- Formatting date columns for temporal analysis.\n\n### **Why Transform Data?**\n- Ensures compatibility with numerical and time-based functions.\n- Reduces inconsistencies in datasets.\n\n### **Code Example:**\n```python\n# Sample DataFrame\ndata = {'Name': ['Alice', 'Bob', 'Charlie'],\n        'Age': ['24', '27', '22'],\n        'Join_Date': ['2023-01-15', '2022-12-20', '2023-03-01']}\ndf = pd.DataFrame(data)\n\n# Convert 'Age' to integer\ndf['Age'] = df['Age'].astype(int)\n\n# Convert 'Join_Date' to datetime\ndf['Join_Date'] = pd.to_datetime(df['Join_Date'])\n\nprint(\"Transformed DataFrame:\")\nprint(df)\n```\n\n### **Explanation:**\n- `astype(int)` converts the `Age` column, originally stored as strings, into integers.\n- `pd.to_datetime()` converts the `Join_Date` column into Python‚Äôs datetime objects for easier date manipulation and comparison.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7313"
        },
        {
          "subsectionOrder": 5,
          "title": "Using Regular Expressions",
          "content": "### A brief introduction to regular expressions\n\n#### History\n- Invented as a theoretical framework in 1951 by Stephen Cole Kleene\n- First used in 1968 for the QED text editor (Ken Thompson)\n- Used in a variety of Unix tools in the 1970's (ed, lex, vi, grep, awk, emacs)\n- Now the most popular string pattern matching language, available in almost all programming languages\n- Most standardized on the perl version which is more powerful and less verbose than other standards\n\n#### Regular expression syntax and matching\n- Python provide regular expressions through the standard library [re module](https://docs.python.org/3/library/re.html)\n- The [regex HOWTO](https://docs.python.org/3/howto/regex.html#regex-howto) provides a tutorial on usage\n- For more practice with regexes [regex101](https://regex101.com/) is a useful site.\n- Raw strings (`r''`) are preferred for regexes in python to avoid excessive `'\\'` characters\n- All regular characters match themselves\n- Meta characters extend patterns for more complex matches\n  - Meta-characters can be included in patterns by escaping with `‚Äò\\‚Äô`\n  - `<pattern>*` : match 0 or more copies of pattern\n  - `<pattern>+` : match 1 or more copies of pattern\n  - `<pattern>{n}` : match exactly `n` copies of the pattern\n  - `<pattern>?` : match 0 or 1 times\n  - `^<pattern>` : match only at the beginning\n  - `<pattern>$` : match only at the end\n  - Character sets: `[a-z0-9]` ‚Äì match any of the characters in the set\n  - Excluding characters: `[^A-Z]` `^` means match characters not in the set (e.g. anything but capital letters)\n    - `^` has a different meaning in this context, specifying a character exclusion rather than the start of the string\n  - `\\d` : match digits (short for `[0-9]`), `\\D` not a digit\n  - `\\w` : match any alphanumeric (word - short for `[a-zA-Z0-9_]`), `\\W` not a word character\n  - `\\s` : match whitespace, `\\S` not a whitespace character\n  - `.` : match any character except newline. Use `\\.` to match a period.  I general `\\` escapes special characters.\n  - Use `|` to specify alternates: `[0-9]+|[a-z]+`:  e.g. match digits or lowercase letters\n    - Surrounding the alternative by `()` limits the scope of the alternatives e.g. `r'start ([0-9]+|[a-z]+) end'`.  This also introduces a match group which is not necessarily used.\n  - Group using `()` to capture substrings\n\n#### Examples\n- `^[a-zA-Z]\\w+`   # Must start at the beginning of the string with a letter followed by any word character\n- `[0-9]+|[a-z]+`  # Match either one or more digits or one or more lowercase letters (but not both)\n- `[a-z]|[0-9]+`   # Match one lowercase letter or one or more digits (but not both)\n- `^\\s*(\\w+)\\s*$`  # Capture a word which may have whitespace before or after\n\n#### Debugging regular expressions\n\nRegular expressions are greedy\n- Eeach part of the pattern matches as many characters as possible\n- Bugs often involve a part of the pattern matching too much (or all) of the rest of the string\n- e.g. putting ^.+ at the beginning of the pattern will match all the characters\n  - Leaving nothing for the rest of the pattern\n\n\n### Using regular expressions with the Python Standard Library\n\nIn the python standard library [re module](https://docs.python.org/3/library/re.html), regular expressions are precompiled into a [pattern object](https://docs.python.org/3/library/re.html#re-objects).  The pattern object can then be matched against entire strings using the [match() method](https://docs.python.org/3/library/re.html#re.Pattern.match) or the [search() method](https://docs.python.org/3/library/re.html#re.Pattern.search) can be used to scan for the pattern in a string.  These methods return `None` if there is no match.  If there is a successful match, a [match object](https://docs.python.org/3/library/re.html#re.Match) is returned.  The [group() method](https://docs.python.org/3/library/re.html#re.Match.group) can be used to retrieve the entire match or subgroups defined using parentheses.  For this class, we will be using regular expressions with Pandas as describe below, however it can be helpful to use the standard library regular expressions to develop and test a pattern.  It's also an important part of the Python ecosystem.  Here are some examples:\n\n```python\nimport re\n# compile the regular expression\n# This regex captures the username from a gmail address as a subgroup\n# It requires one or more word chacaters at the beginning, then any number of words, digits, periods and '-''s\n# The first `()` will be available a group(1) if there is a match\ngmail = re.compile(r'(\\w+[\\w\\d\\.\\-]*)@gmail.com')\nsearch_target = 'Boa-Dreamcode.public@gmail.com'\n# match the entire string\nmatch = gmail.match(search_target)\n# print the group and subgroup\nprint(match.group()) # => Boa-Dreamcode.public@gmail.com (same as group(0))\nprint(match.group(1)) # => Boa-Dreamcode.public\n# now serch within a longer string\nsearch_target = 'My email is Boa-Dreamcode.public@gmail.com, what is yours?'\nmatch = gmail.search(search_target)\nprint(match.group()) # => Boa-Dreamcode.public@gmail.com (same as group(0))\nprint(match.group(1)) # => Boa-Dreamcode.public\n```\n\n### Using regular expressions with Pandas\n  \n Pandas [Series.str](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#string-handling) provides a variety of methods which access or manipulate the values of a Series as strings.  Many of these methods support regular expressions.  All of the examples below assume you have imported `pandas as pd`.  The [Series.filter](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.filter.html#pandas.Series.filter) method also supports regular expressions.  The methods available with `Series.str` are different from those provided by the builtin `str` type.\n\n #### Using `replace`\n\n Sometimes phone numbers or ID fields can contain non-numeric characters (such as dashes, parentheses, or letters) that you want to remove.\n\n ```python\n df = pd.DataFrame({\n    'phone_number': ['(123) 456-7890', '+1-555-123-4567', '555.456.7890']\n})\n# Remove all non-digit characters\ndf['phone_number_clean'] = df['phone_number'].str.replace(r'\\D', '', regex=True)\nprint(df)\n```\n\nRemoving html tags from scraped data.\n\n```python\ndf = pd.DataFrame({\n    'html_content': [\n        '<p>This is a paragraph.</p>',\n        '<div>Some <strong>bold</strong> text</div>'\n    ]\n})\n# <.*?> matches <, then any characters as few as possible (.*? is a non-greedy match), then >\ndf['text_only'] = df['html_content'].str.replace(r'<.*?>', '', regex=True)\nprint(df)\n```\n\n#### Using `extract`\n\nCapture the domain name from email addresses.  Note that the `extract` method only uses regular expressions, whereas they are optional and must be specified for `replace`.\n\n```python\ndf = pd.DataFrame({\n    'email': [\n        'john.doe@example.com',\n        'jane_smith@my-domain.org',\n        'user123@anotherdomain.net'\n    ]\n})\ndf['domain'] = df['email'].str.extract(r'@(\\w+[\\w\\.-]+)')\nprint(df)\n```\nMatch groups can be given names.  When named groups are used with the `dataFrame.extract` method the group names are used as column names in the resulting `DataFrame`.\n\n```python\nseries = pd.Series(['Tom-25-USA', 'Anna-30-UK', 'John-22-Canada'])\npattern = r'(?P<Name>\\w+)-(?P<Age>\\d+)-(?P<Country>\\w+)'\nresult = series.str.extract(pattern)\nprint(result)\n\n```\n\n#### Using `contains`\n\nGet all rows which contain valid emails.\n\n```python\ndf = pd.DataFrame({\n    'email': ['test@example.com', 'invalid-email', 'hello@mydomain.org']\n})\nvalid_emails = df[df['email'].str.contains(r'^\\w+[\\w\\.-]*@\\w+[\\w\\.-]+\\.\\w+$')]\nprint(valid_emails)\n```\n#### Combining filters using bitwise operators\nThe `contains` method returns a `Series` of boolean values also known as a filter.  Bitwise operators (&, |, ~) can be used to combine or invert filters.  Don't confuse these with the boolean operators (`and`, `or`, and `not`). Also, note that the raw strings used to define regular expressions can be entered on multiple lines.  Regexs can be long, and listing them on multiple lines improves readability.  Here a series of alternatives are listed one per line with a trailing `'|'`.  Note that they aren't separated by commas.\n\n```python\norders = [\n    \"Order 1: 2x Cheddar, 1x Gouda\",\n    \"Order 2: 3x Stilton, 2x Rye Crackers\",\n    \"Order 3: 2x Saltines\",\n    \"Order 4: 1x Camembert, 2x Jahrlsberg\",\n    \"Order 5: 2x Gouda, 2x Rye Crackers\",\n    \"Order 6: 1x Ritz, 1x Jahrlsberg\",\n    \"Order 7: 1x Parmesan, 1x Brie\",\n    \"Order 8: 3x Saltine Crackers\",\n    \"Order 9: 2x Rye Crackers\",\n    \"Order 10: 2x Mozzarella, 1x Cheddar\",\n    \"Order 11: 1X Water Crackers\"\n    \"Order 12: 3x Blue Cheese\",\n    \"Order 13: 1x Triscuits\",\n    \"Order 14: 1x Butter Crackers, 2x Multigrain Crackers\",\n    \"Order 15: 1x Feta\",\n    \"Order 16: 1x Havarti\",\n    \"Order 17: 2x Wheat Crackers\",\n    \"Order 18: 1x Ricotta\",\n    \"Order 19: 1x Garlic Herb Crackers\"\n]\norders = pd.Series(orders)\nfavored_cheeses = orders.str.contains(r'Cheddar|'\n                                       r'Stilton|'\n                                       r'Camembert|'\n                                       r'Jahrlsberg|'\n                                       r'Gouda', case=False, regex=True)\nfavored_crackers = orders.str.contains(r'Ritz|'\n                                       r'Triscuit|'\n                                       r'Rye Crackers|'\n                                       r'Multigrain Crackers|'\n                                       r'Water Crackers', case=False, regex=True)\nprint(orders[favored_cheeses | favored_crackers])\nprint(orders[favored_cheeses & favored_crackers])\nprint(orders[~favored_cheeses])\n```\n\n#### Using `filter`\n\nInstead of specifying columns one by one, you can select or drop columns whose names match a pattern using DataFrame.filter.\n\n```python\ndf = pd.DataFrame({\n    'col_2021': [1, 2, 3],\n    'col_2022': [4, 5, 6],\n    'col_other': [7, 8, 9]\n})\n# Select columns that end with digits\ndf_year = df.filter(regex=r'\\d+$')  \nprint(df_year)\n```",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7314"
        },
        {
          "subsectionOrder": 6,
          "title": "Removing Duplicates",
          "content": "### **Overview**\nDuplicates can distort analysis by inflating metrics or introducing bias. Identifying and removing them ensures data quality.\n\n### **Key Method:**\n- `duplicated()`: Identifies duplicates.\n- `drop_duplicates()`: Removes duplicate rows.\n\n### **Code Example:**\n```python\n# Sample DataFrame\ndata = {'Name': ['Alice', 'Bob', 'Alice', 'David'],\n        'Age': [24, 27, 24, 35],\n        'Salary': [50000, 60000, 50000, 80000]}\ndf = pd.DataFrame(data)\n\n# Remove duplicates\ndf_no_duplicates = df.drop_duplicates()\n\nprint(\"DataFrame with duplicates removed:\")\nprint(df_no_duplicates)\n```\n\n### **Explanation:**\n- `drop_duplicates()` removes rows where the entire record is a duplicate of another.\n- `drop_duplicates(subset='Name')` removes rows where the `Name` column is duplicated, keeping only the first occurrence of each name.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7315"
        },
        {
          "subsectionOrder": 7,
          "title": "Handling Outliers",
          "content": "### **Overview**\nOutliers are extreme values that deviate significantly from other observations and can bias statistical calculations.\n\n### **Common Approach:**\n- Replace outliers with statistical measures like the median.\n\n### **Code Example:**\n```python\n# Replace outliers in 'Age' (e.g., Age > 100 or Age < 0)\ndf['Age'] = df['Age'].apply(lambda x: df['Age'].median() if x > 100 or x < 0 else x)\n\nprint(\"DataFrame after handling outliers:\")\nprint(df)\n```\n\n### **Explanation:**\n- Outliers in the `Age` column that are greater than 100 or less than 0 are replaced by the median value of the `Age` column.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7316"
        },
        {
          "subsectionOrder": 8,
          "title": "Standardizing Data",
          "content": "### **Overview**\nStandardizing text data ensures consistency, making it easier to group, filter, and analyze.\n\n### **Key Techniques:**\n- Convert text to lowercase and strip whitespace.\n- Standardize inconsistent entries.\n\n### **Code Example:**\n```python\n# Standardize 'Name' column\ndf['Name'] = df['Name'].str.lower().str.strip()\n\n# Standardize 'City' column with mapping\ndf['City'] = df['City'].replace({'ny': 'New York', 'la': 'Los Angeles'})\n\nprint(\"Standardized DataFrame:\")\nprint(df)\n```\n\n### **Explanation:**\n- The `Name` column is standardized by converting all entries to lowercase and stripping any extra whitespace.\n- The `City` column is standardized by replacing `ny` with `New York` and `la` with `Los Angeles`.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7317"
        },
        {
          "subsectionOrder": 9,
          "title": "Validating Data Ranges",
          "content": "### **Overview**\nValidating data ranges ensures all values fall within acceptable limits, avoiding invalid or erroneous data.\n\n### **Example Task:**\n- Ensure ages are between 18 and 65. Replace invalid values with `NaN` and fill them with the median.\n\n### **Code Example:**\n```python\n# Replace invalid ages with NaN\ndf['Age'] = df['Age'].apply(lambda x: x if 18 <= x <= 65 else None)\n\n# Fill missing values with median\ndf['Age'] = df['Age'].fillna(df['Age'].median())\n\nprint(\"DataFrame after validating age ranges:\")\nprint(df)\n```\n\n### **Explanation:**\n- Any age outside the range of 18 to 65 is replaced with `None` (NaN).\n- Missing values in the `Age` column are filled with the median value of the column.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7318"
        },
        {
          "subsectionOrder": 10,
          "title": "Handling Categorical Data",
          "content": "### **Overview**\nHandling categorical data involves encoding non-numeric values, which is especially useful for machine learning models that require numerical input.\n\n### **Key Techniques:**\n- **Label Encoding**: Converting each category into a number.\n- **One-Hot Encoding**: Creating binary columns for each category.\n\n### **Why Handle Categorical Data?**\n- Many machine learning algorithms require numerical data, so we need some way to convert categories into numbers.\n- Proper encoding helps preserve the categorical structure in the data. There are different ways to represent categorical data numerically: with [one hot encoding](https://www.datacamp.com/tutorial/one-hot-encoding-python-tutorial) each category is represented in a binary fashion as present or absent: this is a very popular technique in machine learning. \n- In pandas, one-hot-encoding is implemented with the `get_dummies()` function. \n\n### **Code Example:**\n```python\n# Sample DataFrame with categorical data\ndata = {'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']}\ndf = pd.DataFrame(data)\n\n# Label encoding: Convert categories to numbers\ndf['Color_Label'] = df['Color'].map({'Red': 1, 'Blue': 2, 'Green': 3})\n\n# One-Hot Encoding: Create binary columns for each category\ndf_encoded = pd.get_dummies(df['Color'], prefix='Color')\n\nprint(\"DataFrame with Categorical Data Handled:\")\nprint(df_encoded)\n```\n\n### **Explanation:**\n- **Label Encoding** maps the `Color` column's categories to integer values.\n- - **One-Hot Encoding** use the `get_dummies()` function to create binary columns for each unique value in the `Color` column. \n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c7319"
        },
        {
          "subsectionOrder": 11,
          "title": "Handling Inconsistent Data",
          "content": "### **Overview**\nInconsistent data can result from typos, different formats, or various naming conventions. Handling inconsistencies ensures uniformity in the dataset.\n\n### **Key Techniques:**\n- **Fuzzy Matching**: Identifying and standardizing similar but non-exact values.\n- **Regex (Regular Expressions)**: Using patterns to extract or replace inconsistent data.\n\n### **Why Handle Inconsistent Data?**\n- Improves the quality of data for analysis.\n- Helps identify patterns across otherwise unmatchable data points.\n\n### **Code Example:**\n```python\nimport re\n\n# Sample DataFrame with inconsistent data\ndata = {'City': ['New York', 'new york', 'San Francisco', 'San fran']}\ndf = pd.DataFrame(data)\n\n# Standardize text data (convert to lowercase and strip spaces)\ndf['City'] = df['City'].str.lower().str.strip()\n\n# Use Regex to replace shorthand names\ndf['City'] = df['City'].replace({'san fran': 'san francisco'})\n\nprint(\"DataFrame with Inconsistent Data Handled:\")\nprint(df)\n```\n\n### **Explanation:**\n- **String standardization**: Converts all entries in the `City` column to lowercase and removes extra spaces.\n- **Regex**: Matches and replaces shorthand for cities with their full names.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c731a"
        },
        {
          "subsectionOrder": 12,
          "title": "Feature Engineering",
          "content": "### **Overview**\nFeature engineering involves creating new features from existing ones in raw data. \nThis is typically used as a first step before the data is fed to machine\nlearning algorithms. We will go over one way to extract features from data, *binning*\ninto discrete categories, but [feature engineering is a large subfield of its own](https://www.ibm.com/think/topics/feature-engineering).\n\n### **Key Techniques:**\n- **Binning**: Categorizing continuous data into discrete bins.\n- **Principal component analysis (PCA)**: for automated extraction of important combinations of features from high dimensional datasets (this is a fairly advanced topic, but if you are interested in an intuitive overview of PCA see [this video](https://www.youtube.com/watch?v=ZgyY3JuGQY8)). \n\n### **Why Feature Engineering?**\n- New features can reveal hidden patterns and relationships.\n- Reduce noise in the raw data.\n- Improves model performance in machine learning.\n\n### **Code Example:**\n```python\n# Sample DataFrame with numerical data\ndata = {'Age': [24, 35, 30, 45, 60]}\ndf = pd.DataFrame(data)\n\n# Binning Age into age groups\nbins = [0, 30, 60, 100]\nlabels = ['Young', 'Middle-Aged', 'Old']\ndf['Age_Group'] = pd.cut(df['Age'], bins=bins, labels=labels)\n\nprint(\"DataFrame after Feature Engineering:\")\nprint(df)\n```\n\n### **Explanation:**\n- **Binning**: Converts `Age` into categories like 'Young', 'Middle-Aged', and 'Old' based on defined intervals.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c731b"
        },
        {
          "subsectionOrder": 13,
          "title": "Summary",
          "content": "In this lesson, you learned how to:\n1. Handle missing data with `dropna()` and `fillna()`.\n2. Transform data types and formats.\n3. Remove duplicate records.\n4. Identify and manage outliers.\n5. Standardize text data for consistency.\n6. Validate data ranges to ensure accuracy.\n7. Handle categorical data with encoding methods.\n8. Address inconsistent data with fuzzy matching and regex.\n9. Apply feature engineering for better insights and analysis.\n\nThese techniques are essential for maintaining clean, reliable datasets, ready for analysis. Explore the [Pandas Documentation](https://pandas.pydata.org/docs/) to deepen your understanding.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812adc22606ecfa5c731c"
        }
      ]
    },
    {
      "id": "68f812aec22606ecfa5c7326",
      "lessonNumber": 8,
      "title": "Lesson 08 ‚Äî Introduction to Databases and SQL",
      "status": "pending",
      "assignment": {
        "title": "Assignment for Lesson 8",
        "objective": "Introduction to Databases and SQL**\n\nFor this assignment, you create code in your python_homework/assignment8 folder.",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [
          {
            "taskNumber": 1,
            "title": "Create a New SQLite Database",
            "description": "1. Within your python_homework repository, create an `assignment8` git branch.\n2. Make the `assignment8`  folder the working folder.  Within the `assignment8` folder, create a file `sql_intro.py`.\n3. Write code to connect to a new SQLite database, `../db/magazines.db` and to close the connection.\n4. Execute the script and confirm the database file is created.  Note: All SQL statements should be executed within a `try` block, followed by a corresponing `except` block, because any SQL statement can cause an exception to be raised.\n\n---",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c7336"
          },
          {
            "taskNumber": 2,
            "title": "Define Database Structure",
            "description": "We have publishers that publish magazines.  Each publisher has a unique name, and so does each magazine.  There is a one-to-many relationship between publishers and magazines.  We also have subscribers, and each subscriber has a name and an address.  We have a many-to-many association between subscribers and magazines, because a subscriber may subscribe to several magazines, and a magazine may have many subscribers.  So, we have a join table called subscriptions.  The subscriptions table also stores the expiration_date (a string) for the subscription.  All the names, the address, and the expiration_date must be non-null.  \n\n1. Think for a minute.  There is a one-to-many relationship between publishers and magazines.  Which table has a foreign key? Where does the foreigh key point?  How about the subscriptions table: What foreigh keys does it have?\n\n2. Add SQL statements to `sql_intro.py` that create the following tables:\n   - `publishers`\n   - `magazines`\n   - `subscribers`\n   - `subscriptions`\n   Be sure to include the columns you need in each, with the right data types, with UNIQUE and NOT NULL constraints as needed, and with foreign keys as needed.  You can reuse column names if you choose, i.e. you might have a name column for publishers and a name column for magazines.  By the way, if you mess up this or the following steps, you can just delete `db/magazines.db`.\n\n3. Open the `db/magazines.db` file in VSCode to confirm that the tables are created.\n\n---",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c7337"
          },
          {
            "taskNumber": 3,
            "title": "Populate Tables with Data",
            "description": "1. Add the following line to sql_intro.py, right after the statement that connects to the database:\n   ```\n   conn.execute(\"PRAGMA foreign_keys = 1\")\n   ```\n   This line tells SQLite to make sure the foreign keys are valid.\n2. Create functions, one for each of the tables, to add entries.  Include code to handle exceptions as needed, and to ensure that there is no duplication of information.  The subscribers name and address columns don't have unique values -- you might have several subscribers with the same name -- but when creating a subscriber you should check that you don't already have an entry where BOTH the name and the address are the same as for the one you are trying to create.\n3. Add code to the main line of your program to populate each of the 4 tables with at least 3 entries.  Don't forget the `commit`!\n4. Run the program several times.  View the database to ensure that you are creating the right information, without duplication.\n\n---",
            "codeExample": "```\n   conn.execute(\"PRAGMA foreign_keys = 1\")\n   ```",
            "_id": "68f812aec22606ecfa5c7338"
          },
          {
            "taskNumber": 4,
            "title": "Write SQL Queries",
            "description": "1. Write a query to retrieve all information from the subscribers table.\n2. Write a query to retrieve all magazines sorted by name.\n3. Write a query to find magazines for a particular publisher, one of the publishers you created.  This requires a `JOIN`. \n4. Add these queries to your script.  For each, print out all the rows returned by the query.\n\n---",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c7339"
          },
          {
            "taskNumber": 5,
            "title": "Read Data into a DataFrame",
            "description": "You will now use Pandas to create summary data from the `../db/lesson.db` database you populated as part of the lesson.  We want to find out how many times each product has been ordered, and what was the total price paid by product.\n\n1. While still within the `python_homework/assignment8` directory, create a program, `sql_intro_2.py`.\n2. Read data into a DataFrame, as described in the lesson.  The SQL statement should retrieve the line_item_id, quantity, product_id, product_name, and price from a JOIN of the line_items table and the product table. Hint: Your `ON` statement would be `ON line_items.product_id = products.product_id`.\n3. Print the first 5 lines of the resulting DataFrame.  Run the program to make sure this much works.\n4. Add a column to the DataFrame called \"total\".  This is the quantity times the price.  (This is easy: `df['total'] = df['quantity'] * df['price']`.)  Print out the first 5 lines of the DataFrame to make sure this works.\n5. Add groupby() code to group by the product_id.  Use an agg() method that specifies 'count' for the line_item_id column, 'sum' for the total column, and 'first' for the 'product_name'.  Print out the first 5 lines of the resulting DataFrame.  Run the program to see if it is correct so far.\n6. Sort the DataFrame by the product_name column.\n7. Add code to write this DataFrame to a file `order_summary.csv`, which should be written in the `assignment8` directory.  Verify that this file is correct.\n\nAs we'll learn in the next lesson, the ordering, grouping, count, and sum operations can be done in SQL, more efficiently than in Pandas.  The key concepts of pandas and SQL overlap very strongly.\n\n---\n\n### Submit Your Assignment on GitHub**  \n\nüìå **Follow these steps to submit your work:**  \n\n#### **1Ô∏è‚É£ Add, Commit, and Push Your Changes**  \n- Within your python_homework folder, do a git add and a git commit for the files you have created, so that they are added to the `assignment8` branch.\n- Push that branch to GitHub. \n\n#### **2Ô∏è‚É£ Create a Pull Request**  \n- Log on to your GitHub account.\n- Open your `python_homework` repository.\n- Select your `assignment8` branch.  It should be one or several commits ahead of your main branch.\n- Create a pull request.\n\n#### **3Ô∏è‚É£ Submit Your GitHub Link**  \n- Your browser now has the link to your pull request.  Copy that link. \n- Paste the URL into the **assignment submission form**. \n\n---",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c733a"
          }
        ],
        "submissionInstructions": "Please submit on time",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Introduction",
          "content": "# **Lesson 08 ‚Äî Introduction to Databases and SQL**",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7327"
        },
        {
          "subsectionOrder": 2,
          "title": "Lesson Overview",
          "content": "**Learning objective**: Students will gain foundational knowledge of SQL databases using Python and SQLite. They will define relational schemas, insert and query data using SQL, handle many-to-many relationships, and interact with databases directly from Pandas for analysis and reporting.\n\n**Topics**:\n1. Introduction to SQL: What SQL is, why relational databases matter, and how constraints, associations, and transactions work.\n2. SQLite Setup: Installing dependencies (Windows only) and connecting to a local SQLite database.\n3. Defining Tables: Creating tables with `CREATE TABLE` statements and specifying primary keys, foreign keys, and constraints.\n4. Populating Tables: Using `INSERT INTO`and parameterized queries to add data to tables.\n5. Querying Data: Writing `SELECT` queries, using `WHERE`, `ORDER BY`, and comparison operators.\n6. Foreign Keys and Relationships: Creating and managing associations using `JOIN` tables.\n7. Joins and Complex Queries: Writing multi-table joins with `JOIN`, `LEFT JOIN`, and `AS` aliases.\n8. Modifying Data: Updating and deleting records using `UPDATE` and `DELETE` statements.\n9. SQL Practice: Practicing queries using the `sqlcommand.py` interface and exploring a sample dataset.\n10. Using SQL with Pandas: Loading SQL query results directly into Pandas DataFrames using `pd.read_sql_query()`.\n11. (Optional) Additional Practice: Guided practice via the SQLBolt tutorial.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7328"
        },
        {
          "subsectionOrder": 3,
          "title": "Setup",
          "content": "An additional package is needed for this lesson, but ONLY ON WINDOWS.  Do not install this package if you are on a Mac or Linux, because on those platforms, the capability is part of the Python base.  If you are on Windows, then within the VSCode terminal of your `python_homework` folder, type:\n\n```bash\npip install pyreadline3\n```\n\n### **Topics:**\n\n1. What SQL Is, and Why it is Used\n2. Creating a New SQLite Database\n3. Defining the Database Structure\n4. Populating Tables with Data\n5. Writing SQL Queries\n6. Creating Entries with Foreign Keys\n7. More complicated queries, incuding JOINs\n8. The UPDATE Statement\n9. The Delete Statement\n10. SQL Query Practice\n11. SQL from Pandas\n12. (Optional) More SQL Practice\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7329"
        },
        {
          "subsectionOrder": 4,
          "title": "What SQL Is, and Why it is Used",
          "content": "SQL is the language used to access relational databases.  In a relational database, the data is stored in tables, each of which looks like a spreadsheat.  The database has a schema, and for each table in the database, the schema describes the columns in each table, giving each a name and a datatype.  There aren't many datatypes in a relational database.  We'll use SQLite, and SQLite only supports TEXT, NUMERIC, INTEGER, REAL, or BLOB datatypes.  Other datatypes are supported in SQLite by mapping them to one of these four.  (Other SQL implementations support more.)  One can compare this to no-SQL databases like MongoDB, when you can store any JSON document you like.  The schema can seem like a straitjacket, but it is really more a set of rails, organizing data into a structured form.\n\nRead the following introduction: <https://www.theodinproject.com/lessons/databases-databases-and-sql>.  Or, if you know this stuff, jump to the bottom of that page and do the Knowledge Check.  Be sure that you understand the concepts of Primary Key and Foreign Key.\n\nThere are two important words left out of that introduction: Association and Transaction.\n\n### **Associations**\n\nAn association exists between tables if one table has a foreign key that points to the other.  Consider the following cases:\n\n1. An application has a `users` table and a `user_profiles` table.  Each record in the `user_profiles` table has a foreign key, which is the primary key of a record in the `users` table.  This is a one-to-one association.\n2. An application has blogs.  Each blog has a series of posts.  The application might have a `blogs` table and a `posts` table.  Each record in the `posts` table would have a foreign key for a `blogs` table record, indicating the blog to which it belongs.  This is a one-to-many association, as one blog has many posts.\n3. A magazine publisher has magazines and subscribers.  Each subscriber may subscribe to several magazines, and each magazine may have many subscribers.  Now we have a problem.  \nWe can't put a list of subscribers into a magazine record. Relational database records can't contain lists.  For a given magazine, we could create one record for each subscriber, but we'd be duplicating all the information that describes the magazine many times over.  Similarly, there is no way for the `subscribers` table to contain records for each magazine for each subscriber.  So, you need a table in the middle, sometimes called a **join table**.  In this case, the join table might be `subscriptions`.  Each subscription record has two foreign keys, one for the magazine and one for the subscriber.  This is a many-to many association.\n\n### **Transactions**\n\nA transaction is a write operation on an SQL database that guarantees consistency.  Consider a banking operation.  A user wants to transfer money from one account to another.  The sequence of SQL operations is as follows (this is pseudocode of course):\n\n- Begin the transaction.\n- Read the amount in account A to make sure there's enough.\n- Update that record to decrease the balance by the desired amount.\n- Update that record to increase the balance by the desired amount.\n- Commit the transaction\n\nThe transaction maintains consistency.  When the read occurs, that entry is locked. (This depends on the isolation level and other stuff we won't get into now.)  That lock is important, as otherwise there could be another withdrawal from the account that happens after the read but before the update, and the account would go overdrawn.  Neither do you want the update that decreases the balance to complete while the update that increases the balance in the other account fails.  That would anger the user, and justifiably so.  With transactions, either both write operations succeed or neither succeeds.\n\nRelational databases' strength, by comparision with no-SQL databases, is the efficient handing of structured and interrelated data and transactional operations on that data.\n\n### **Constraints**\n\nWhen a table is defined in the schema, one or several **constraints** on the values may also be specified.\n\n- Datatype constraints: One constraint comes from the datatype of the column: you can't put a TEXT value in an INTEGER column, etc.  \n- NOT NULL constraint:  When present, it means that whenever a record is created or updated, that column in the record must have a value.  \n- UNIQUE constraint: You wouldn't want several users to have the same ID for example.  \n- FOREIGN KEY constraint.  In the blog example above, each post must belong to a blog, meaning that the post record has the blog's primary key as a foreign key.  Otherwise you'd have a post that belonged to no blog, a worthless situation.\n\nIf you try to create a record that doesn't comply with constraints, or update one in violation of constraints, you get an exception.\n\nSQLite, by default, does not turn on the foreign key constraint, but in the examples below, and in most real world situations, you will.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c732a"
        },
        {
          "subsectionOrder": 5,
          "title": "Creating a New SQLite Database",
          "content": "### **Overview**\n\nSQLite is a file-based database, meaning the database itself is stored in a file on disk. Python's `sqlite3` module allows easy interaction with SQLite databases.  It is built into Python, so there is nothing more to install.\n\n### **Creating a Database:**\n\nThe `sqlite3.connect()` statement in Python creates a database if it does not exist.  Then it connects to that database.\n\n1. Open VSCode for your `python_homework` folder.  As you will create files as part of the lesson, create your `assignment8` branch now.\n2. Open your code editor within your `python_homework/assignment8` folder.  Create a new Python script file called `school_a.py`.  In your VSCode terminal, `cd` to the assignment8 folder.\n3. Add the following code to that program to create or connect to a new SQLite database.\n\n### **Example Code:**\n\n```python\nimport sqlite3\n\n# Connect to a new SQLite database\nwith  sqlite3.connect(\"../db/school.db\") as conn:  # Create the file here, so that it is not pushed to GitHub!\n    print(\"Database created and connected successfully.\")\n\n# The \"with\" statement closes the connection at the end of that block.  You could close it explicitly with conn.close(), but in this case\n# the \"with\" statement takes care of that.\n\n```\nRun the program to create a database.\n\n**Outcome:**\n\nRunning the script creates a `school.db` file in the db folder.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c732b"
        },
        {
          "subsectionOrder": 6,
          "title": "Defining the Database Structure",
          "content": "### **Overview**\n\nDatabases store structured data in tables. SQL queries allow you to define the schema of the tables by specifying columns, data types, and relationships.\n\n### **Steps:**\n\n1. Use SQL `CREATE TABLE` statements to define the structure of your database.  This code is added to `school_a.py`.  **Note** It is best to use `CREATE TABLE IF NOT EXISTS`, as in the example code that follows this section.  Otherwise your code will fail the second time you run it, as the table already exists.\n2. Execute these queries within your Python script using the `execute()` method of a database cursor.\n\n**Tables:**\n\n- **Students:** Contains `student_id`, `name`, `age`, and `major`.\n- **Courses:** Contains `course_id`, `course_name`, and `instructor_name`.\n- **Enrollments:** Contains `enrollment_id`, `student_id`, and `course_id`.\n\n### **Example Code:**\n\n```python\nimport sqlite3\n\n# Connect to the database\nwith sqlite3.connect(\"../db/school.db\") as conn:\n    cursor = conn.cursor()\n\n    # Create tables\n    cursor.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS Students (\n        student_id INTEGER PRIMARY KEY,\n        name TEXT NOT NULL UNIQUE,\n        age INTEGER,\n        major TEXT\n    )\n    \"\"\")\n\n    cursor.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS Courses (\n        course_id INTEGER PRIMARY KEY,\n        course_name TEXT NOT NULL UNIQUE,\n        instructor_name TEXT\n    )\n    \"\"\")\n\n    cursor.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS Enrollments (\n        enrollment_id INTEGER PRIMARY KEY,\n        student_id INTEGER,\n        course_id INTEGER,\n        FOREIGN KEY (student_id) REFERENCES Students (student_id),\n        FOREIGN KEY (course_id) REFERENCES Courses (course_id)\n    )\n    \"\"\")\n\n    print(\"Tables created successfully.\")\n```\n\n### Check for Understanding\n\n1. What kind of association exists between Students and Enrollments?\n\n2. What kind of association exists between Students and Courses?\n\nAnswers:\n\n1. A student may have several or many enrollments, but for each enrollment there is only one student.  This is a one-to-many association between Students and Enrollments.  The student_id is the foreign key.\n\n2. A student may enroll in many courses, and a given course may have many students enrolled.  This is a many-to-many association.  The Enrollments table acts as the join table to tie the two together, and both of its foreign keys are needed to manage the association.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c732c"
        },
        {
          "subsectionOrder": 7,
          "title": "Populating Tables with Data",
          "content": "### **Overview**\n\nPopulating tables with data helps simulate real-world scenarios, allowing you to practice querying data. Use the `INSERT INTO` SQL query to insert sample records into your tables.\n\n### **Steps:**\n\n1. Create a file in the assignment7 folder called `school_b.py`. Within it, connect to the database, create a curson, and use SQL `INSERT INTO` queries to add data to the tables.\n2. Add a commit() statement\n\n### **Example Code:**\n\n```python\nimport sqlite3 \n\n# Connect to the database\nwith sqlite3.connect(\"../db/school.db\") as conn:\n    conn.execute(\"PRAGMA foreign_keys = 1\") # This turns on the foreign key constraint\n    cursor = conn.cursor()\n\n    # Insert sample data into tables\n    cursor.execute(\"INSERT INTO Students (name, age, major) VALUES ('Alice', 20, 'Computer Science')\")\n    cursor.execute(\"INSERT INTO Students (name, age, major) VALUES ('Bob', 22, 'History')\") \n    cursor.execute(\"INSERT INTO Students (name, age, major) VALUES ('Charlie', 19, 'Biology')\") \n    cursor.execute(\"INSERT INTO Courses (course_name, instructor_name) VALUES ('Math 101', 'Dr. Smith')\")\n    cursor.execute(\"INSERT INTO Courses (course_name, instructor_name) VALUES ('English 101', 'Ms. Jones')\") \n    cursor.execute(\"INSERT INTO Courses (course_name, instructor_name) VALUES ('Chemistry 101', 'Dr. Lee')\") \n\n    conn.commit() \n    # If you don't commit the transaction, it is rolled back at the end of the with statement, and the data is discarded.\n    print(\"Sample data inserted successfully.\")\n```\nNote that you do not have to specify the primary keys (student_id and course_id), although you can.  If you don't specify the primary keys, these are chosen for you, and the primary key column will never be null and will always have unique values.  If you do specify the primary keys, they have to be unique -- there can be no record in that table with the same primary key, or you get an exception.  You notice that there is no 'begin' statement for the commit.  By default, SQLite begins a transaction for you automatically.\n\nAt this point, you should install the SQLite Viewer plugin for your VSCode.  Once you've done that, open the db/school.db file in VSCode, and the tables you have created are displayed, along with any data you have added.\n\nThe INSERT statement can insert several rows in one statement.  You specify multiple sets of values for each of the columns, as follows (you don't need to add this to your program):\n```python\n    cursor.execute(\"INSERT INTO Students (name, age, major) VALUES ('Alice', 20, 'Computer Science'), ('Bob', 22, 'History')\")\n```\n\nNow, you'd like to insert into the Enrollments table as well -- but, each enrollment record has foreign keys, which are primary keys from the Students and Courses tables.  You didn't choose the primary keys above, so you don't know what they are.  You'll solve that problem below.\n\n### Check for Understanding\n\n1. What happens if you try to run your program twice?\n\nAnswer:\n\n1. You get an exception!  Student and course names must be unique.  You need to handle the exceptions!  So change your code as follows:\n\nBut, you still get an exception, for the next Insert.  You need to add the try/except for each of the insert statements.  This seems to violate the DRY principle (Do not Repeat Yourself).  You solve this by creating functions, as follows\n\n```python\nimport sqlite3 \n\n# Connect to the database\n\ndef add_student(cursor, name, age, major):\n    try:\n        cursor.execute(\"INSERT INTO Students (name, age, major) VALUES (?,?,?)\", (name, age, major))\n    except sqlite3.IntegrityError:\n        print(f\"{name} is already in the database.\")\n\ndef add_course(cursor, name, instructor):\n    try:\n        cursor.execute(\"INSERT INTO Courses (course_name, instructor_name) VALUES (?,?)\", (name, instructor))\n    except sqlite3.IntegrityError:\n        print(f\"{name} is already in the database.\")\n\nwith sqlite3.connect(\"../db/school.db\") as conn:\n    conn.execute(\"PRAGMA foreign_keys = 1\") # This turns on the foreign key constraint\n    cursor = conn.cursor()\n\n    # Insert sample data into tables\n\n    add_student(cursor, 'Alice', 20, 'Computer Science')  \n    add_student(cursor, 'Bob', 22, 'History')\n    add_student(cursor, 'Charlie', 19, 'Biology')\n    add_course(cursor, 'Math 101', 'Dr. Smith')\n    add_course(cursor, 'English 101', 'Ms. Jones')\n    add_course(cursor, 'Chemistry 101', 'Dr. Lee')\n\n    conn.commit() \n    # If you don't commit the transaction, it is rolled back at the end of the with statement, and the data is discarded.\n    print(\"Sample data inserted successfully.\")\n```\n\nThe code above uses a **parameterized statement**.  You have `?` markers in the original SQL.  You pass a tuple as a second parameter for the `cursor.execute()`, and the values from the tuple are plugged into the statement in place of the `?` markers.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c732d"
        },
        {
          "subsectionOrder": 8,
          "title": "Writing SQL Queries",
          "content": "### **Overview**\n\nSQL queries allow you to interact with and analyze data. You will learn how to use SQL commands to retrieve and manipulate data.\n\n**Queries:**\n\nThe SQL statements in this lesson, unless included in Python code, are just examples -- you do not need to add these to your code.\n\n- Retrieve all student information:\n\n```sql\nSELECT * FROM Students;\n```\n\n- Find courses taught by a specific instructor:\n\n```sql\nSELECT * FROM Courses WHERE instructor_name = 'Dr. Smith';\n```\n\n- Find only the student_ids and names from the Student table:\n\n```sql\nSELECT student_id, name FROM Students;\n```\n\n- List students ordered by age:\n\n```sql\nSELECT * FROM Students ORDER BY age;\n```\n\n- Find the students of a given age majoring in English:\n\n```sql\nSELECT * FROM Students WHERE age = 22 AND major = 'History';\n```\n\nWithin Python, SQL SELECT statements are executed like the INSERT statements, but you also need to retrieve the results.  Add the following to your school_b.py program:\n\n```python\n    cursor.execute(\"SELECT * FROM Students WHERE name = 'Alice'\")\n    result = cursor.fetchall()\n    for row in result:\n        print(row)\n```\nWhen the SELECT statement is executed, it makes a collection of rows available.  The fetchall() creates an iterable connection of the matching rows, and each row is a tuple of the values from the requested columns.  In this case, you are using '*' which means all columns.  In this case, the first row returns the record for \"Alice\", and the first element in the tuple is the student_id.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c732e"
        },
        {
          "subsectionOrder": 9,
          "title": "Adding Entries with Foreign Keys",
          "content": "Now you are ready to add enrollments, because you can resolve the primary keys that are used as foreign keys in the Enrollments table.  The steps are:\n\n1. Find the student_id for the student.\n\n2. Find the course_id for the course.\n\n3. Insert the enrollment record.\n\nYou are going to do this for various students and courses, so again you create a function.  Add this code to school_b.py:\n\n```python\ndef enroll_student(cursor, student, course):\n    cursor.execute(\"SELECT * FROM Students WHERE name = ?\", (student,)) # For a tuple with one element, you need to include the comma\n    results = cursor.fetchall()\n    if len(results) > 0:\n        student_id = results[0][0]\n    else:\n        print(f\"There was no student named {student}.\")\n        return\n    cursor.execute(\"SELECT * FROM Courses WHERE course_name = ?\", (course,))\n    results = cursor.fetchall()\n    if len(results) > 0:\n        course_id = results[0][0]\n    else:\n        print(f\"There was no course named {course}.\")\n        return\n    cursor.execute(\"INSERT INTO Enrollments (student_id, course_id) VALUES (?, ?)\", (student_id, course_id))\n\n    ... # And at the bottom of your \"with\" block\n\n    enroll_student(cursor, \"Alice\", \"Math 101\")\n    enroll_student(cursor, \"Alice\", \"Chemistry 101\")\n    enroll_student(cursor, \"Bob\", \"Math 101\")\n    enroll_student(cursor, \"Bob\", \"English 101\")\n    enroll_student(cursor, \"Charlie\", \"English 101\")\n    conn.commit() # more writes, so we have to commit to make them final!\n```\n\nTry this out, and then view the database in VSCode to see if the entries are created.  You may have to close your view of `school.db` and open it again to see your changes.\n\nYou could add exception handling, but it is not clear which exceptions you might get at this point, so it's not easy to add appropriate exception handling.\n\n### Check for Understanding\n\n1. What would happen if you tried the following:\n    ```python\n    cursor.execute(\"INSERT INTO Enrollments (student_id, course_id) (95, 43)\")\n    ```\n\n2. Try running school_b.py again, and check what happens with Enrollments.  You get duplicate records!  Duplicate, that is, except for the enrollment_id.  How could you prevent this?\n\nAnswer:\n\n1. The foreign key constraint is ON!  You would get an exception.  There is no record in the Students table with a student_id of 95.  There is no record in the Courses table with a course_id of 43, so the foreign key constraint prevents this invalid record from being created.\n\n2. You can't add another UNIQUE constraint to fix this problem, because you need to reuse the course_id and student_id values in multiple records.  But, you can check to see if the record already exists before you do the insert, as follows:\n\n    ```python\n    cursor.execute(\"SELECT * FROM Enrollments WHERE student_id = ? AND course_id = ?\", (student_id, course_id))\n    results = cursor.fetchall()\n    if len(results) > 0:\n        print(f\"Student {student} is already enrolled in course {course}.\")\n        return\n    ```\n    Try this out!",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c732f"
        },
        {
          "subsectionOrder": 10,
          "title": "More Complicated Queries",
          "content": "In the WHERE clause, you can use various comparison operators such as `< > <= >= <>`.  The `<>` means not equals.  You can also do math, such as `quantity * price`.  And, you can use the LIKE operator to find strings with the `%` sign used as a wildcard.  For example, to find all the math courses, you could do this:\n\n```sql\nSELECT * FROM Courses WHERE course_name LIKE \"math%\";\n```\n### Joins\n\nA join is used within a SELECT statement to combine two or more tables.  The records returned from the SELECT include columns from both tables.  Joins use an ON clause to pair up the entries from each table.  The following SELECT gets the pairs of student name and course name, according to Enrollments, which is the join table.\n```sql\nSELECT Students.name, Courses.course_name FROM Students JOIN Enrollments ON Students.student_id = Enrollments.student_id JOIN Courses ON Enrollments.course_id = Courses.course_id\n```\nThe statement creates a combined row.  For each Student record, the Enrollments for that student are retrieved, and a combined record is created for any matches that are found.  The `ON` clause gives the matching logic.  Then, for each of these combined records, the corresponding Course record is retrieved, and added to the row.  The SELECT returns the name from the Students table and course_name from the Course table.  We can use 'AS' to save typing:\n\n```sql\nSELECT s.name, c.course_name FROM Students AS s JOIN Enrollments AS e ON s.student_id = e.student_id JOIN Courses AS c ON e.course_id = c.course_id;\n```\nAnd, one can even leave out the `AS`:\n```sql\nSELECT s.name, c.course_name FROM Students s JOIN Enrollments e ON s.student_id = e.student_id JOIN Courses c ON e.course_id = c.course_id;\n```\n\nSuppose you want to include customers that have done no orders.  You do a LEFT JOIN (because the customer table is on the left in the join statement.)\n```sql\nSELECT c.customer_name, o.order_id FROM customers c LEFT JOIN orders o on c.customer_id = o.customer_id;\n```\nIf there are customers without orders, this statement will include them in the list, but the order_id column will be empty.  Similarly, one can do a RIGHT JOIN to show the orders without customers ... but there won't be any.  Why? Because the foreign key constraint is on, so you can't have an order record that doesn't belong to a customer.  You can also do a FULL JOIN to get both customers without orders and orders without a corresponding customer.\n\nSuppose we want to list all the students with the corresponding courses for which they are enrolled.  There is, in this case, a many-to-many association between students and courses -- but there is no foreign key in either table that points to the other table.  We need to use the Enrollments table as a join table, combining information from all three tables.  This is a compound join.  You may join three or more tables in this way.\n\n```sql\nSELECT Students.name, Courses.course_name \nFROM Enrollments\nJOIN Students ON Enrollments.student_id = Students.student_id\nJOIN Courses ON Enrollments.course_id = Courses.course_id;\n```\n\n### **Check for Understanding**\n\n1. We see that the JOIN statement for Students and Enrollments has an ON clause with Students.student_id = Enrollments.student_id.  Why doesn't it use Enrollments.enrollment_id?\n\nAnswer:\n\n2. The enrollment_id doesn't correspond to anything in the Students table.  You have to use the foreign key, student_id, from the Enrollments table.  An ON cause typically combines a primary key (Students.student_id) with a foreign key (Enrollments.student_id).  By the way, the primary key doesn't have to have the same name as the foreign key.  If the primary key in the Students table were named `id`, you'd do `ON Students.id = Enrollments.student_id`.\n\n### **Example Code:**\n\nOf course, any of the SQL above can be executed from Python.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7330"
        },
        {
          "subsectionOrder": 11,
          "title": "The UPDATE Statement",
          "content": "The UPDATE SQL statement changes one or more existing rows.  You specify the rows you want to change with a WHERE clause, like you would use in a SELECT statement.\n\n```sql\nUPDATE Students SET name=\"Charles\", age=20 WHERE name=\"Charlie\";\n```\nNote that the UPDATE statement updates all rows that match the WHERE clause.  You can also apply math functions to the update, as follows:\n\n```sql\nUPDATE products SET price=price * 1.1;\n```\nThis statement raises all the prices by 10%.  As there is no WHERE statement, every record is changed.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7331"
        },
        {
          "subsectionOrder": 12,
          "title": "The DELETE Statement",
          "content": "The DELETE Statement deletes one or more existing rows.  For example, the following statement deletes all product records if the price is less than 1.00:\n\n```sql\nDELETE FROM products WHERE price<1.0;\n```\nIf you leave off the WHERE clause, it deletes every record in the table!\n\n### **A Reference**\n\nThis is a very brief and incomplete summary of the SQL language.  The SQLBolt tutorial mentioned below gives more complete instruction, and a very good reference is available here: <https://www.w3schools.com/sql/default.asp>.  Some advanced topics, such as aggregation and subqueries, are discussed in the next lesson.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7332"
        },
        {
          "subsectionOrder": 13,
          "title": "SQL Query Practice",
          "content": "Your python_homework folder contains a program called `load_db.py`.  Take a look at its contents.  It creates a series of tables, for employees, customers, orders, products, and order_details.  Each order is associated with a customer and an employee.  For each order, there are line_items associated with the order, one line item for each product comprising the order.  You'll see the schema created at the top of the file.  Then, the file uses pandas to load data for each table from csv files into the database.  The resulting database is stored as `./db/lesson.db`.\n\nChange the directory to the python_homework folder and run the `load_db.py` program to create and populate the database.  You can run it again as needed to restore the database to its initial state.  Next, run the `sqlcommand.py` program.  This prompts you with a command line you can use to enter SQL statements.  (Each statement must end with a semicolon.)  Experiment with various SELECT, INSERT, UPDATE, and DELETE statements.  Have a look at the code in `sqlcommand.py` to see how it works.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7333"
        },
        {
          "subsectionOrder": 14,
          "title": "SQL from Pandas",
          "content": "How does a data analyst use SQL?\n\nYou have learned how to load a Pandas DataFrame from a CSV file.  A limitation of CSV files is that they are static.  Each reflects data as it was some time in the past, when the file was written.  Suppose you want to use Pandas to analyse the baseball standings.  These change from day to day -- but instead of a CSV file, one can have a relational database with this information that is updated continuously.  You'd want to load that information into Pandas.  Fortunately, this is very easy. \n\n```python\nimport pandas as pd\nimport sqlite3\n\nwith sqlite3.connect(\"../db/lesson.db\") as conn:\n    sql_statement = \"\"\"SELECT c.customer_name, o.order_id, p.product_name FROM customers c JOIN orders o ON c.customer_id = o.customer_id \n    JOIN line_items li ON o.order_id = li.order_id JOIN products p ON li.product_id = p.product_id;\"\"\"\n    df = pd.read_sql_query(sql_statement, conn)\n    print(df)\n```\nThis loads a dataframe from the results of a SELECT statement.  You then have access to all the statistical power of pandas.  In this case, we get a dataframe that lists all the customer names, all the orders, and the names of all the product that were ordered.  Note that there is a many-to-many association between orders and products, in that an order may have many products, but there may be many orders for a given product. The line_items table acts as a join table for orders and products.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7334"
        },
        {
          "subsectionOrder": 15,
          "title": "Optional: More Practice",
          "content": "An excellent tutorial on SQL is available at the following link: <https://sqlbolt.com/>.  This tutorial is optional, but it is **strongly recommended, including the More Topics section**.\n\n### **Summary**\nIn this lesson, you learned:\n\n1. How to create and connect to an SQLite database.\n2. How to define tables using SQL queries.\n3. How to populate tables with sample data.\n4. How to write SQL queries to retrieve and analyze data.\n5. How to commit changes and close the database connection.\n6. How to modify and delete data.\n7. How to access data from Pandas.\n8. By mastering these techniques, you can efficiently interact with databases using Python. For further exploration, refer to the SQLite Documentation and Python‚Äôs sqlite3 library documentation.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7335"
        }
      ]
    },
    {
      "id": "68f812aec22606ecfa5c733c",
      "lessonNumber": 9,
      "title": "Lesson 09 ‚Äî Advanced SQL and Database Integration",
      "status": "pending",
      "assignment": {
        "title": "**Advanced SQL and Database Integration**",
        "objective": "---\n\n## **Lesson Overview**\n\n**Learning Objective**:  \nStudents will deepen their understanding of SQL by learning advanced techniques such as subqueries, complex `JOIN`s, aggregation with functions, and using the `HAVING` clause for conditional filtering.\n\n---\n\n## **Assignment Instructions**\n\nYou create the code for this assignment in your python_homework/assignment9 folder.  You may want to have two VSCode terminal sessions.  In one, you have changed directories to `assignment9`.  This is the session where you will run your code.  In the other terminal session, you will run `sqlcommand.py` from the `python_homework` folder.  You need to have the working directory set differently, so that each program will be able to find `db/lesson.db`.  Be sure to create an `assignment9` git branch before you start.  As usual, mark the code that completes each task with a comment line.\n\n### **Preparation and Practice**\n\nYou have already experimented with the `sqlcommand.py` program.  Run it again, and practice what you've learned.  You can do SELECT, INSERT, UPDATE and DELETE.  The SELECT statements can have JOIN, GROUP BY, ORDER BY, subqueries, HAVING, etc.  Practice these until you feel familiar with them.\n\nFor each of the following tasks, you first use the sqlcommand command line to get the right SQL statement.  Then you add it to your program.\n\n**Help Available!**  \n\nThis lesson combines a lot of concepts that have been presented only briefly. You may find these tasks a little challenging.  If you get stuck, 1:1 mentors are available to answer your questions.  Appointments are available in the [1:1 Mentor Table](https://airtable.com/appoSRJMlXH9KvE6w/shrQinGb1phZYwdiL)\n\n---",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [
          {
            "taskNumber": 1,
            "title": "Complex JOINs with Aggregation",
            "description": "1. **Problem Statement**:  \n   Find the total price of each of the first 5 orders.  There are several steps.  You need to join the orders table with the line_items table and the products table.  You need to GROUP_BY the order_id.  You need to select the order_id and the SUM of the product price times the line_item quantity.  Then, you ORDER BY order_id and LIMIT 5.  You don't need a subquery. Print out the order_id and the total price for each of the rows returned.\n\n2. **Deliverable**: \n   - Within the python_homework folder, create an `assignment9` branch.  Change to the `assignment9` folder.\n   - Get the SQL statement working in sqlcommand.\n   - Within the `assignment9` folder, create `advanced_sql.py`. This should open the database, issue the SQL statement, print out the result, and close the database.\n   - test your program.\n\n---",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c734a"
          },
          {
            "taskNumber": 2,
            "title": "Understanding Subqueries",
            "description": "1. **Problem Statement**:  \n   For each customer, find the average price of their orders.  This can be done with a subquery. You compute the price of each order as in part 1, but you return the customer_id and the total_price.  That's the subquery. You need to return the total price using `AS total_price`, and you need to return the customer_id with `AS customer_id_b`, for reasons that will be clear in a moment.  In your main statement, you left join the customer table with the results of the subquery, using `ON customer_id = customer_id_b`.  You aliased the customer_id column in the subquery so that the column names wouldn't collide.  Then group by customer_id -- this `GROUP BY` comes *after* the subquery -- and get the average of the total price of the customer orders.  Return the customer name and the average_total_price.\n\n2. **Deliverable**:  \n   - Again, get the SQL statement working in sqlcommand.\n   - Add code to `advanced_sql.py` to print out the result.\n\n---",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c734b"
          },
          {
            "taskNumber": 3,
            "title": "An Insert Transaction Based on Data",
            "description": "1. **Problem Statement**:  \n   You want to create a new order for the customer named Perez and Sons.  The employee creating the order is Miranda Harris.  The customer wants 10 of each of the 5 least expensive products.  You first need to do a SELECT statement to retrieve the customer_id, another to retrieve the product_ids of the 5 least expensive products, and another to retrieve the employee_id.  Then, you create the order record and the 5 line_item records comprising the order.  You have to use the customer_id, employee_id, and product_id values you obtained from the SELECT statements. You have to use the order_id for the order record you created in the line_items records. The inserts must occur within the scope of one transaction. Then, using a SELECT with a JOIN, print out the list of line_item_ids for the order along with the quantity and product name for each.\n\n   You want to make sure that the foreign keys in the INSERT statements are valid.  So, add this line to your script, right after the database connection:\n   ```\n   conn.execute(\"PRAGMA foreign_keys = 1\")\n   ```\n\n   In general, when creating a record, you don't want to specify the primary key.  So leave that column name off your insert statements.  SQLite will assign a unique primary key for you.  But, you need the order_id for the order record you insert to be able to insert line_item records for that order.  You can have this value returned by adding the following clause to the INSERT statement for the order:\n   ```\n   RETURNING order_id\n   ```\n\n2. **Deliverable**:   \n   - Get this working in sqlcommand.  (Note that sqlcommand does not provide a way to begin and end transactions, so for sqlcommand, the creation of the order and line_item records are separate transactions.)\n   - Use sqlcommand to delete the line_items records for the order you created.  (This is one delete statement.)  Delete also the order record you created.\n   - Add statements for the complete transaction and the subsequent SELECT statement into `advanced_py.sql`, and to print out the result of the SELECT.\n   - Test your program.\n\n---",
            "codeExample": "```\n   conn.execute(\"PRAGMA foreign_keys = 1\")\n   ```\n\n```\n   RETURNING order_id\n   ```",
            "_id": "68f812aec22606ecfa5c734c"
          },
          {
            "taskNumber": 4,
            "title": "Aggregation with HAVING",
            "description": "1. **Problem Statement**:  \n   Find all employees associated with more than 5 orders.  You want the first_name, the last_name, and the count of orders.  You need to do a `JOIN` on the employees and orders tables, and then use GROUP BY, COUNT, and HAVING.\n\n2. **Deliverable**:  \n   - Get it working in sqlcommand.\n   - Add code `advanced_sql.py` to print out the employee_id, first_name, last_name, and an order count for each of the employees with more than 5 orders.\n   - Test your program.\n\n### Submit Your Assignment on GitHub**  \n\nüìå **Follow these steps to submit your work:**  \n\n#### **1Ô∏è‚É£ Add, Commit, and Push Your Changes**  \n- Within your python_homework folder, do a git add and a git commit for the files you have created, so that they are added to the `assignment9` branch.\n- Push that branch to GitHub. \n\n#### **2Ô∏è‚É£ Create a Pull Request**  \n- Log on to your GitHub account.\n- Open your `python_homework` repository.\n- Select your `assignment9` branch.  It should be one or several commits ahead of your main branch.\n- Create a pull request.\n\n#### **3Ô∏è‚É£ Submit Your GitHub Link**  \n- Your browser now has the link to your pull request.  Copy that link. \n- Paste the URL into the **assignment submission form**. \n\n---\n\n### **Resources**\n- [SQLite Documentation](https://www.sqlite.org/docs.html)\n- [Python `sqlite3` Library Documentation](https://docs.python.org/3/library/sqlite3.html)\n```",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c734d"
          }
        ],
        "submissionInstructions": "Please submit on time",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Introduction",
          "content": "# **Lesson 09 ‚Äî Advanced SQL and Database Integration**",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c733d"
        },
        {
          "subsectionOrder": 2,
          "title": "Lesson Overview",
          "content": "**Learning objective:** Students will deepen their understanding of SQL by learning advanced techniques such as subqueries, complex `JOIN`s, aggregation with functions, and using `HAVING` for conditional filtering. This lesson also introduces performance optimization techniques, transactions, parameterized queries, window functions, and more.\n\n**Topics:**\n1. Subqueries: Embedding queries within other SQL statements for dynamic filtering or calculations.\n2. Complex JOINs: Using INNER and LEFT JOINs across multiple related tables.\n3. Aggregation Functions: Using `MIN()`, `MAX()`, `AVG()`, `COUNT()` with `GROUP BY`.\n4. Aggregation with HAVING: Filtering grouped results using `HAVING` after aggregation.\n5. Performance Optimization: Creating indexes to speed up frequent queries.\n6. Transactions and Rollbacks: Ensuring consistency with `BEGIN`, `COMMIT`, and `ROLLBACK`.\n7. Parameterized Queries: Preventing SQL injection using safe input handling with placeholders.\n8. Window Functions: Applying `RANK()`, `ROW_NUMBER()`, and `OVER()` for row-level analytics.\n9. Date and Time Functions: Calculating durations and extracting date components using `JULIANDAY()` and related functions.\n10. Python Integration: Writing and executing SQL queries within Python scripts using `sqlite3`.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c733e"
        },
        {
          "subsectionOrder": 3,
          "title": "Understanding Subqueries",
          "content": "### **Overview**\nSubqueries are nested SQL queries used to perform intermediate calculations or selections before the main query executes.\n\n### **Key Concepts:**\n- Use subqueries to fetch results dynamically within another query.\n- Common use cases include finding maximum, minimum, or aggregated values.\n\n### **Example:**\nFind the highest-paid employee in each department using a subquery.\n```sql\nSELECT department_id, employee_id, salary\nFROM Employees AS e\nWHERE salary = (\n    SELECT MAX(salary)\n    FROM Employees\n    WHERE department_id = e.department_id\n);\n```\n\n### **Implementation in Python:**\n```python\n# Example using SQLite\nconn = sqlite3.connect(\"company.db\")\ncursor = conn.cursor()\n\n# Execute the query\nquery = \"\"\"\nSELECT department_id, employee_id, salary\nFROM Employees AS e\nWHERE salary = (\n    SELECT MAX(salary)\n    FROM Employees\n    WHERE department_id = e.department_id\n);\n\"\"\"\ncursor.execute(query)\nprint(cursor.fetchall())\n\nconn.close()\n```\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c733f"
        },
        {
          "subsectionOrder": 4,
          "title": "Complex JOINs",
          "content": "### **Overview**\nComplex `JOIN`s allow you to retrieve data from multiple related tables.\n\n### **Steps:**\n1. Create a `Projects` table and insert sample data.\n2. Use a `JOIN` to combine employee and project information through the department field.\n\n### **Key Concepts:**\n- `INNER JOIN`: Retrieves records with matching values in both tables.\n- `LEFT JOIN`: Retrieves all records from the left table and matching records from the right.\n\n### **SQL Example: Create and Populate a Projects Table**\n```sql\nCREATE TABLE Projects (\n    id INTEGER PRIMARY KEY,\n    name TEXT NOT NULL,\n    department TEXT NOT NULL\n);\n\nINSERT INTO Projects (name, department) VALUES\n('Project A', 'HR'),\n('Project B', 'IT'),\n('Project C', 'Finance');\n```\n\n### **SQL Example: Complex JOIN**\nList employees working in departments responsible for a specific project:\n```sql\nSELECT Employees.name, Projects.name AS project_name\nFROM Employees\nJOIN Projects ON Employees.department = Projects.department\nWHERE Projects.name = 'Project A';\n```\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7340"
        },
        {
          "subsectionOrder": 5,
          "title": "Aggregation",
          "content": "### **Overview**\nAggregation functions like `MIN()`, `MAX()`, `COUNT()`, and `AVG()` allow you to summarize data across groups.\n\n### **Task:**\nCalculate the minimum and maximum salaries and the number of employees in each department.\n\n### **SQL Example:**\n```sql\nSELECT department_id, \n       MIN(salary) AS min_salary, \n       MAX(salary) AS max_salary, \n       COUNT(employee_id) AS num_employees\nFROM Employees\nGROUP BY department_id;\n```\n\n### **Key Notes:**\n- Use `GROUP BY` to organize results by department.\n- Ensure fields in the `SELECT` clause are either aggregated or part of the `GROUP BY`.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7341"
        },
        {
          "subsectionOrder": 6,
          "title": "Aggregation with HAVING",
          "content": "### **Overview**\nThe `HAVING` clause filters aggregated results after the `GROUP BY` operation.\n\n### **Task:**\nList all departments where the average salary exceeds 70,000 and display the department manager.\n\n### **SQL Example:**\n```sql\nSELECT d.department_name, \n       d.manager_id, \n       AVG(e.salary) AS avg_salary\nFROM Departments AS d\nJOIN Employees AS e ON d.department_id = e.department_id\nGROUP BY d.department_id\nHAVING AVG(e.salary) > 70000;\n```\n\n### **Key Notes:**\n- Use `HAVING` instead of `WHERE` to filter aggregated results.\n- Combine `JOIN`s and `GROUP BY` for advanced aggregations.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7342"
        },
        {
          "subsectionOrder": 7,
          "title": "Performance Optimization: Indexing",
          "content": "### **Overview**\nSQL queries can sometimes be slow if they involve large tables. Indexes can be created on columns that are frequently used in `WHERE`, `JOIN`, or `ORDER BY` clauses to speed up query performance.\n\n### **SQL Example:**\n```sql\nCREATE INDEX idx_department ON Employees(department_id);\n```\n\nThis creates an index on the `department_id` column to speed up queries that filter by department.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7343"
        },
        {
          "subsectionOrder": 8,
          "title": "Transactions and Rollbacks",
          "content": "### **Overview**\nTransactions ensure that multiple database operations are completed successfully before committing them. If an error occurs, you can roll back the changes to keep the database in a consistent state.\n\n### **Example:**\n```python\n# Start a transaction\nconn = sqlite3.connect(\"company.db\")\ncursor = conn.cursor()\n\ntry:\n    cursor.execute(\"INSERT INTO Employees (name, department_id) VALUES ('John Doe', 2)\")\n    cursor.execute(\"INSERT INTO Employees (name, department_id) VALUES ('Jane Smith', 3)\")\n    conn.commit()  # Commit transaction\nexcept Exception as e:\n    conn.rollback()  # Rollback transaction if there's an error\n    print(\"Error:\", e)\n\nconn.close()\n```\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7344"
        },
        {
          "subsectionOrder": 9,
          "title": "Parameterized Queries to Prevent SQL Injection",
          "content": "### **Overview**\nSQL injection can be prevented by using parameterized queries, ensuring that user input is treated safely.\n\n### **Example:**\n```python\ncursor.execute(\"SELECT * FROM Employees WHERE department_id = ?;\", (department_id,))\n```\n\nThis ensures that `department_id` is treated as a parameter and not part of the SQL statement itself.  If any part of the SQL statement comes from the end user or other untrusted source, always put that part in a parameter so that it can be sanitized to strip out rogue SQL.  Don't do it like this:\n\n```python\ncursor.execute(f\"SELECT * FROM Employees WHERE department_id = {department_id};\")\n# or, equally bad:\ncursor.execute(\"SELECT * FROM Employees WHERE department_id = \" + department_id + \";\")\n```\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7345"
        },
        {
          "subsectionOrder": 10,
          "title": "Window Functions",
          "content": "### **Overview**\nSQL window functions allow for advanced analysis over a specified range of rows. For example, calculating the rank of employees within a department based on salary.\n\n### **SQL Example:**\n```sql\nSELECT name, salary, department_id,\n       RANK() OVER (PARTITION BY department_id ORDER BY salary DESC) AS rank\nFROM Employees;\n```\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7346"
        },
        {
          "subsectionOrder": 11,
          "title": "Date and Time Functions",
          "content": "### **Overview**\nSQL provides functions for manipulating and querying date and time data, which are useful when working with time-based analysis.\n\n### **SQL Example:**\n```sql\nSELECT name, date_of_birth, \n       JULIANDAY('now') - JULIANDAY(date_of_birth) AS age_in_days\nFROM Employees;\n```\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7347"
        },
        {
          "subsectionOrder": 12,
          "title": "Implementing All Techniques in Python",
          "content": "### **Implementation Tips**\n\n1. **Test Queries Separately:**\n   - Write and test each query independently in your script or database interface before integrating into Python.\n\n2. **Iterative Debugging:**\n   - Verify intermediate outputs (e.g., after `JOIN` or subquery execution).\n\n3. **Error Handling in Python:**\n   - Use `try-except` blocks to handle database connection errors.\n\n### **Example Python Script:**\n```python\nimport sqlite3\n\n# Connect to the database\nconn = sqlite3.connect(\"company.db\")\ncursor = conn.cursor()\n\n# Aggregation with HAVING\nquery = \"\"\"\nSELECT d.department_name, \n       d.manager_id, \n       AVG(e.salary) AS avg_salary\nFROM Departments AS d\nJOIN Employees AS e ON d.department_id = e.department_id\nGROUP BY d.department_id\nHAVING AVG(e.salary) > 70000;\n\"\"\"\n\n# Execute and fetch results\ncursor.execute(query)\nresults = cursor.fetchall()\nprint(results)\n\nconn.close()\n```\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7348"
        },
        {
          "subsectionOrder": 13,
          "title": "Summary",
          "content": "In this lesson, you‚Äôve learned:\n1. How to use subqueries to dynamically fetch values for other queries.\n2. How to use complex `JOIN`s to integrate data from multiple tables.\n3. How to use aggregation functions to summarize data across groups.\n4. How to apply `HAVING` for conditional filtering on aggregated data.\n5. How to optimize performance with indexes.\n6. How to use transactions and rollbacks to ensure data consistency.\n7. How to prevent SQL injection with parameterized queries.\n8. How to use window functions for advanced analytics.\n9. How to handle date and time data for time-based analysis.\n\nFor further exploration, refer to the [SQLite Documentation](https://www.sqlite.org/docs.html) and Python's `sqlite3` library documentation.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7349"
        }
      ]
    },
    {
      "id": "68f812aec22606ecfa5c734f",
      "lessonNumber": 10,
      "title": "Lesson 10 ‚Äî Introduction to Web Scraping",
      "status": "pending",
      "assignment": {
        "title": "**Introduction to Web Scraping**",
        "objective": "### **Objective**\n\nYou learn to harvest data from a live web site, scraping values from the HTML page and storing them in a CSV file.\n\n## **Assignment Instructions**\n\nYou create the code for this assignment in the assignment10 folder within your python_homework folder.  Be sure to create an `assignment10` git branch before you start.  As usual, mark the code that completes each task with a comment line.\n\n---\n\n## **Overview**\nThis lesson introduces web scraping concepts and practices, including:\n1. Understanding HTML and the DOM structure in a live web page.\n2. Extracting content using Selenium.\n3. Storing data in CSV and JSON files.\n4. Ethical considerations in web scraping.\n\n---\n\n## **Tasks**",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [
          {
            "taskNumber": 1,
            "title": "Review robots.txt to Ensure Policy Compliance",
            "description": "In this assignment, you collect data from the Durham County Library site.  As always, when you do web scraping, you need to review the policy described in robots.txt, to ensure that you comply.\n\n1. Open [https://durhamcountylibrary.org/robots.txt]\n\n2. Verify that the following steps are not in breach of policy.",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c735a"
          },
          {
            "taskNumber": 2,
            "title": "Understanding HTML and the DOM for the Durham Library Site",
            "description": "You are going to find where in the HTML page the data you want is located.\n\n1. Open [https://durhamcounty.bibliocommons.com/v2/search?query=learning%20spanish&searchType=smart]\n\n2. Open your browser developer tools to show the HTML elements.  For Chrome, this is `shift-ctrl-J`.\n\n3. Find the HTML element for a single entry in the search results list.  This is a little tricky.  When you select an element in the Chrome developer tools, that part of the web page is highlighted.  But, that typically only gets you to the main `div`.  There is a little arrow next to that div in the developer tools element window, and you click on that to open it up to show the child elements.  Then, you select the element that corresponds to the search results area.  Continue in this way, opening up divs or other containers and selecting the one that highlights the area you want.  Eventually, you'll get to the first search result.  Hint: You are looking for an `li` element, because this is an element in an unordered list.  Note the values for the class attribute of this entry.  You'll want to save these in some temporary file, as your program will need them.\n\n3. Within that element, find the element that stores the title.  Note the tag type and the class value.  Your program will need this value too, so save it too.\n\n4. Within the search results `li` element, find the element that stores the author.  Hint: This is a link.  Note the class value and save it.  Some books do have multiple authors, so you'll have to handle that case.\n\n5. Within the search results `li` element, find the element that stores the format of the book and the year it was published.  Note the class value and save it.  Now in this case, the class might not be enough to find the part of the `li` element you want.  So look at the `div` element that contains the format and year.  You want to note that class value too.",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c735b"
          },
          {
            "taskNumber": 3,
            "title": "Write a Program to Extract this Data",
            "description": "1. `get_books.py`. The program should import from selenium and webdriver_manager, as shown in your lesson.  You also need pandas and json.\n\n2. Add code to load the web page given in task 2.\n\n3. Find all the `li` elements in that page for the search list results.  You use the class values you stored in task 2 step 3.  Also use the tag name when you do the find, to make sure you get the right elements.\n\n5. Within your program, create an empty list called `results`.  You are going to add `dict` values to this list, one for each search result.\n\n6. Main loop: You iterate through the list of `li` entries.  For each, you find the entry that contains title of the book, and get the text for that entry.  Then you find the entries that contain the authors of the book, and get the text for each.  If you find more than one author, you want to join the author names with a semicolon `;` between each.  Then you find the `div` that contains the format and the year, and then you find the `span` entry within it that contains this information.  You get that text too.  You now have three pieces of text.  Create a `dict` that stores these values, with the keys being `Title`, `Author`, and `Format-Year`.  Then append that `dict` to your `results` list.\n\n7. Create a DataFrame from this list of dicts.  Print the DataFrame.\n\n**Hint** You can put little print statements in at each step, to see if that part works.  For example, when you find all the `li` entries, you could print out the length of the list.  Then, you could just implement the part of the main loop that finds the Title, and just print out that title.  In this way, you can get the program done incrementally.\n\n**For Further Thought**  You are getting the search results from the first page of the search results list.  How would you get all the search results from all the pages?  How can you make the program do this regardless of how many pages you might have?  **Optional:** Change your program to page through the search results so as to get all of the results.  However! You need to make sure your program pauses between pages.  Fast screen scraping, where many requests are sent in short order, is an abuse of the privilege.",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c735c"
          },
          {
            "taskNumber": 4,
            "title": "Write out the Data",
            "description": "Modify your program to do the following:\n\n1. Write the DataFrame to a file called `get_books.csv`, within the assignment10 folder.  Examine the file to see if it looks right.\n\n2. Write the `results` list out to a file called `get_books.json`, also within the assignment10 folder.  You should write it out in JSON format.  Examine the file to see if it looks right.",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c735d"
          },
          {
            "taskNumber": 5,
            "title": "Ethical Web Scraping",
            "description": "**Goal**:  \nUnderstand the importance of ethical web scraping and `robots.txt` files.\n\n1. Access the `robots.txt` file for Wikipedia: [Wikipedia Robots.txt](https://en.wikipedia.org/robots.txt).\n2. Analyze the file and answer the following questions.  Put your answers in a file called `ethical_scraping.txt` in your python_homework/assignment10 directory\n   - Which sections of the website are restricted for crawling?\n   - Are there specific rules for certain user agents?\n3. Reflect on why websites use `robots.txt` and write 2‚Äì3 sentences explaining its purpose and how it promotes ethical scraping.  Put these in `ethical_scraping.txt` in your python_homework directory.\n\n\n---",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c735e"
          },
          {
            "taskNumber": 6,
            "title": "Scraping Structured Data",
            "description": "**Goal**:  \nExtract a web page section and store the information.\n\n1. Use your browser developer tools to view this page: [https://owasp.org/www-project-top-ten/].  You are going to extract the top 10 security risks reported there.  Figure out how you will find them.\n\n2. Within your python_homework/assignment10 directory, write a script called `owasp_top_10.py`.  Use selenium to read this page.\n\n3. Find each of the top 10 vulnerabilities.  Hint: You will need XPath.  For each of the top 10 vulnerabilites, keep the vulnerability title and the href link in a dict.  Accumulate these dict objects in a list.\n\n4. Print out the list to make sure you have the right data.  Then, add code to the program to write it to a file called `owasp_top_10.csv`.  Verify that this file appears correct.\n\n5. Create a file, `challenges.txt`, also within your lesson9 directory.  In this file, describe any challenges you faced in completing this assignment and how you resolved them.\n\n---\n\n\n### Submit Your Assignment on GitHub**  \n\nüìå **Follow these steps to submit your work:**  \n\n#### **1Ô∏è‚É£ Add, Commit, and Push Your Changes**  \n- Within your python_homework folder, do a git add and a git commit for the files you have created, so that they are added to the `assignment10` branch.\n- Push that branch to GitHub. \n\n#### **2Ô∏è‚É£ Create a Pull Request**  \n- Log on to your GitHub account.\n- Open your `python_homework` repository.\n- Select your `assignment10` branch.  It should be one or several commits ahead of your main branch.\n- Create a pull request.\n\n#### **3Ô∏è‚É£ Submit Your GitHub Link**  \n- Your browser now has the link to your pull request.  Copy that link. \n- Paste the URL into the **assignment submission form**. \n\n---\n\n## **Resources**\n\n- [Selenium Documentation](https://www.selenium.dev/documentation/webdriver/)\n- [MDN Web Docs: Understanding the DOM](https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model)\n- [OWASP robots.txt](https://owasp.org/robots.txt)\n\n---",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c735f"
          }
        ],
        "submissionInstructions": "Please submit on time",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Introduction",
          "content": "# **Lesson 10 ‚Äî Introduction to Web Scraping**",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7350"
        },
        {
          "subsectionOrder": 2,
          "title": "Lesson Overview",
          "content": "**Learning objective:** Students will gain a comprehensive understanding of web scraping, focusing on the fundamentals such as HTML structure, DOM representation, and using Python libraries like `Selenium` and `WebDriver Manager` to scrape and extract data from web pages. Additionally, students will explore the ethical aspects of web scraping, including adhering to guidelines provided by `robots.txt` and managing server requests responsibly.\n\n### **Topics:**\n1. Basics of HTML and DOM\n2. Web Scraping Tool: `Selenium` and `WebDriver Manager`\n3. Ethical Web Scraping: Understanding `robots.txt` and ethical considerations\n4. Scraping Structured Data: Extracting specific data points\n5. Managing Requests: Delays, retries, and handling errors\n6. Saving Scraped Data to Files\n7. Frailty in Scraping Programs\n\n**Why Do Web Scraping?**\n\nThere is a lot of information on the Internet, as you know, but it is in web pages that are provided so that humans can read them.  If you want to automate the process of searching or archiving some of this information, you need to extract it into a structured format and store it separately, so that you can then do SQL searches or the like on what you save.\n\nHere are some examples:\n\n- Finding the prices of airline flights to a given destination for multiple airlines.\n- Finding all scholarly articles on a particular topic you want to research and saving this information for reference.\n- Finding the schedule of public meetings for local government bodies.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7351"
        },
        {
          "subsectionOrder": 3,
          "title": "Basics of HTML and DOM",
          "content": "### **Overview**\nHTML (Hypertext Markup Language) is the backbone of web pages, providing structure and content. The Document Object Model (DOM) is a tree structure that represents the page content. Understanding the structure of web pages and the DOM is essential for locating and extracting data during web scraping.  When you pull up a web page in your browser, a request is sent to a web server and an HTML document is returned.  Your browser then parses that document and presents the page.  When you do web scraping, you'll also parse that page, not for presentation, but to capture information from that page so that it can be stored in a structured way.  You need to understand HTML and the DOM structure to do screen scraping.  Many of you may already be familiar with HTML -- if so, skip ahead.\n\n### Elements of an HTML Document\n\nEach element of the DOM has the following:\n- A tag, the name for the type of element.\n- Attributes.  These are name-value pairs, and vary with the element type.  For example, a `class` attribute might describe the display style for the element.  For a link element, the `href` attribute describes web address to use for the link.  There are many others.\n- Content.  This may be text, or it may be other elements.  As the DOM is a tree, some elements contain others.\n\n### **Key HTML Elements:**\n- **`<title>`**: The title of the page.\n- **`<p>`**: Paragraphs of text.\n- **`<h1>, <h2>, <h3>`**: Headers of varying levels.\n- **`<a>`**: Links with `href` attributes pointing to URLs.\n- **`<img>`**: Images with `src` attributes indicating the image source.\n\nThere are many others.\n\n### For Further Reference\n\nA good online refernce to HTML is available at [W3Schools](https://www.w3schools.com/html/).\n\n### **Hands-On Activity:**\n\n1. Open the Wikipedia page for \"Web Scraping\" ([Wikipedia - Web Scraping](https://en.wikipedia.org/wiki/Web_scraping)).\n2. Open the developer tools for your browser.  For Chrome this is `Shift-Ctrl-J`.  Inspect the elements of the page.  In the Chrome developer tools, they are in the `Elements` tab. \n3. Traverse this collection of elements to:\n   - Identify the title of the page.\n   - Locate the first paragraph and examine its structure.\n   - Explore headers (`<h1>, <h2>, <h3>`) and links (`<a>`).\n\nWhen you are creating a web scraping program, this is often the first step.  Look at the page you want to scrape in the browser developer tools and find where the information you want resides within the DOM, so that you can write code to extract it.\n\n### **How the DOM Is Structured:**\nThe DOM is a hierarchical tree structure. Here's an example:\n```html\n<html>\n  <head>\n    <title>Web Scraping</title>\n  </head>\n  <body>\n    <h1>Introduction</h1>\n    <p>This is an example paragraph.</p>\n    <a href=\"https://example.com\">Example Link</a>\n  </body>\n</html>\n```\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7352"
        },
        {
          "subsectionOrder": 4,
          "title": "Web Scraping Tools: Selenium and WebDriver Manager",
          "content": "### **Overview and Setup**\n\nTo scrape data from a web page, two primary libraries are commonly used in Python:\n- **`Selenium`**: A library that executes the same actions as a web browser, and makes the resulting page available for subsequent operations.\n- **`WebDriver Manager`**: Selenium requires a driver, which performs the actual web operations, just as a browser would.  This library makes the appropriate driver available to your program in an automated way.\n\nYou need to install these into your python_homework directory.  Within your VSCode terminal session for `python_homework` do:\n\n```bash\npip install selenium\npip install webdriver-manager\n```\n\n### **Steps to Web Scraping:**\n1. Initialize Selenium and the appropriate driver.  We'll use the Chromium driver, which is the same one as is in Chrome and other browsers.\n2. Retrieve a web page.\n3. Extract the desired elements using CSS selectors.\n\nThis is the initialization you'll need to do:\n\n```python\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service as ChromeService\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.common.by import By\n\ndriver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n\n```\n\nOnce you have a driver, you can fetch a web page, as follows:\n\n```python\ndriver.get(\"https://en.wikipedia.org/wiki/Web_scraping\")\n```\n\n### **Explanation: Why do we Need So Much Machinery?**\n\nThis may seem a pretty heavy collection of stuff just to scrape a web page.  However, most web pages these days are dynamic.  They contain JavaScript, and after the page loads, the scripts run to populate the page.  So, we need an engine that will not only get the page, but also run the JavaScript, and the JavaScript may itself make Ajax or Fetch calls to get more data.  A lot happens to create the appearance of the page you see.\n\nTry the code above in your Python interative shell.  You'll see something a little odd.  A window will flash up with the actual page we are trying to scrape.  This can be interesting or annoying, depending on which side of the bed you got up on.  You can configure your driver to run in 'headless' mode, without a window, when you create the driver:\n\n```python\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')  # Enable headless mode\noptions.add_argument('--disable-gpu')  # Optional, recommended for Windows\noptions.add_argument('--window-size=1920x1080')  # Optional, set window size\n\ndriver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),options=options)\n\n```\n\n### **Finding Information**\n\nA web page is comprised of HTML elements.  You can find elements within the page, and the most flexible way to find them is by CSS selector.  If you have done web page styling, you have used CSS selectors to specify which elements should have a particular style: font, color, background color, border, etc.  Selectors have various criteria.  Here are some examples:\n\n```\nSelect by element name:\n\n'p' : select all paragraphs\n'h2' : select all h2 headings\n\nSelect by element class:\n\n'.this-style-class' : All elements that have a class of this-style-class.  The . means select by class\n\nSelect by attribute:\n\n'[src]' : All elements that have a value for the src attribute, such as img elements.\n'[data-event=\"meeting\"]' : All elements that have a value for the data-event attribute equal to \"meeting\".\n\nThere are others.  There are also combined selectors:\n\n'div.main-container' : All div elements that have a class of main-container.\n```\n\nCSS selectors can be as complicated as need be, but typically you do not want to do complicated selection when web scraping, as this will make your scraping program frail.  If you want a full tutorial on CSS selectors, go here: [https://www.w3schools.com/css/css_selectors.asp]\n\nHere are some examples.  Try them out in your Python interative shell (assuming you have already loaded the web page into your driver as shown above):\n\n```python\nfrom selenium.webdriver.common.by import By\n\ntitle = driver.title # Find the title.  Parts of the header are accessed directly, not via find_element(), which only works on the body\nprint(title)\n\nbody = driver.find_element(By.CSS_SELECTOR,'body') # Find the first body element, typically only one\nif body:\n    links = body.find_elements(By.CSS_SELECTOR,'a') # Find all the links in the body.\n    if len(links) > 0:\n        print(\"href: \", links[0].get_attribute('href'))  # getting the value of an attribute\n\n\n\nmain_div = body.find_element(By.CSS_SELECTOR,'div[id=\"mw-content-text\"]')\nif main_div:\n    bolds = main_div.find_elements(By.CSS_SELECTOR,'b')\n    if len(bolds) > 0:\n        print(\"bolds: \",bolds[0].text)\n\n# Extract all images with their src attributes\nimages = [(img.get_attribute('src')) for img in body.find_elements(By.CSS_SELECTOR,'img[src]')] # all img elements with a src attribute\nprint(\"Image Sources:\", images)\n# hmm, this example uses a list comprehension.  We haven't talked about those.  This is the same as:\nimage_entries = driver.find_elements(By.CSS_SELECTOR,'img[src]')\nimages = []\nfor img in image_entries:\n    images.append(img.get_attribute('src'))\n\nprint(\"Image Sources:\", images)\n# You can see that list comprehensions are a useful shortcut in Python!\n```\n\nOnce you get an element, you can do the following (this is not an exhaustive list):\n\n- Get the text value, if any.\n- Get the value of any attributes.\n- Search the descendants.\n\nYou can then close the browser connection with:\n\n```python\ndriver.quit()\n```\n\nThe `driver.get()` method can raise an exception.  It's a good idea to do it in a try block:\n\n```python\ntry:\n    driver.get(\"https://nonsense.never.com\")\nexcept Exception as e:\n    print(\"couldn't get the web page\")\n    print(f\"Exception: {type(e).__name__} {e}\")\nfinally:\n    driver.quit()\n```\n\n**Note:** Selenium is not used only for web scraping.  It can automate interaction with a browser page. One common use is to do end-to-end testing.  You can type into the fields of forms, you can click on submit buttons, you can refresh the page, you can hit the back button, and so on.  We won't do any of this for this lesson.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7353"
        },
        {
          "subsectionOrder": 5,
          "title": "Ethical Web Scraping",
          "content": "### **Overview**\nWhile web scraping can be a powerful tool, it is important to follow ethical guidelines and respect website owners‚Äô wishes to avoid excessive server load and legal issues.\n\n### **Key Concepts:**\n- **What is `robots.txt`?**\n  - A file that specifies which sections of a website can or cannot be accessed by web crawlers.\n- **Why is `robots.txt` important?**\n  - It helps website owners control the traffic to their site and avoid server overload.\n  - Ethical scraping involves reviewing and adhering to the rules set in `robots.txt`.\n\n### **Activity: Explore `robots.txt` for Wikipedia**\n1. Access the `robots.txt` file for Wikipedia: [Wikipedia Robots.txt](https://en.wikipedia.org/robots.txt).\n2. Identify restricted sections of the site.\n3. Discuss why these sections are restricted.\n\n### **Example Code: Accessing `robots.txt`**\n```python\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service as ChromeService\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.common.by import By\n\ndriver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\nrobots_url = \"https://en.wikipedia.org/robots.txt\"\ndriver.get(robots_url)\nprint(driver.page_source)\ndriver.quit()\n\n```\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7354"
        },
        {
          "subsectionOrder": 6,
          "title": "XPath to Traverse the DOM",
          "content": "### **Overview**\nAfter scraping basic data, you can refine your scraping strategy to focus on specific sections of a webpage.  Sometimes, though, the find by CSS selector approach does not suffice.  Have a look at the [Wikipedia Web Scraping page.](https://en.wikipedia.org/wiki/Web_scraping) Somewhere near the bottom, you see the `See also` section.  Suppose you want to scrape each of the links in that section.  You first open up the developer tools in your browser to see how you find the section.  Open up the elements tab and do a ctrl-f to find \"See also\".  The second hit gets you pretty close.  This is an `h2` element.  If you scroll down a little bit, you see a `div` further down in the page, and that div has the list of links you want.  But (sigh) there are no good identifiers like class or id or attribute to find this div.  So you go back to that h2 with \"See also\".  That h2 has the id of \"See_also\".  This helps some.  What you want to do is go up to the parent div, the one that contains that h2, call it parent_div.  Then you get the div that is the next div sibling of parent_div.  That's the one you want.\n\nIt is clear that finding by CSS selector won't work in this case.  You have to use another technique, called XPath.  There is an XPath tutorial [here.](https://www.w3schools.com/xml/xpath_intro.asp) We won't need most of XPath, but we do need to know the XPath axes, specifically parent and sibling.  Here's the code:\n\n```python\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service as ChromeService\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.common.by import By\n\ndriver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\ndriver.get(\"https://en.wikipedia.org/wiki/Web_scraping\")  # this much you've seen before\n\nsee_also_h2 = driver.find_element(By.CSS_SELECTOR,'[id=\"See_also\"]') # our starting point\nlinks = []\nif (see_also_h2):\n    parent_div = see_also_h2.find_element(By.XPATH, '..') # up to the parent div\n    if parent_div:\n        see_also_div = parent_div.find_element(By.XPATH,'following-sibling::div' ) # over to the div with all the links\n        link_elements = see_also_div.find_elements(By.CSS_SELECTOR, 'a')\n        for link in link_elements:\n            print(f\"{link.text}: {link.get_attribute('href')}\")\n            name = link.text.strip()\n            url = link.get_attribute(\"href\")\n            if name and url:\n                links.append({\"name\": name, \"url\": url})\n\n\ndriver.quit()\n```\n\n### **Testing on Another Page:**\n\nTest the script on different Wikipedia pages to ensure consistency and robustness.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7355"
        },
        {
          "subsectionOrder": 7,
          "title": "Managing Requests and Handling Errors",
          "content": "### **Overview**\nWhen scraping large numbers of web pages, managing requests responsibly is essential. Techniques include delaying requests and handling connection errors.  \n\n- The `driver.get()` operation may raise an exception. Sometimes this is a timeout.  Or perhaps the network might be down.  It may not help to retry the request in this case.\n- The `driver.get()` operation may return an HTTP response code like 404 or 500.  Selenium does *not* raise an exception in this case.  Unfortunately, there is no easy way to access the HTTP response code in Selenium.  You can check if driver.title is the string you expect.\n- You can do a subsequent driver.get() to scrape another page, but if you do, you want to pause between pages.  It is an abuse to drive excessive load by many rapid requests.\n    ```python\n    from time import sleep\n    try:\n        driver.get('https://acme.com/index.html')\n        ... # extract data\n        sleep(2) # wait 2 seconds\n        driver.get('https://acme.com/another.html')\n    except Exception as e:\n        print(f\"An exception occurred: {type(e).__name__} {e}\")\n    finally:\n        driver.quit()\n\n    ```\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7356"
        },
        {
          "subsectionOrder": 8,
          "title": "Saving Scraped Data",
          "content": "### **Overview**\nOnce you've scraped valuable data, you may want to save it in a structured format, such as CSV or JSON.\n\n### **Saving Data to CSV:**\n```python\nimport csv\n\n# Save extracted data to a CSV file\nwith open('scraped_data.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Name\", \"Link\"])\n    for link in links:\n        writer.writerow([link[\"name\"], link[\"url\"]])\n```\n\n### **Saving Data to JSON:**\n```python\nimport json\n\n# Save data to a JSON file\ndata = {\"links\": links}\nwith open('scraped_data.json', 'w') as json_file:\n    json.dump(data, json_file, indent=4)\n```\n\n### **Saving Data to an SQL Database**\n\nSuppose you are scraping from multiple sites.  You may want to record different information from each site, and keep track of which site provided which information.  \n\nFor example, you may scrape airline sites for destinations, prices, and departure times.  You could create three tables, one for airlines, which would record the airline name and URL, and then three others for destinations, prices, and departure_times.  There would be a many-to-many relationship between airlines and destinations, which might have where_we_fly as a join table.  There would be a one-to-many relationship between where_we_fly entries and departure_times, and a one-to-many relationship between departure_times and prices (for economy, business, and first class). This is a little more complicated, to be sure -- but think about your data model when you get ready to store web scraping data!",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7357"
        },
        {
          "subsectionOrder": 9,
          "title": "Frailty in Web Scraping",
          "content": "Web scraping programs can be frail.  The authors of the websites that you are accessing to scrape data can change the format of the page.  This can break the scraping logic, as the DOM may no longer be organized as you expect, or because the entries have different classes or ids.  Sometimes you can make your scraping more robust by finding known strings within the formatted page, and traversing from that point, but sometimes you can only log the error so as to trigger work on a fix.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7358"
        },
        {
          "subsectionOrder": 10,
          "title": "Summary",
          "content": "In this lesson, you learned:\n1. How to understand the structure of web pages using HTML and the DOM.\n2. How to use Python libraries `Selenium` and `WebDriver Manager` for web scraping.\n3. Ethical considerations of web scraping, including respecting `robots.txt`.\n4. Techniques for extracting specific data, handling errors, and managing requests.\n5. How to save scraped data to CSV and JSON formats for future use.\n\n### **Important Notes:**\n- Always respect the website‚Äôs `robots.txt` file and terms of service.\n- Avoid overloading a server with excessive scraping requests.\n- Use appropriate delay and retry mechanisms to manage web traffic.\n\nFor further learning, explore the [Selenium Documentation](https://www.selenium.dev/documentation/webdriver/).",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7359"
        }
      ]
    },
    {
      "id": "68f812aec22606ecfa5c7361",
      "lessonNumber": 11,
      "title": "Lesson 11 ‚Äî Introduction to Data Visualization",
      "status": "pending",
      "assignment": {
        "title": "**Assignment 11: Introduction to Data Visualization**",
        "objective": "This assignment is to be implemented in a Kaggle notebook, using a Kaggle dataset.  As usual, mark the section of code that implements each task with a markdown cell.\n\n---",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [
          {
            "taskNumber": 1,
            "title": "Data Understanding",
            "description": "1. In Kaggle, go to the Datasets link on the left side of the page.\n2. Search for \"Diabetes Health Indicators Dataset\".  You should find one from Alex Teboul.\n3. In Data Explorer on the right, click on the one that starts with \"diabetes_012\".\n4. We need to understand particular columns.  Click on the pulldown for the 22 columns. Click on \"Select All\" and then \"Apply\".  Then select \"Column\" from the menu.  This explains the meaning of all the columns.  Scroll down throuh the information about each column.  You see that \"Diabetes_012\" can have 3 numeric values.  You see that \"Age\" is broken down into 13 buckets.  You see that \"GenHlth\" has values from 1 to 5, 5 being worst!  You see that PhysHlth is the number of days in the past month where the person's physical health was not good.  Again, a high number is bad!  We can't understand the plots we make **unless we know what the data means.**",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c7369"
          },
          {
            "taskNumber": 2,
            "title": "Creating the Notebook and Loading Data",
            "description": "1. Create a Kaggle Notebook called \"CTD_Assignment_11\".  Add the \"Diabetes Health Indicators Dataset\".  Run the first cell so that you get the filenames.\n2. Load the one that has \"diabetes_012\" in the name into a `diabetes` DataFrame.\n3. Print out the first 5 lines of the DataFrame.",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c736a"
          },
          {
            "taskNumber": 3,
            "title": "A Histogram For Age Distributions",
            "description": "1. Use Matplotlib to create a histogram of age distributions in the `diabetes` DataFrame, where the X axis is \"Age\" and the Y axis is \"Count\". Give the plot a meaningful title, and give the axes meaningful labels.\n2. Show the plot to see if it is as you expect.",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c736b"
          },
          {
            "taskNumber": 4,
            "title": "General Health over Time",
            "description": "1. Create a `health_by_age` DataFrame, using groupby() on the `diabetes` DataFrame.  Group by age.  Aggregate the `GenHlth` column using `mean`.\n2. Add a column called \"Health\" to the resulting DataFrame.  The value of this should be 5 minus the `GenHlth` column.  It is usually more meaningful to have high values mean good and low values mean bad.\n3. Sort `health_by_age` by the index.  (You use the sort_index() method.)  The index for this DataFrame is \"Age\", because of the groupby().\n4. Use Matplotlib to create a line plot where the X axis is \"Age\" (the index) and the Y axis is \"Health\".  Add a meaningful title and axes labels.\n5. Show the plot. It's kind of a sad story.",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c736c"
          },
          {
            "taskNumber": 5,
            "title": "A Heat Map of All Columns",
            "description": "1. Create a correlation matrix called `diabetes_corr` for all columns.\n2. Use Seaborn to create a heat map from the correlation matrix.  Note: It appears that Seaborn uses some Python deprecated features, so you will see warning messages.  You can ignore these.\n3. Show the heat map.\n\nAs you will notice, this heat map is hard to read.  You solve that problem in the next task by subsetting the correlation matrix.",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c736d"
          },
          {
            "taskNumber": 6,
            "title": "Subset Heat Maps",
            "description": "Suppose you decide that the data you care about most is in the \"Diabetes_012\", \"HeartDiseaseorAttack\", and \"GenHlth\" columns.\n\n1. Create a subset correlation matrix called `diabetes_corr_subset`, selecting these columns from `diabetes_corr`.\n2. Sort this in descending order on the `Diabetes_012` column.\n3. Create a heatmap for the first 10 rows and show it.  These are the factors most strongly correlated with diabetes.\n4. Create a heatmap for the last 10 rows, with an appropriate title, and show it.  These are the factors that are negatively correlated (or weakly correlated) with diabetes.\n5. Sort the matrix again on the \"GenHlth\" column in descending order, and again show heat maps for the first and last 10 rows.\n6. Notice the factors that are most consequential for diabetes and health, according to this dataset.  Add a markdown cell that describes these.",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c736e"
          },
          {
            "taskNumber": 7,
            "title": "A Pair Plot: Body Mass Index vs. Age",
            "description": "1. Using the `diabetes` DataFrame, create a pair plot for \"BMI\" and \"Age\", with a `hue` of \"Diabetes_012\".  I find that `palette=['#FF5733', '#33FF57', '#3357FF']` gives a helpful display.\n2. Give the pair plot a descriptive title and show it.\n3. The plot is hard to read.  There are too many values for BMI.  Use `qcut()` to divide BMI into 10 quantiles, and add the resulting Series to the `d_h_i` DataFrame.  Then plot Age vs. BMI Quantiles, with the same hue as before.\n4. Give this pair plot a descriptive title and show it.\n\n### **Submit the Notebook for Your Assignment**  \n\nüìå **Follow these steps to submit your work:**  \n\n#### **1Ô∏è‚É£ Get a Sharing Link for Your Assignment**  \n- On the upper right of the Kaggle page, click on Save Version and save, accepting all defaults.  You can just do a quick save.\n- On the upper right, click on Share.  Choose Public, make sure that Allow Comments is on, and copy the public URL to your clipboard.\n\n#### **2Ô∏è‚É£ Submit Your Kaggle Link**  \n- Paste the URL into the **assignment submission form**.  \n\n---",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c736f"
          }
        ],
        "submissionInstructions": "Please submit on time",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Introduction",
          "content": "# **Lesson 11 ‚Äî Introduction to Data Visualization**",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7362"
        },
        {
          "subsectionOrder": 2,
          "title": "Lesson Overview",
          "content": "**Learning objective:** In this lesson, students will learn to create and customize both basic and advanced data visualizations using Python libraries such as Matplotlib and Seaborn. By the end of this lesson, students will be able to effectively tell stories with data using visual representation, enhancing their ability to communicate insights.\n\n### **Topics:**\n1. Introduction to Matplotlib: Creating basic plots (line, bar, histogram).\n2. Customizing Plots: Labels, titles, legends, and colors.\n3. Introduction to Seaborn: Creating advanced visualizations (heatmaps, pair plots).\n4. Advanced Customization: Subplots, color palettes, and annotations.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7363"
        },
        {
          "subsectionOrder": 3,
          "title": "Introduction to Matplotlib",
          "content": "### **Setup**\n\nYou should run each of the code examples below from within your `python_homework` directory.  You need several additional libraries, so within `python_homework`, do the following:\n\n```bash\npip install matplotlib\npip install seaborn\n```\n\n### **Overview**\nMatplotlib is a foundational Python library for creating static, animated, and interactive visualizations. It is highly customizable and widely used for a variety of chart types, from basic plots to more complex visualizations.\n\n### **Key Plot Types:**\n1. **Line Plot:** Shows trends over time or continuous data.\n2. **Bar Plot:** Compares categorical data.\n3. **Histogram:** Displays the distribution of numerical data.\n\n### **When to Use:**\n- **Line Plot**: Best used when you want to display **trends over time** or track changes in continuous data, such as revenue, stock prices, or temperature variations.\n- **Bar Plot**: Ideal for comparing **categorical data**, such as sales by region, number of items sold by category, or performance scores across different teams.\n- **Histogram**: Used to display the **distribution** of a continuous variable and to detect outliers, skewness, or the normality of data.\n\n### **Example Code: Creating Basic Plots**\n\nThese examples should be run from the Python interactive shell.  Each `show()` operation causes a graphics window to open, and the processing in the shell (or in a Python program running within your terminal) stops until you close that window.  The graphics window has several controls.  One brings up some sliders, allowing you to change the appearance of the chart or plot.  Another button allows you to save the image.  Using these buttons, you can create an image file that might be added to a document or slide or web page.  Experiment with these.  However, don't save the images within the `python_homework` folder, or they'll be delivered to your GitHub repository, and that wouldn't be useful.  For the assignment, you'll display plots using a Kaggle notebook.  When you do that, you don't see these buttons or sliders, and the graphics appear in the notebook itself, not in a separate window.  \n\nHere are the examples you should try.  \n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Line Plot\nmonths = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\"]\nrevenue = [1000, 1200, 1500, 1700, 1600, 1800]\nplt.plot(months, revenue, marker='o', linestyle='-', color='blue')\nplt.title(\"Monthly Revenue\")\nplt.xlabel(\"Months\")\nplt.ylabel(\"Revenue ($)\")\nplt.show()\n\n# Bar Plot\ncategories = [\"Region A\", \"Region B\"]\nsales = [500, 700]\nplt.bar(categories, sales, color=['green', 'orange'])\nplt.title(\"Sales by Region\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Sales ($)\")\nplt.show()\n\n# Histogram\nrandom_data = np.random.randn(1000)\nplt.hist(random_data, bins=30, color='purple', alpha=0.7)\nplt.title(\"Random Data Distribution\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.show()\n```\n\n### **Outcome:** \n- The **Line Plot** illustrates trends over time.\n- The **Bar Plot** compares categorical data.\n- The **Histogram** shows the distribution of random data.\n\nThe third example uses an interesting function, `randn()`.  This gives a random collection of values, but the random values cluster around the average value.  This is called a normal or Gaussian distribution, and many things in nature, such as people's heights, lifetimes of lightbulbs, sizes of snowflakes, and measurement errors in typical experiments, are roughly Gaussian.\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7364"
        },
        {
          "subsectionOrder": 4,
          "title": "Customizing Plots",
          "content": "### **Overview**\nCustomizations make plots more informative and visually appealing. This includes:\n- Changing colors, line styles, or bar widths.\n- Adding grids to make value estimation easier.\n- Using legends, titles, and labels for better context.\n\n### **When to Customize:**\n- **Line Plot Customization:** Adjust the style (dashed, dotted, solid) and color of the line to match the importance or theme of the data. Adding markers can help highlight key points.\n- **Bar Plot Customization:** Customize the width, color, and edge of bars for visual distinction between categories.\n- **Histogram Customization:** Customize bin sizes, color, and alpha (transparency) to enhance the data‚Äôs visibility and readability.\n\nRun these examples in the Python shell.  \n### **Example Code: Customizing Plots**\n```python\nimport matplotlib.pyplot as plt\n\nmonths = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\"]\nrevenue = [1000, 1200, 1500, 1700, 1600, 1800]\n\n# Customized Line Plot\nplt.plot(months, revenue, marker='o', linestyle='--', color='red', linewidth=2)\nplt.title(\"Monthly Revenue\", fontsize=14, fontweight='bold')\nplt.xlabel(\"Months\", fontsize=12)\nplt.ylabel(\"Revenue ($)\", fontsize=12)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.legend([\"Revenue\"], loc=\"upper left\")\nplt.show()\n\n# Customized Bar Plot\nplt.bar(categories, sales, color=['skyblue', 'salmon'], width=0.5, edgecolor='black')\nplt.title(\"Sales by Region\", fontsize=14, fontweight='bold')\nplt.xlabel(\"Region\", fontsize=12)\nplt.ylabel(\"Sales ($)\", fontsize=12)\nplt.grid(axis='y', color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n```\n\n### **Key Customization Features:**\n- Line Plot customization includes adjusting line style, color, and adding a grid.\n- Bar Plot customization changes the bar color, width, and adds gridlines for better readability.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7365"
        },
        {
          "subsectionOrder": 5,
          "title": "Introduction to Seaborn",
          "content": "### **Overview**\nSeaborn is a powerful Python visualization library built on top of Matplotlib. It simplifies the creation of complex statistical plots and integrates well with Pandas DataFrames. Seaborn provides better default aesthetics for some plots, making them visually appealing with minimal effort.\n\n### **Key Plot Types:**\n1. **Heatmap:** Visualizes correlation or relationships in tabular data.\n    As was discussed in an earlier lesson, there might be a correlation between variables in a dataset.  Correlations are typically expressed as a number between -1.0 and 1.0.  A strong positive correlation is a positive number that is above, say, 0.3.  For people, there is a positive correlation between height and weight, for example.  A strong negative correlation is a negative number that is less than, say, -0.3.  For gas powered cars, there is a negative correlation between engine displacement and miles per gallon.  Even small correlations may be statistically meaningful if you have enough accurate measurements.  The heat map presents the correlations as a grid, where the positive correlations might be redder and the negative correlations might be bluer.\n\n2. **Pair Plot:** Displays pairwise relationships between multiple variables.\n    One form of the pair plot is the scatter plot.  Each dot on the plot represents one observation.  If there is a good correlation between the paired values, the points may line up.  You can also give a color for each dot, depending on the value of yet another column.  In the case of vehicles, you might have vehicle weight plotted vs. miles per gallon, and then you might have the dots colored by horsepower.  For a given horsepower, the mpg vs. vehicle weight might line up, but then you'd have high horsepower cars that are off of this line in one direction, indicated by a particular color.  One can customise the plot in various ways, such as by showing countour lines instead of individual dots.\n\n### **When to Use:**\n- **Heatmap**: Ideal for visualizing **correlations** or **relationships** in matrix-style data, such as the correlation between different features in a dataset.\n- **Pair Plot**: Great for **multivariate analysis**, where you want to explore relationships between multiple features in a dataset.\n\nFor the example below, you use the Titanic dataset.  This dataset is built into Seaborn, so you can do sns.load_dataset().  In more typical cases, you load CSV files using pandas.  The Titanic dataset has the following columns:\n\n- survived: True or False.\n- pclass: Passenger class, meaning what kind of ticket or accomodation the passenger had.\n- age\n- sibsp: How many siblings or spouses each passenger had on the ship.\n- parch: How many parents or children each passenger had on the ship.\n- fare: How much the passenger's ticket cost.\n- adult_male: True or False.\n- alone: True or False, whether the passenger was traveling alone.\n\n### **Example Code: Creating Advanced Visualizations**\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load Titanic dataset: This dataset is \ntitanic = sns.load_dataset(\"titanic\")\n\n# Heat map of correlations\nplt.figure(figsize=(10, 6))\ncorrelation_matrix = titanic.corr(numeric_only=True)\nsns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n# Pair Plot\nsns.pairplot(titanic, vars=[\"age\", \"fare\", \"adult_male\"], hue=\"survived\", palette=\"Set2\")\nplt.title(\"Pair Plot of Age, Fare, and Survival\")\nplt.show()\n```\n\n### **Outcome:**\n- The **Heat Map** visualizes the correlation matrix of the Titanic dataset.\n    Take a look at the heatmap, and assess which factors most important in affecting who survived.  Two factors made survival much less likely: Can you guess why?  One factor made survival more likely: Which one?\n\n- The **Pair Plot** provides pairwise relationships between 'age', 'fare', and 'adult_male', as colored by whether these passengers survived.\n    Some of these plots are not very useful.  Scatter plots where the variable on one of the axes only has two values are typically not very helpful, as you see.  The plots on the diagonal have a single variable plotted against itself, which doesn't seem too useful either, except that the default plot in this case is a \"kde\" plot, which shows the distribution of that single variable, as colored by who survived, and those plots do convey a story.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7366"
        },
        {
          "subsectionOrder": 6,
          "title": "Advanced Customization",
          "content": "### **Overview**\nAdvanced customization techniques include working with multiple subplots, adjusting figure sizes, and using color palettes to enhance the aesthetics of visualizations.\n\n### **Key Concepts:**\n- **Subplots:** Create multiple plots in one figure.\n- **Color Palettes:** Use predefined or custom color palettes for better aesthetics.\n- **Annotations:** Add text annotations to specific points on the plot.\n\n### **When to Customize Further:**\n- **Subplots**: Useful for comparing multiple visualizations in one figure, especially when you want to show different types of plots.\n- **Color Palettes**: Enhance visual appeal and help in distinguishing between categories.\n- **Annotations**: Add extra context or insights to specific parts of a plot.\n\n### **Example Code: Advanced Customization**\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\ncategories = [\"Region A\", \"Region B\"]\nsales = [500, 700]\nrandom_data = np.random.randn(1000)\n\n# Subplot Example\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Left plot: Bar Plot\naxes[0].bar(categories, sales, color=['skyblue', 'salmon'])\naxes[0].set_title(\"Sales by Region\")\naxes[0].set_xlabel(\"Region\")\naxes[0].set_ylabel(\"Sales ($)\")\n\n# Right plot: Histogram\naxes[1].hist(random_data, bins=30, color='purple')\naxes[1].set_title(\"Random Data Distribution\")\naxes[1].set_xlabel(\"Value\")\naxes[1].set_ylabel(\"Frequency\")\n\nplt.tight_layout()  # Adjust layout for better spacing\nplt.show()\n\n# Using a Custom Color Palette in Seaborn\nsns.set_palette(\"muted\")\nsns.barplot(x=categories, y=sales)\nplt.title(\"Sales by Region with Custom Palette\")\nplt.show()\n```\n\n### **Key Techniques:**\n- **Subplots** allow the creation of multiple plots in a single figure for better comparison.\n- Seaborn‚Äôs **custom color palette** gives more control over the visual appearance.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7367"
        },
        {
          "subsectionOrder": 7,
          "title": "Summary",
          "content": "In this lesson, you learned:\n1. How to create basic visualizations (line plots, bar plots, histograms) using Matplotlib.\n2. Techniques for customizing plots to improve readability and presentation.\n3. How to use Seaborn for advanced statistical visualizations like heatmaps and pair plots.\n4. Advanced techniques for creating subplots, using color palettes, and adding annotations for more insightful visualizations.\n\nFor more details, explore the [Matplotlib Documentation](https://matplotlib.org/stable/contents.html), [Seaborn Documentation](https://seaborn.pydata.org/), and the [Python Data Visualization](https://matplotlib.org/stable/gallery/index.html).\n\n---\n\n### Additional Resources:\n1. **Matplotlib Tutorials:** For more detailed Matplotlib tutorials, check out [Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/index.html).\n2. **Seaborn Gallery:** Explore different plot examples at the [Seaborn Gallery](https://seaborn.pydata.org/examples/index.html).\n3. **Data Visualization in Python:** To explore more about data visualization strategies and best practices, visit [Data Visualization in Python](https://realpython.com/python-data-visualization-using-matplotlib/).",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7368"
        }
      ]
    },
    {
      "id": "68f812aec22606ecfa5c7371",
      "lessonNumber": 12,
      "title": "Lesson 12 ‚Äî Advanced Data Visualization Techniques",
      "status": "pending",
      "assignment": {
        "title": "Assignment 12: Advanced Data Visualization",
        "objective": "This assignment is to be created in the `assignment12` folder of your `python-assignment12` directory which is a separate repository. Continue to work in the ``assignment12`` branch.\n\n---",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [
          {
            "taskNumber": 1,
            "title": "Plotting with Pandas",
            "description": "1. Create a file called employee_results.py.\n2. Load a DataFrame called employee_results using SQL.  Copy the `db/lesson.db` database from your `python_homework` folder to your `python-assignment12` folder.  Copy the `db` folder and the `lesson.db` file within it.  This can be done using the `cp -r` command.  In your `assignment12` folder, connect to `../db/lesson.db`. You use SQL to join the employees table with the orders table with the line_items table with the products table.  You then group by employee_id, and you SELECT the last_name and revenue, where revenue is the sum of price * quantity.  Ok, that's a lot of SQL to mess with, so here is the statement you need:\n   ```SQL\n   SELECT last_name, SUM(price * quantity) AS revenue FROM employees e JOIN orders o ON e.employee_id = o.employee_id JOIN line_items l ON o.order_id = l.order_id JOIN products p ON l.product_id = p.product_id GROUP BY e.employee_id;\n   ```\n3. Use the Pandas plotting functionality to create a bar chart where the x axis is the employee last name and the y axis is the revenue.\n4. Give appropriate titles, labels, and colors.\n5. Show the plot.\n\n---",
            "codeExample": "```SQL\n   SELECT last_name, SUM(price * quantity) AS revenue FROM employees e JOIN orders o ON e.employee_id = o.employee_id JOIN line_items l ON o.order_id = l.order_id JOIN products p ON l.product_id = p.product_id GROUP BY e.employee_id;\n   ```",
            "_id": "68f812aec22606ecfa5c7380"
          },
          {
            "taskNumber": 2,
            "title": "A Line Plot with Pandas",
            "description": "1. Create a file called cumulative.py.  The boss wants to see how money is rolling in.  You use SQL to access `../db/lesson.db` again.  You create a DataFrame with the order_id and the total_price for each order.  This requires joining several tables, GROUP BY, SUM, etc.\n2. Add a \"cumulative\" column to the DataFrame.  This is an interesting use of apply():\n   ```python\n   def cumulative(row):\n      totals_above = df['total_price'][0:row.name+1]\n      return totals_above.sum()\n\n   df['cumulative'] = df.apply(cumulative, axis=1)\n   ```\n   Because axis=1, apply() calls the cumulative function once per row.  Do you see why this gives cumulative revenue?  One can instead use cumsum() for the cumulative sum:\n   ```python\n   df['cumulative'] = df['total_price'].cumsum()\n   ```\n3. Use Pandas plotting to create a line plot of cumulative revenue vs. order_id.\n4. Show the Plot.\n\n---",
            "codeExample": "```python\n   def cumulative(row):\n      totals_above = df['total_price'][0:row.name+1]\n      return totals_above.sum()\n\n   df['cumulative'] = df.apply(cumulative, axis=1)\n   ```\n\n```python\n   df['cumulative'] = df['total_price'].cumsum()\n   ```",
            "_id": "68f812aec22606ecfa5c7381"
          },
          {
            "taskNumber": 3,
            "title": "Interactive Visualizations with Plotly",
            "description": "1. Load the Plotly wind dataset, via the following:\n   ```python\n   import plotly.express as px\n   import plotly.data as pldata\n   df = pldata.wind(return_type='pandas')\n   ```\n   Print the first and last 10 lines of the DataFrame.\n2. Clean the data.  You need to convert the 'strength' column to a float.  Use of str.replace() with regex is one way to do this, followed by type conversion.\n3. Create an interactive scatter plot of strength vs. frequency, with colors based on the direction.\n4. Save and load the HTML file, as `wind.html`.  Verify that the plot works correctly.\n\n---",
            "codeExample": "```python\n   import plotly.express as px\n   import plotly.data as pldata\n   df = pldata.wind(return_type='pandas')\n   ```",
            "_id": "68f812aec22606ecfa5c7382"
          },
          {
            "taskNumber": 4,
            "title": "A Dashboard with Dash",
            "description": "Ok, deep breath.  Start by copying `python-assignment12/assignment12/lesson12_c.py` to `python-assignment12/myapp.py`. We can reuse the template.  This is in the root of the project folder because you are going to deploy this to the cloud in Task 5.\n\n1. The dataset to use is the Plotly built in `gapminder` dataset.  This has, among other things, the per capita GDP for various countries for each year.  For a given country, there will be one row per year.  This means that the 'countries' column has many duplicates.\n2. You want a dropdown that has each unique country name. You create a Series called `countries` that is the list of countries with duplicates removed.  You use this Series to populate the dropdown.  Give the dropdown the initial value of 'Canada'.\n3. You give the dropdown the id of 'country-dropdown' and also create a dcc.Graph with id 'gdp-growth'.\n4. You create the decorator for the callback, associating the input with the dropdown and the output with the graph.\n5. The decorator decorates an `update_graph()` function.  This is passed the country name as a parameter.  You need to filter the dataset to get only the rows where the country column matches this name.  Then you create a line plot for 'year' vs. 'gdpPercap`.  Give the plot a descriptive name that includes the country name.\n6. The line to run the app doesn't need to change.\n7. Run the program, and check it out in the browser.  Make bug fixes as needed.\n\n---",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c7383"
          },
          {
            "taskNumber": 5,
            "title": "Deploying to Render.com",
            "description": "1. Create a free account at Render.com.\n2. Change `myapp.py` to add a line:\n   ```python\n   app = Dash(__name__)\n   server = app.server # <-- This is the line you need to add\n   ```\n3. Add, commit, and push your changes to GitHub.  If you are using a branch, create a PR and merge that branch with main.\n4. Go to your render.com dashboard and create a new web service.  Provide the public URL of your python-assignment12 repository.  You must specify a unique name.  The default name is the same as your GitHub repository, so that will likely conflict with another student.\n5. The \"Start Command\" for the web service should be changed to read:\n   ```\n   gunicorn myapp:server\n   ```\n   The gunicorn package is a Python web server, which is used to run the Flask server for your Dash app.\n6. Click the \"Deploy Service\" button.\n7. Wait. Wait. Wait. The Render free plan is not fast.\n8. Eventually, it will say that the service is running.  Wait.  Wait some more.\n9. Click on the https link for your service in the upper part of your render.com dashboard.  Wait.  Wait.  Keep waiting.\n10. After a while, you'll see the app!  Congratulations, you are live! \n\n---",
            "codeExample": "```python\n   app = Dash(__name__)\n   server = app.server # <-- This is the line you need to add\n   ```\n\n```\n   gunicorn myapp:server\n   ```",
            "_id": "68f812aec22606ecfa5c7384"
          },
          {
            "taskNumber": 6,
            "title": "Reflection",
            "description": "Create a file in the assignment12 folder called reflection.txt, and put in the following thoughts:\n\n1. Reflect on the differences between static and interactive visualizations.\n2. Write a short paragraph discussing the advantages of using dashboards for real-time data exploration.\n3. Explain how interactive tools like Plotly and Dash can improve data communication in professional settings.",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c7385"
          },
          {
            "taskNumber": 7,
            "title": "Commit Your Work",
            "description": "Add and commit all of the files created for the assignment to the `assignment12` branch.\n\n---\n\n#  Optional Assignment on Streamlit\n\nYou can use either Dash or Streamlit for your capstone project.\n\n## Overview\nThis assignment is to be implemented using **Streamlit**.  \nYou will **import a dataset**, **build a dashboard**, **visualize insights**, **showcase data cleaning**, and **deploy your app** to **Streamlit Community Cloud**.\n\n### Requirements\n- Import a dataset that has already been cleaned and prepared\n- Explain what cleaning and preparation was done\n- Visualize key insights through interactive dashboards\n- Deploy your app using Streamlit Community Cloud",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c7386"
          },
          {
            "taskNumber": 1,
            "title": "Project Setup",
            "description": "1. Create a new folder called ``streamlit-assignment`` for your project on your local machine.  It will be initialized as a git repository, so make sure it is outside of any other git repository.  \n\n2. Initialize a Git repository inside this folder:\n```bash\ngit init\n```\n\n3. Create a .gitignore file and make sure it includes:\n```bash\n.venv/\n__pycache__/\n*.pyc\n.DS_Store\n```\n\n4. Set up a virtual environment:\n```bash\npython -m venv .venv\nsource .venv/bin/activate  # on macOS/Linux\n.venv\\Scripts\\activate     # on Windows\n```\n\n5. Create requirements.txt file.  You can use the same requirements.txt file which you used for the Streamlit lesson.\n```bash\nstreamlit\npandas\nplotly\nnumpy\nmatplotlib\n```\n6. Install the dependencies. Run following command in your vs code terminal.\n```bash\npip install -r requirements.txt\n```\n7. Create a Python file named `streamlit_app.py` in your project folder.\n   This is the main script Streamlit will run when deploying your app.",
            "codeExample": "```bash\ngit init\n```\n\n```bash\n.venv/\n__pycache__/\n*.pyc\n.DS_Store\n```\n\n```bash\npython -m venv .venv\nsource .venv/bin/activate  # on macOS/Linux\n.venv\\Scripts\\activate     # on Windows\n```\n\n```bash\nstreamlit\npandas\nplotly\nnumpy\nmatplotlib\n```\n\n```bash\npip install -r requirements.txt\n```",
            "_id": "68f812aec22606ecfa5c7387"
          },
          {
            "taskNumber": 2,
            "title": "Build Your Streamlit Dashboard",
            "description": "Choose a dataset from kaggle to work with for your assignment.  Check the data and make sure it's clean, perform any necessary cleaning before proceeding.\n\n### Example Submissions\n1. Canada Dashboard:\n   - App: https://canada.streamlit.app/\n   - Code: https://github.com/parker84/canada-dashboard\n\n2. Dashboard v2:\n   - App: https://dash-board.streamlit.app/\n   - Code: https://github.com/dataprofessor/dashboard-v2\n\n### Required Components\n\n1. **Title and Description**\n   - Use `st.title()` and `st.markdown()` to introduce your dashboard\n\n2. **Dataset Overview**\n   - Import your cleaned dataset using Pandas.\n   - Display the dataset using `st.dataframe()` or `st.write()`\n   - Optionally, summarize with `.describe()` or key statistics\n\n3. **Data Cleaning Summary**\n   - Briefly describe what cleaning steps you performed\n   - Optional: Show comparison table (raw vs cleaned)\n\n4. **Visualizations**\n   - Include at least two interactive charts\n   - Examples: bar chart, pie chart, line chart, scatter plot, histogram\n   - Use Streamlit's built-in charts or libraries like Plotly/Matplotlib",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c7388"
          },
          {
            "taskNumber": 3,
            "title": "Deploy Your App",
            "description": "1. Create a **GitHub repository** and push your code:\n   - Log in to your GitHub account and create a new repository.(https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-new-repository)\n   - Copy the repository URL.\n   - Link your local project folder to the GitHub repository:\n     ```bash\n     git remote add origin <repository-url>\n     ```\n   - Add and commit your changes:\n     ```bash\n     git add .\n     git commit -m \"Initial commit\"\n     ```\n   - Push your code to the GitHub repository:\n     ```bash\n     git branch -M main\n     git push -u origin main\n     ```\n2. Deploy to **Streamlit Community Cloud**:\n   - Visit [Streamlit Cloud](https://streamlit.io/cloud) and log in with your Streamlit account. If you don't have an account, create one using your email or GitHub credentials.\n\n   - Click on the **\"New App\"** button to start the deployment process.\n\n   - In the **\"Select a repository\"** section:\n      - Connect your GitHub account if you haven't already.\n      - Choose the repository where your Streamlit app code is stored.\n\n   - In the **\"Branch\"** dropdown, select the branch containing your code (usually `main`).\n\n   - In the **\"Main file path\"** field, specify the path to your Streamlit app file (e.g., `streamlit_app.py`).\n\n   - Click **\"Deploy\"** to start the deployment process.\n\n   - Wait for the deployment to complete. Once done, you will see a URL where your app is hosted.\n\n   - Test your app by visiting the provided URL to ensure everything works as expected.\n\n   - If you need to make updates to your app, push the changes to your GitHub repository. Streamlit Cloud will automatically redeploy your app with the latest changes.\n3. Verify your app loads successfully and is publicly accessible",
            "codeExample": "```bash\n     git remote add origin <repository-url>\n     ```\n\n```bash\n     git add .\n     git commit -m \"Initial commit\"\n     ```\n\n```bash\n     git branch -M main\n     git push -u origin main\n     ```",
            "_id": "68f812aec22606ecfa5c7389"
          },
          {
            "taskNumber": 4,
            "title": "Submit Your Assignment",
            "description": "### Required Submissions\n- Your **Streamlit Community Cloud app URL** (deployment link), this link is added to the ``service_urls.txt`` in the ``assignment12` folder in the `python-assignment12` repo.\n- Your **GitHub repository URL** link is also added to the `service_urls.txt`\n- Add and commit the `service_urls.txt` file in the `assignment12` branch.\n\n### Resources\n- [Streamlit Cheat Sheet](https://cheat-sheet.streamlit.app/)\n\n---\n\n### **Submit Your Assignment on GitHub**  \n\nüìå **Follow these steps to submit your work:**  \n\n#### **1Ô∏è‚É£ Add, Commit, and Push Your Changes** \n- Create a file called `service_urls.txt` in the assignment12 folder.  In it, paste the URL for your Render.com service.  If you did the Streamlit assignment, make sure the Streamlit github repository url and streamlit.io service url are added to the `service_urls.txt` file as well.\n- Within your `python-assignment12` folder, do a git add and a git commit for the files you have created, so that they are added to the `assignment12` branch.\n- Push that branch to GitHub. \n\n#### **2Ô∏è‚É£ Create a Pull Request**  \n- Log on to your GitHub account.\n- Open your `python-assignment12` repository.\n- Select your `assignment12` branch.  It should be one or several commits ahead of your main branch.\n- Create a pull request.\n\n#### **3Ô∏è‚É£ Submit Your GitHub Link**  \n- Your browser now has the link to your pull request.  Copy that link. \n- Paste the URL into the **assignment submission form**.\n- if you did the optional Streamlit assignment, make sure that repository link and the url for the streamlit.io service are included in the `service_urls.txt` file. \n\nTo summarize, a pull request for the `assignment12` branch in your new `python-assignment12` repository is pasted into the link submission field in the **assignment submission form**.  The `render.com` Dash service is published in `service_urls.txt` file. If you do the streamlit assignment, the link for the `streamlit-assignment` repository and the url for the `streamlit.io` service are included in the `service_urls.txt` file.\n\n---",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c738a"
          }
        ],
        "submissionInstructions": "Please submit on time",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Introduction",
          "content": "# **Lesson 12 ‚Äî Advanced Data Visualization Techniques**",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7372"
        },
        {
          "subsectionOrder": 2,
          "title": "Lesson Overview",
          "content": "**Learning objective:** students will learn how to build advanced, interactive visualizations using Python libraries such as Pandas, Plotly, and Dash. They will practice visualizing data from DataFrames, creating interactive charts, and developing simple dashboards for real-time data exploration. To prepare for the final project, students will complete this lesson and assignment in a personal GitHub repository rather than in `python_homework`. An optional section introduces Streamlit as an alternative to Dash for dashboard development.\n\n### **Topics:**\n1. Plotting with Pandas: Visualizing data directly from DataFrames.\n2. Interactive Visualizations: Using Plotly for interactive plotting.\n3. Dashboards: Creating dynamic dashboards with Plotly and Dash.\n4. Advanced Customization: Advanced interactivity, subplots, and real-time updates.\n5. Optional lesson on Streamlit\n6. Dash and Streamlit compared.\n\nNote:\nFor your final project, you will create a dashboard using one of these tools.  Which one you use is up to you.  Check out the optional Streamlit information and assignment if you are interested in Streamlit.\n\n### **Setup**\n\nYou are using your own repository, both for the lesson and for the assignment.  If you do the Streamlit portions, these are also to be done within a second new repository.  The steps are these:\n\n1. Create a folder called  `python-assignment12`.  This should **not** be inside of the `python_homework` folder.  Change to this directory.\n2. Do a `git init`.\n3. Create a `.gitignore` file.  You can copy the one from `python_homework`, but be sure you know why that one says what it does.\n4. Create a virtual environment called `.venv`.  See the README.md for `python_homework` if you don't remember how this is done.\n5. Create a `requirements.txt` file.  This should include the following packages.  These will also cover the optional streamlit lesson.\n    - numpy\n    - pandas\n    - matplotlib\n    - plotly\n    - seaborn\n    - dash\n    - gunicorn\n    - streamlit\n    \n    If you do the Streamlit assignment, some further setup is needed since a separate repo will be created.  You can specify specific versions of these packages (see the requirements.txt for `python_homework`), but if you don't, you will get the latest version of each of these.\n6.  **Important** Activate the virtual environment, with the command:\n    ```bash\n    source .venv/bin/activate\n    ```\n    Or, for Windows Git Bash:\n    ```bash\n    source .venv/Scripts/activate\n    ```\n    Verify that the virtual environment is active with:\n    ```bash\n    which python\n    ```\n    This should return a python location within your python-assignment12 folder.\n7. Load the required packages as follows:\n    ```bash\n    pip install -r requirements.txt\n    ```\n8. Do `VSCode .`.  Bring up VSCode command palette (Ctrl-Shift-P) and select Python: Select Interpreter.  Select the one with `.venv`.  Close any VSCode terminal sessions and start a new one.  You should see in the command prompt that `.venv` is active.\n9. On GitHub, create a new public repository called python-assignment12.  Do not create a README.md or .gitignore or license.  Copy the URL of the repository.  You can use either the HTTL or SSH URL, depending on your preference.  Set the remote for the repository, and push your code.\n    ```bash\n    git remote add origin <url>\n    git add -A\n    git commit -m \"first commit\"\n    git push origin main\n    ```\n10. Create an `assignment12` git branch. Create a folder called `assignment12`.  This is for the exercises prior to the assignment and will also be used for the assignment.\n\nFor the following code examples, you create programs in the `assignment12` folder.  Some of this code won't run correctly within the Python interactive shell.  As you do the lesson and assignment, periodically add and commit your changes and push the `assignment12` branch to GitHub.  This is to practice the procedures of a development shop.  When you use these procedures, you can be confident that you won't break something and have to start over.  You can just switch back to the last commit if something breaks.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7373"
        },
        {
          "subsectionOrder": 3,
          "title": "Plotting with Pandas",
          "content": "### **Overview**\nPandas simplifies data visualization by providing built-in plotting methods for DataFrames and Series. These plots are ideal for quick data exploration and basic visualizations.\n\n### **Key Plot Types:**\n- **Line Plot:** Displays trends over time or continuous data.\n- **Bar Plot:** Used for comparing categorical data.\n- **Histogram:** Shows the distribution of numerical data.\n\n### **When to Use These Plots:**\n- **Line Plots** are typically used for showing data trends over time, such as sales or stock prices over months.\n- **Bar Plots** are ideal when you need to compare quantities between different categories, such as the sales of different products or regions.\n- **Histograms** are useful for analyzing the distribution of numerical data, identifying patterns, skewness, or the range of values.\n\nWithin the `assignment12` folder of your `python-assignment12` directory, create `lesson12_a.py`.  This code uses the DataFrame plot() method, which is part of Pandas, but, to actually display the plot, you also need Matplotlib, to do the `show()`.  Your program should contain the following code:\n\n### **Example Code: Plotting with Pandas**\n\nCreate a file called `lesson12_a.py`, with the following content:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load a dataset\ndata = {\n    \"Month\": [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\"],\n    \"Sales\": [100, 150, 200, 250, 300, 350],\n    \"Expenses\": [80, 120, 180, 200, 220, 300]\n}\ndf = pd.DataFrame(data)\n\n# Line Plot\ndf.plot(x=\"Month\", y=[\"Sales\", \"Expenses\"], kind=\"line\", title=\"Sales vs. Expenses\")\nplt.show()\n\n# Bar Plot\ndf.plot(x=\"Month\", y=\"Sales\", kind=\"bar\", color=\"skyblue\", title=\"Monthly Sales\")\nplt.show()\n```\n\nTry this out.  The behavior of Matplotlib is like what you saw in the previous lesson.  A graphic window appears, and the program stops to wait at that point, until you close the window.\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7374"
        },
        {
          "subsectionOrder": 4,
          "title": "Interactive Visualizations with Plotly",
          "content": "### **Overview**\nPlotly is a powerful library for creating interactive, highly customizable plots. It allows for hover tooltips, zooming, and dynamic interactions that improve user experience.  The code below uses a sample dataset that is provided as part of Plotly, but in the general case, you would use a Pandas DataFrame loaded from a CSV file or database.  Within the `assignment12` folder, create `lesson12_b.py` with the following code:\n\n### **Example Code: Interactive Scatter Plot**\n```python\nimport plotly.express as px\nimport plotly.data as pldata\n\n\ndf = pldata.iris(return_type='pandas') # Returns a DataFrame.  plotly.data has a number of sample datasets included.\nfig = px.scatter(df, x='sepal_length', y='petal_length', color='species',\n                 title=\"Iris Data, Sepal vs. Petal Length\", hover_data=[\"petal_length\"])\nfig.write_html(\"iris.html\", auto_open=True)\n\n# Do not try fig.show()!  This sometimes works, but usually it just hangs.\n```\nTry it out!  The interactive plot comes up in your browser, and you can hover over data points zoom, select, etc.  The HTML file (with lots of embedded JavaScript) can be used in other contexts.exit\n\n### **Key Features of Plotly:**\n- **Interactivity:** Hover tooltips, zooming, and panning.\n- **Customization:** Wide range of customization options for visual aesthetics and user interaction.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7375"
        },
        {
          "subsectionOrder": 5,
          "title": "Building Dashboards with Dash",
          "content": "### **Overview**\nDash is a framework for creating interactive web applications in Python. It leverages Plotly for visualizations and allows you to create dashboards that update in real-time based on user input.  When you install Dash, you also install Flask as a dependency.  Flask is a web application server framework, like Node or Rails, but in Python.  When you run a Dash dashboard, the Flask server runs.  Dash pages are dynamic, in that you can add dropdown lists or other controls to affect what is displayed.  Within the `assignment12` folder, create a file `lesson12_c.py`, with the following content:\n\n### **Example Code: Simple Dashboard**\n```python\nfrom dash import Dash, dcc, html, Input, Output\nimport plotly.express as px\nimport plotly.data as pldata\n\ndf = pldata.stocks(return_type='pandas', indexed=False, datetimes=True)\n\n\n# Initialize Dash app\napp = Dash(__name__)\n\n# Layout\napp.layout = html.Div([\n    dcc.Dropdown(\n        id=\"stock-dropdown\",\n        options=[{\"label\": symbol, \"value\": symbol} for symbol in df.columns],\n        value=\"GOOG\"\n    ),\n    dcc.Graph(id=\"stock-price\")\n])\n\n# Callback for dynamic updates\n@app.callback(\n    Output(\"stock-price\", \"figure\"),\n    [Input(\"stock-dropdown\", \"value\")]\n)\ndef update_graph(symbol):\n    fig = px.line(df, x=\"date\", y=symbol, title=f\"{symbol} Price\")\n    return fig\n\n# Run the app\nif __name__ == \"__main__\": \n    app.run(debug=True) \n```\n\nOk, now run this file.  And, what seems to happen is ... nothing, except the program seems to hang.  But, actually, you have started a web server.  Use your web browser to connect to it, at `http://localhost:8050`.  You see the interactive chart!  At the top, you can select a stock symbol, and you see the corresponding line chart for the stock price.  Sometimes you might have a port conflict, in which case you can specify an alternate port, by adding `port=8055` or some such port to your app.run() statement.\n\nNow, to explain the code above.  This code uses another sample dataset built into Plotly.  That is loaded via the `pldata.stocks()` statement above.  You can find these documented here: [https://plotly.com/python-api-reference/generated/plotly.data.html].  Of course, you could load a CSV file instead.  \n\nTo explain the code, a bunch of comments are added below.\n\n```python\nfrom dash import Dash, dcc, html, Input, Output # Dash components you need\nimport plotly.express as px # Dash relies on Plotly to actually do the plotting.  Plotly creates an HTML page with lots of JavaScript.\nimport plotly.data as pldata # This is only needed to give access to the Plotly built in datasets.\n\ndf = pldata.stocks(return_type='pandas', indexed=False, datetimes=True) # This loads one of the datasets\n\n\n# Initialize Dash app\napp = Dash(__name__) # This creates the app object, to wich various things are added below. \n# __name__ is the name of the running Python module, which is your main module in this case\n\n# Layout: This section creates the HTML components\napp.layout = html.Div([ # This div is for the dropdown you see at the top, and also for the graph itself\n    dcc.Dropdown( # This creates the dropdown\n        id=\"stock-dropdown\", # and it needs an id\n        options=[{\"label\": symbol, \"value\": symbol} for symbol in df.columns], # This populates the dropdown with the list of stocks\n        value=\"GOOG\" # This is the initial value\n    ),\n    dcc.Graph(id=\"stock-price\") # And the graph itself has to have an ID\n])\n\n# Callback for dynamic updates\n@app.callback( # OK, now this is a decorator.  Hmm, we haven't talked about decorators in Python.  This decorator is decorating the update_graph() function.\n    # Because of the decorator, the update_graph() will be called when the stock-dropdown changes, passing the value selected in the dropdown.\n    Output(\"stock-price\", \"figure\"),  # And ... you get the graph back\n    [Input(\"stock-dropdown\", \"value\")] # When you pass in the value of the dropdown.\n)\ndef update_graph(symbol): # This function is what actually does the plot, by calling Plotly, in this case a line chart of date (which is the index) vs. the chosen stock price.\n    fig = px.line(df, df.index, y=symbol, title=f\"{symbol} Price\")\n    return fig\n\n# Run the app\nif __name__ == \"__main__\": # if this is the main module of the program, and not something included by a different module\n    app.run(debug=True) # start the Flask web server\n```\nWe'll explain decorators in the next lesson, but here is some additional explanation on the `@app.callback` decorator.  That decorator is provided by Dash and is associated with the app object.  Within your `app.layout`, you can have one or several HTML controls, each with an ID.  In this case, you have just one, the dropdown.  When you use `app.@callback`, the function that follows (the function is update_graph() in this case) will be called any time one of the controls that is specified as an Input for that callback has a change in value, that is, each time the user enters or clicks on something.  The changed value or, in the case of multiple Inputs, the changed values, are then passed to the decorated function.  That function returns the Output, in this case a graph.  (It is also possible to have multiple Outputs for the callback, but that's beyond the scope of this lesson.)  You can have multiple `@app.callback` decorator statements within a Dash program, each decorating a different function.  So, for example, you could have several different graphs on the page, each of which is controlled by a different set of HTML controls.\n\nWhew, clear as mud, eh?  Let's give a summary of how Dash works.\n\n### **A Summary of Dash**\n\n1. The app: You always create an app, with `app = Dash(__name__)`.  This gives you access to `app.layout()` and `@app.callback()`.\n2. HTML components: You declare these with `html.something()`.  This is the usual list of html components: div, container, paragraph, whatever, nested as you choose, and styled as you choose.  (Styling is out of scope for this lesson, but there is, for example, a Bootstrap package for Dash.)\n3. Dynamic components and controls: You declare these with `dcc.something()`.  These are (a) components you want to modify in response to user input, or (b) controls that catch that user input.  For controls, you have the usual list: radio buttons, dropdowns, input, sliders, tabs, etc.  For components you update, one is `dcc.graph`, but you can update a paragraph or div or whatever you like.  One useful thing to update is a data table, You import dash_table from dash, and then do a `dash_table.DataTable(...)`.  You can create pandas DataFrames, convert them to dicts, and display them in the table.\n4. Callbacks.  If you have controls, you will have one or more functions that are decorated with @app.callback.  As follows:\n    ```python\n    @app.callback(Output(id, what), [Input(id, value), Input(id2, value2), ...])\n    update_function(value, value2, ...):\n        # logic depending on the values passed\n        ...\n        return what\n    ```\n    Let's break this down.  The Output specifies the id of the HTML element to update, and the attribute (what) of that component that is to be changed.  You have a list of one or more inputs, and for each, you have the id of the HTML control and the value it has.  (Note that for a multi-select list, `value` may be a list.)  Any time the value of one of these controls changes as a result of user input, the update_function() will be called so that it can do its thing and return the updated stuff.  Several different update functions might be registered for the same input, so as to update different HTML elements when a particular control or set of controls changes.\n\nThat's Dash in a nutshell.  Your homework doesn't include DataTable, but maybe it should, as DataTable provides a way to display DataFrames.  The DataTable works like:\n\n```python\nimport pandas as pd\nfrom dash import dash_table, Dash, html\n\napp = Dash(__name__)\n\ndf = pd.read_csv(\"some csv file\")\n\napp.layout = html.Div([dash_table.DataTable(df.to_dict('records'), [{\"name\": i, \"id\": i} for i in df.columns], id='tbl')])\n```\nAnd you can make the table interactive, for example by having a callback any time a cell is clicked.  \n\nTo understand Dash and Plotly fully, you need to spend time studying the Plotly and Dash documentation, or perhaps by asking your friendly AI how to do this or that.\n\n---",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7376"
        },
        {
          "subsectionOrder": 6,
          "title": "Reflection",
          "content": "### **Differences Between Static and Interactive Visualizations:**\n- **Static Visualizations:** Easier to create and quicker to render but lack user interaction.\n- **Interactive Visualizations:** Allow users to explore data, zoom, filter, and interact, providing a deeper and more engaging analysis experience.\n\n### **Advantages of Dashboards:**\n- Real-time data exploration and updates.\n- User interaction with data (e.g., dropdowns, sliders) enables custom insights.\n- Efficient presentation of key metrics in a professional setting.\n\n---\n\n# Building Interactive Apps with Streamlit\n\nThis portion of the lesson supports the optional assignment on Streamlit.  For the capstone final project, you can use either Dash or Streamlit.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7377"
        },
        {
          "subsectionOrder": 7,
          "title": "Lesson Overview",
          "content": "**Learning objective:** Learn to create interactive web applications for data visualization and analysis using Streamlit.\n\nTopics: \n  * Introduction to Streamlit and its benefits\n  * Basic Streamlit components and layout\n  * Interactive data visualization with Streamlit\n  * Deploying Streamlit applications",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7378"
        },
        {
          "subsectionOrder": 8,
          "title": "What is Streamlit?",
          "content": "Streamlit is a Python library that makes it easy to create custom web apps for machine learning and data science. It turns data scripts into shareable web apps in minutes, not weeks.\n\n### Key Features\n* Simple Python-first syntax\n* Rich set of UI components\n* Easy integration with data science libraries\n* Quick deployment options\n\n### Installation and Setup\n\nFirst, let's set up a virtual environment and install Streamlit:\n\n1. Create a project folder named `streamlit_project` in the top level of your `assignment12` folder and change to that folder.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7379"
        },
        {
          "subsectionOrder": 9,
          "title": "Basic Streamlit Components",
          "content": "### Text and Data Display\n\nStreamlit provides a variety of methods to render static content such as text, markdown, and code. These elements are useful for building the layout and guiding users through your app.\n\n#### Exercise 1: Text and Data Display\n- Create a python script app.py\nIn this exercise, you will add static content to your app by writing the following code into your app.py file:\n```python\nimport streamlit as st  # Importing the Streamlit library\n\n# Basic text elements\nst.title(\"My First Streamlit App\")  # Adds a big title at the top of the app\nst.header(\"Section 1\")  # Adds a section header ‚Äî good for breaking content into parts\nst.subheader(\"Header\")  # Slightly smaller than header ‚Äî useful for structure\nst.subheader(\"Subheader\")  # Another level down ‚Äî keeps things organized\nst.text(\"Simple text\")  # Displays plain, unformatted text ‚Äî like a basic message\nst.markdown(\"**Bold** and *italic* text\")  # Markdown lets you add simple formatting like bold and italics\n\n# Display data\nst.write(\"Automatic data display\")  # Streamlit's flexible method ‚Äî handles strings, numbers, dataframes, and more\nst.code(\"print('Hello World')\", language='python')  # Nicely formats code blocks with syntax highlighting\nst.latex(r\"\\int_{a}^{b} x^2 dx\")  # Renders LaTeX math formulas ‚Äî great for equations\n\n```\n\nOnce you've saved and run your app.py file using the command:\n\n```bash \nstreamlit run app.py\n```\nOpen your browser to http://localhost:8501 to view your app.\n\nNote: Any time you change a value in one of the input components, go to the browser tab and refresh ,Streamlit  reruns the entire script from top to bottom using the updated values. This means your app always reflects the latest state .\nYou can refresh the tab manually, or use the ‚ÄúAlways rerun‚Äù option in the top-right of the Streamlit page for instant updates as you code.\n\n#### Exercise - 2\n### Data Input Components\n```python\n# Text input\nst.header(\"Section 2\")  # A new section to group interactive input components\nname = st.text_input(\"Enter your name\", \"John Doe\")  # Simple text field with a default value\ndescription = st.text_area(\"Description\", \"Write something...\")  # Multi-line text box for longer input\n\n# Numeric input\nage = st.number_input(\"Age\", min_value=0, max_value=120, value=25)  # Number picker with min/max range\nscore = st.slider(\"Score\", 0, 100, 50)  # Slider to pick a number in a range ‚Äî great for ratings or scores\n\n# Selection widgets\noption = st.selectbox(\"Choose an option\", [\"A\", \"B\", \"C\"])  # Dropdown menu ‚Äî user picks one option\noptions = st.multiselect(\"Multiple options\", [\"X\", \"Y\", \"Z\"])  # Allows multiple selections at once\n\n# Date and time\ndate = st.date_input(\"Select date\")  # Calendar-style date picker\ntime = st.time_input(\"Select time\")  # Clock-style time picker\n\n# Buttons and checkbox\nif st.button(\"Click me\"):  # A button that runs code when clicked\n    st.write(\"Button clicked!\")  # Responds when the button is pressed\n    \nif st.checkbox(\"Show/Hide\"):  # Checkbox to toggle something on/off\n    st.write(\"Visible content\")  # Displays this text only if the box is checked\n```\n- you can again now refresh your tab in browser to see the updated output.\n\nNote:Unlike dropdowns or sliders (which always keep a selected value), buttons in Streamlit are \"stateless\" ‚Äî they don‚Äôt hold their state after being clicked. Instead, Streamlit checks whether the button was pressed during that specific run of the script. That‚Äôs why we use an if statement with them.\n\nAlso, clicking a button triggers a full rerun of the script, just like other controls.\n\nNote: üìç Where does st.write() show output?\nStreamlit renders output in the order the code runs ‚Äî so the st.write() here appears right under the button. To control placement more precisely, you can use layout elements like columns or placeholders.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c737a"
        },
        {
          "subsectionOrder": 10,
          "title": "Exercise 3: Layout and Containers",
          "content": "In this section, you‚Äôll learn how to organize your app using columns, expanders, and a sidebar.\n\nContinue working in the same app.py file you created earlier. You can either:\n-Append the new code at the bottom of the file\n\n\n\n```python\nst.header(\"Section 3\")\n\n# Create two side-by-side columns\ncol1, col2 = st.columns(2)\n\nwith col1:  # Everything under this goes into the left column\n    st.header(\"Column 1\")\n    st.write(\"Content for column 1\")\n\nwith col2:  # Everything under this goes into the right column\n    st.header(\"Column 2\")\n    st.write(\"Content for column 2\")\n\n# Expandable sections\nwith st.expander(\"Click to expand\"):\n    st.write(\"Expanded content here\")\n\n# Sidebar\nst.sidebar.title(\"Sidebar\")\nsidebar_option = st.sidebar.selectbox(\"Select option\", [\"A\", \"B\", \"C\"])\n```\n\nStreamlit uses the Python with statement to define scoped areas for content. \n```python\nwith col1:\n    st.write(\"Some content\")\n```\n‚Ä¶it means \"put this content inside column 1.\" Streamlit handles layout placement based on these scopes ‚Äî it‚Äôs a readable way to group content visually.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c737b"
        },
        {
          "subsectionOrder": 11,
          "title": "Exercise 4: Building a Simple Dashboard",
          "content": "Create a new python script named 'dashboard_app.py'.\n\n```python\nimport streamlit as st  \nimport pandas as pd     # Used to work with tabular data\nimport numpy as np      # Helps generate random numbers\nimport plotly.express as px  # For interactive charts\n\n# Create sample data ‚Äî just faking some numbers to simulate a small product dataset\nnp.random.seed(42)  # Setting a seed so results are consistent every time you run\nsample_data = {\n    'Product': ['Product A', 'Product B', 'Product C', 'Product D'],\n    'Sales': np.random.randint(100, 500, size=4),   # Random sales numbers\n    'Profit': np.random.randint(20, 100, size=4)    # Random profit numbers\n}\ndf = pd.DataFrame(sample_data)  # Convert the data into a DataFrame for easy handling\n\n# Sidebar filters ‚Äî this shows up in the sidebar for user interaction\nst.sidebar.header('Filter Options')  # Sidebar title\nselected_product = st.sidebar.selectbox('Select Product', df['Product'])  # Dropdown to choose a product\n\n# Filter the data based on the user's selection\nfiltered_df = df[df['Product'] == selected_product]  # Show only the row that matches the selected product\n\n# Main app content starts here\nst.title('Simple Product Dashboard')  # Big title for the dashboard\n\n# Display key numbers using metrics ‚Äî side-by-side using columns\ncol1, col2 = st.columns(2)  # Create two columns for layout\nwith col1:\n    st.metric('Sales', f\"${filtered_df['Sales'].values[0]:,}\")  # Show sales in a pretty format\nwith col2:\n    st.metric('Profit', f\"${filtered_df['Profit'].values[0]:,}\")  # Show profit similarly\n\n# Add a bar chart comparing all products ‚Äî gives full context beyond the filter\nst.subheader('Sales and Profit Comparison')  # Subheading for the chart\nbar_chart = px.bar(df, x='Product', y=['Sales', 'Profit'], barmode='group')  # Grouped bar chart\nst.plotly_chart(bar_chart)  # Render the chart in the app\n```\n\nin your terminal execute\n```bash\nstreamlit run dashboard_app.py\n```\n\nIf you completed the Streamlit lesson, add and commit the additional folder and files created.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c737c"
        },
        {
          "subsectionOrder": 12,
          "title": "Conclusion",
          "content": "You now know how to:\n- Create basic Streamlit apps\n- Add input forms, layouts, and sidebars\n- Build simple dashboards with metrics and charts",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c737d"
        },
        {
          "subsectionOrder": 13,
          "title": "Dash and Streamlit",
          "content": "Streamlit, like Dash, is a way of creating a web based dashboard for data presentation.\n\nAdvantages of Dash:\n- Widely used\n- Gives you a lot of control, all the power that you have in any front end framework\n- Better for large complicated projects\n\nDisadvantages of Dash:\n- Steep learning curve\n\nAdvantages of Streamlit:\n- Pretty easy as compared with Dash\n\nDisadvantages:\n- Not as widely used (though gaining popularity)\n- In a production environment, it is not very scalable",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c737e"
        },
        {
          "subsectionOrder": 14,
          "title": "Summary",
          "content": "In this lesson, you learned:\n1. How to visualize data directly from Pandas DataFrames.\n2. How to create interactive visualizations with Plotly.\n3. How to build dynamic dashboards using Dash.\n4. The differences between static and interactive visualizations and their real-world applications.\n\nFor more details, explore the [Plotly Documentation](https://plotly.com/python/) and [Dash Documentation](https://dash.plotly.com/).\n\n---\n\n### Additional Resources:\n1. **Matplotlib Tutorials:** For more detailed Matplotlib tutorials, check out [Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/index.html).\n2. **Seaborn Gallery:** Explore different plot examples at the [Seaborn Gallery](https://seaborn.pydata.org/examples/index.html).\n3. **Data Visualization in Python:** To explore more about data visualization strategies and best practices, visit [Data Visualization in Python](https://realpython.com/python-data-visualization-using-matplotlib/).",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c737f"
        }
      ]
    },
    {
      "id": "68f812aec22606ecfa5c738c",
      "lessonNumber": 13,
      "title": "Lesson 13 ‚Äî Capstone Project - Kaggle Dataset Project",
      "status": "pending",
      "assignment": {
        "title": "Capstone Project - Kaggle Data Pipeline",
        "objective": "No objective specified",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [
          {
            "taskNumber": 1,
            "title": "Task 1",
            "description": "# Capstone Project - Kaggle Data Pipeline\n\nSelect one of the following datasets and perform cleaning, aggregation, analysis and visualization, illustrating at least 3 insights gleaned from the data.  Optionally, students can propose an alternative dataset and set of metrics to their CIL by the end of week 13.  The projects are intentionally open-ended in that it is up to the student to decide what they learn from the data, and how it is documented and visualized.\n\n## **Global Superstore**\n\nThe [Global Superstore](https://www.kaggle.com/datasets/anandaramg/global-superstore/data) is a collection of sales data from a large retailer.  The goal is to find, document and visualize at least three business insights from the data.\n\n1. **Data Conversion and Cleaning**\n\nLoad the data into a ``DataFrame`` using the appropriate delimiter.  Evaluate data quality, looking for issues such as duplicates, null values, and mixed formats.  If these are found, decide whether they signify an issue which needs to be corrected.  Some potential cleaning tasks include:\n\n* Convert dates to ``datetime``\n* Strip whitespace\n* Unify abbreviations such as state names\n* Convert numbers to ``float``\n\nDocument the data cleaning performed in a Markdown block\n\n2. **Feature Engineering**\n\nCreate at least three additional columns which can be used to derive insights from the data.  For example:\n\n* Columns for just ``year`` and ``month`\n* Derive ``Gross Margin`` from ``Profit`` and ``Sales``\n* Discretize discounts into buckets such as ``none``, ``low``, ``medium``, ``high``\n\n3. **Aggregation**\n\nPerform at least three aggregations to help drive insights.  For example:\n\n* top/bottom 10 states by total profit\n* Category ``Gross Margin``\n* ``Customer Lifetime Value`` (``CLV``) - Total profit each customer has generated across all orders\n\n4. **Analyze, Document, and Visualize**\n\nCreate at least three plots and associated metrics to illustrate the insights found in the data.\nInclude ``Markdown`` sections which explain the graphs and analysis.\n\n## **TMDB 5000 Movie Dataset**\n\nThe [TMDB 5000 Movie Dataset](https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata?select=tmdb_5000_movies.csv) is a collection of movies with a variety of data about production along with ratings.  The goal is to find the factors which influence critical and financial success.\n\n1. **Data Conversion and Cleaning**\n\nLoad the data into a ``DataFrame``.  Evaluate data quality, looking for issues such as duplicates, null values, composite data which could be expanded into additional columns, and mixed formats.  If these are found, decide whether they signify an issue which needs to be corrected.  Some potential cleaning tasks include:\n\n* Convert dates to ``datetime``, year vs date consistency\n* Handle null values if necessary\n* Numeric conversion e.g. ``revenue`` to ``float``\n* Expand/convert ``json`` formated columns, e'g' ``genres``, ``keywords``, ``production companies``\n* This is an explanation of the [format](https://www.kaggle.com/code/sohier/tmdb-format-introduction)\n\nDocument the data cleaning performed in a Markdown block\n\n2. **Feature Engineering**\n\nCreate at least three additional columns which can be used to derive insights from the data.  For example:\n\n* Can be satisfied by expanding ``json`` columns\n* Derive ``Gross Margin`` from ``Profit`` and ``Sales``\n* Discretize discounts into buckets such as ``none``, ``low``, ``medium``, ``high``\n\n3. **Aggregation**\n\nPerform at least three aggregations to help drive insights.  For example:\n\n* top/bottom 10 directors/actors/studios by rating(average vote)/revenue/budget/profit\n* return on investment profit vs budget\n* Discretize budgets\n\n4. **Analyze, Document, and Visualize**\n\nCreate at least three plots and associated metrics to illustrate the insights found in the data.\nInclude ``Markdown`` sections which explain the graphs and analysis.  Examples:\n\n* Best ``genres`` for a given success metric\n* Impact of particular actors and directors on success metrics\n* Genre popularity derived from vote count\n\n## **Life Expectancy (WHO)**\n\nThe [Life Expectancy (WHO)](https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who/data) is a collection of data by country and year with a variety of factors which may impact health.  The goal is to find, document and visualize at least three insights into the factors which influence longevity.\n\n1. **Data Conversion and Cleaning**\n\nLoad the data into a ``DataFrame``.  Evaluate data quality, looking for issues such as duplicates, null values, and mixed formats.  If these are found, decide whether they signify an issue which needs to be corrected.  Some potential cleaning tasks include:\n\n* Missing values as blanks or zeros\n* Conversion to numeric types\n* Unify country names, some appear in different forms\n* Outliers\n\nDocument the data cleaning performed in a Markdown block\n\n2. **Feature Engineering**\n\nCreate at least three additional columns which can be used to derive insights from the data.  For example:\n\n* Adult vs infant mortality\n* Discretize factors such as GDP\n* Combined vaccination rate\n\n3. **Aggregation**\n\nPerform at least three aggregations to help drive insights.  For example:\n\n* Global life-expectancy trend\n* Trends for developing vs developed\n* Life expectancy deciles (10 buckets) vs various driving factors\n\n4. **Analyze, Document, and Visualize**\n\nCreate at least three plots and associated metrics to illustrate the insights found in the data.\nInclude ``Markdown`` sections which explain the graphs and analysis.\n\n## **Seattle Airbnb Open Data (2016)**\n\nThe [Seattle Airbnb Open Data (2016)](https://www.kaggle.com/datasets/airbnb/seattle/data?select=listings.csv) is a collection of data on Airbnb listing in Seattle.  The goal is to find, document and visualize at least three insights into the factors which influence listing success, or conclusion which can be drawn about Seattle such as desirability or affluence for particular neighborhoods.\n\n1. **Data Conversion and Cleaning**\n\nLoad the data into a ``DataFrame``.  Evaluate data quality, looking for issues such as duplicates, null values, and mixed formats.  If these are found, decide whether they signify an issue which needs to be corrected.  Some potential cleaning tasks include:\n\n* Columns with a large percentage of nulls\n* Numeric formatting and conversion\n* Conversion of ``json``\n* Text with html tags\n\nDocument the data cleaning performed in a Markdown block\n\n2. **Feature Engineering**\n\nCreate at least three additional columns which can be used to derive insights from the data.  For example:\n\n* Derived from ``json`` columns\n* revenue per month\n* Simplified room type\n* Availability ratio\n\n3. **Aggregation**\n\nPerform at least three aggregations to help drive insights.  For example:\n\n* Median price by neighbourhood & simplified room type\n* Review score vs. amenity count\n* Monthly availability\n* 10 most/least affluent neighborhoods and associated profitability\n\n4. **Analyze, Document, and Visualize**\n\nCreate at least three plots and associated metrics to illustrate the insights found in the data.\nInclude ``Markdown`` sections which explain the graphs and analysis.\n\n\n## Kaggle Project Rubric\n\n* **General Code Quality**\n\n    * [ ]  Code demonstrates a strong understanding of Python basics. \n    * [ ]  Code is well organized and documented with comments.\n    * [ ]  Functions are used to structure and organize the code.\n\n* **File Handling and Data Loading**\n\n    * [ ]  Data is loaded from appropriate file formats (CSV, JSON, etc.) using Pandas.\n    * [ ]  File paths and loading procedures are clearly defined and handled robustly.\n    * [ ]  Demonstrates effective use of Pandas `read_csv()`, `read_json()`, or similar functions.\n    * [ ]  Uses `head()`, `tail()`, and `info()` effectively to preview and inspect the data.\n\n* **Data Wrangling and Transformation**\n\n    * [ ]  Demonstrates proficiency in using Pandas for data selection, filtering, and transformation.\n    * [ ]  Implements advanced data manipulation techniques, including indexing, slicing, and data type conversion.\n    * [ ]  Handles missing data effectively using `dropna()` or `fillna()` with appropriate strategies.\n    * [ ]  Identifies and removes duplicate records if necessary using Pandas.\n    * [ ]  Code is efficient, well-documented, and follows Pandas best practices.\n    * [ ]  At least three extracted features\n\n* **Data Aggregation**\n\n    * [ ]  Uses Pandas `groupby()` function effectively to aggregate data and gain insights.\n    * [ ]  Applies a variety of aggregation functions (e.g., `sum()`, `mean()`, `count()`, `min()`, `max()`) to analyze grouped data.\n    * [ ]  Clearly presents and interprets the results of data aggregation.\n    * [ ]  At least 3 aggregations\n\n* **Visualization Quality**\n\n    * [ ]  Creates multiple (3+) high-quality, informative, and visually appealing visualizations using appropriate libraries (e.g., Matplotlib, Seaborn, Plotly).\n    * [ ]  Visualizations are clear, concise, and easy to understand, with appropriate titles, labels, legends, and color schemes.\n    * [ ]  Demonstrates strong understanding of design principles.\n    * [ ]  Provides clear explanations of the insights conveyed by each visualization.\n\n* **Chart Types and Interpretation**\n\n    * [ ]  Uses a diverse range of chart types (e.g., scatter plots, bar charts, histograms, box plots, heatmaps) to provide a comprehensive view of the data.\n    * [ ]  Demonstrates a clear understanding of the strengths and weaknesses of each chart type and selects them strategically.\n    * [ ]  Provides insightful interpretations of the visualizations, connecting them to the data analysis and the problem domain.\n\n* **Dataset and Feature Understanding**\n\n    * [ ]  Uses a dataset that is appropriate for the analysis.\n    * [ ]  Demonstrates a clear understanding of the dataset's characteristics, limitations, and potential biases.\n    * [ ]  Selects and uses a sufficient number of relevant features to support a meaningful analysis.\n\n* **Conclusions and Insights**\n\n    * [ ]  Provides a clear, concise, and insightful summary of the project's key findings and conclusions.\n    * [ ]  Connects the findings to the original problem or question and discusses their implications.\n    * [ ]  Identifies potential limitations of the analysis.\n    * [ ]  Demonstrates a strong understanding of the data's story and effectively communicates it.\n    * [ ]  At least 3 conclusions supported by charts and text.\n\n* **Reproducibility**\n\n    * [ ]  Provides a well-organized and clearly documented notebook or script that allows others to easily reproduce the entire analysis.\n    * [ ]  All dependencies are clearly specified.\n\nWhen you are ready to submit your Kaggle and Web Scraping projects, use the [final project submission form](https://airtable.com/appoSRJMlXH9KvE6w/shrthD4fozy4UI21I?prefill_Lessons=Python%20100%20v1:%20Lesson%2015%20-%20Project%20Completion%20and%20Presentations).",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c7390"
          }
        ],
        "submissionInstructions": "Please submit on time",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Introduction",
          "content": "# Lesson 13 ‚Äî Capstone Project - Kaggle Dataset Project",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c738d"
        },
        {
          "subsectionOrder": 2,
          "title": "Lesson Overivew",
          "content": "In the final 3 weeks, student will create two capstone projects to demonstrate the skills learned in the class in a portfolio suitable for presentation.\n\n**Learning Objective:** \nBy completing this project, students will demonstrate their ability to load, clean, analyze, and visualize real-world data using Python. They will apply core programming skills, data wrangling techniques, and effective visualization practices to extract and communicate insights. Students will also build fluency with project structure, code clarity, and reproducibility standards in preparation for real-world data work.\n\n**Projects**\n1. A data pipeline which demonstrates cleaning, aggregation, analysis and visualization in a Kaggle Notebook\n2. A local project published in github which demonstrates web scraping, cleaning, analysis and presentation using plotly and Dash or Streamlit",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c738e"
        },
        {
          "subsectionOrder": 3,
          "title": "Data Sources",
          "content": "### Kaggle Data Pipeline\n\n  The student can select one of four curated datasets listed below or one of their own choosing with CIL approval.\n\n  **Global Superstore**\n\n    Find business insights from sales data at a global superstore to drive business decisions.\n\n  **TMDB 5000 Movie Dataset**\n\n    Find the factors which influence movie performance to find factors which influence success.\n\n  **Life Expectancy (WHO)**\n\n    Find the factors which influence life expectancy.\n\n  **Seattle Airbnb Open Data**\n\n    Find the factors which influence listing performance.\n\n\n### Web Scraping Dashboard\n\nStudents will scrape the [Major League Baseball History site](https://www.baseball-almanac.com/yearmenu.shtml) and display results in a dashboard.  Details will be provided in lesson/assignment 14.\n\n**Kaggle Data Pipeline Project details are provided in the Assignments Section**",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c738f"
        }
      ]
    },
    {
      "id": "68f812aec22606ecfa5c7392",
      "lessonNumber": 14,
      "title": "Lesson 14 ‚Äî Web Scraping and Dashboard Project",
      "status": "pending",
      "assignment": {
        "title": "Lesson 14 ‚Äî Web Scraping and Dashboard Project",
        "objective": "No objective specified",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [
          {
            "taskNumber": 1,
            "title": "Task 1",
            "description": "# Lesson 14 ‚Äî Web Scraping and Dashboard Project\n\n## Lesson Overview\nIn this lesson, students will create **four distinct programs** that retrieve baseball history data from the **Major League Baseball History** website and display the results in an interactive dashboard. The project will involve **Selenium** for web scraping, data cleaning, transforming the data into a structured format, storing it in a SQLite database, querying it via command line, and presenting it using **Streamlit** or **Dash** with interactive visualizations.\n\n**Learning Objective:**  \nBy completing this project, students will:\n- Use **Selenium** to scrape data from a website.\n- Clean and transform the raw data into a structured format.\n- Store the data in a **SQLite** database, with each CSV file as a separate table.\n- Query the database using **joins** via command line.\n- Build an interactive dashboard using **Streamlit** or **Dash** to display the insights.\n\n## Projects\n\n### 1. **Web Scraping Program**  \n- **Goal**: Scrape data from [Major League Baseball History](https://www.baseball-almanac.com/yearmenu.shtml).\n- **Steps**:\n  - Use **Selenium** to retrieve the data.\n  - Extract relevant details (year, event names, statistics).\n  - Save the raw data into **CSV** format for each dataset.\n  - Handle challenges such as:\n    - Pagination\n    - Missing tags\n    - User-agent headers for mimicking a browser request.\n\n### 2. **Database Import Program**  \n- **Goal**: Import the CSV files into a **SQLite** database.\n- **Steps**:\n  - Create a program that imports each CSV as a separate table in the database.\n  - Ensure proper data types (numeric, date, etc.) during the import.\n  - Check for errors during the import process.\n\n### 3. **Database Query Program**  \n- **Goal**: Query the database via the command line.\n- **Steps**:\n  - Allow users to run queries, including at least **joins** (e.g., combining player stats with event data).\n  - Ensure the program can handle flexible querying, allowing for filtering by year, event, or player statistics.\n  - Handle errors and display results appropriately.\n\n### 4. **Dashboard Program**  \n- **Goal**: Build an interactive dashboard using **Streamlit** or **Dash**.\n- **Steps**:\n  - Display insights from the data using at least three visualizations.\n  - Implement interactive features like:\n    - Dropdowns to select years or event categories.\n    - Sliders to adjust the data view.\n  - Dynamically update the visualizations based on user input.\n  - Deploy the dashboard on **Render** or **Streamlit.io** for public access.\n\nAll four programs will be included in a new **GitHub** repository, and the dashboard will be deployed for public access.  This github repo should be separate from any of the other ones created for the class.\n\n## Data Sources\n\nStudents will scrape data from the **[Major League Baseball History Site](https://www.baseball-almanac.com/yearmenu.shtml)**. This site contains historical data such as notable events, player statistics, and achievements year by year.\n\n---\n\n## **Rubric for Lesson 14 - Web Scraping and Dashboard Project**\n\n### **Web Scraping**\n- Uses **Selenium** to retrieve data from the web.\n- Handles common scraping challenges like missing tags, pagination, and user-agent headers.\n- Saves raw data as a **CSV**.\n- Avoids scraping duplication or redundant requests.\n\n### **Data Cleaning & Transformation**\n- Loads raw data into a **Pandas DataFrame**.\n- Cleans missing, duplicate, or malformed entries effectively.\n- Applies appropriate transformations, groupings, or filters.\n- Shows before/after stages of cleaning or reshaping.\n\n### **Data Visualization**\n- Includes at least three visualizations using **Streamlit** or **Dash**.\n- Visuals are relevant, well-labeled, and support the data story.\n- User interactions such as dropdowns or sliders are implemented.\n- Visualizations respond correctly to user input or filters.\n\n### **Dashboard / App Functionality**\n- Built with **Streamlit** or **Dash** to display data and insights.\n- Features clean layout and responsive components.\n- Allows users to explore different aspects of the data.\n- Provides clear titles, instructions, and descriptions for user guidance.\n\n### **Code Quality & Documentation**\n- Code is well-organized and split into logical sections or functions.\n- Inline comments or markdown cells explain major steps or choices.\n- All dependencies are listed and environment setup is reproducible.\n- Comments or markdown cells explain logic.\n- **README.md** includes summary, setup steps, and a screenshot.\n\n---\n\n## **Submission Instructions:**\nWhen you are ready to submit your Kaggle and Webscraping projects, use the [link for the final project submission form](https://airtable.com/appoSRJMlXH9KvE6w/shrthD4fozy4UI21I?prefill_Lessons=Python%20100%20v1:%20Lesson%2015%20-%20Project%20Completion%20and%20Presentations).\n\n1. **Submit the Github Link**: Submit the link to your GitHub repository with all code files, data files, and the **README.md**.\n2. **Final Project Presentation**: Provide a brief explanation of your dashboard functionality and insights during the presentation. Submit your video link in the form.",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c7396"
          }
        ],
        "submissionInstructions": "Please submit on time",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Introduction",
          "content": "# Lesson 14 ‚Äî Web Scraping and Dashboard Project",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7393"
        },
        {
          "subsectionOrder": 2,
          "title": "Lesson Overview",
          "content": "In this lesson, students will create **four programs** that retrieve baseball history data from the **Major League Baseball History** website and display the results in an interactive dashboard. The project will involve **Selenium** for web scraping, data cleaning, transforming the data into a structured format, storing it in a SQLite database, querying it via command line, and presenting it using **Streamlit** or **Dash** with interactive visualizations.\n\n**Learning Objective:**  \nBy completing this project, students will:\n- Use **Selenium** to scrape data from a website.\n- Clean and transform the raw data into a structured format.\n- Store the data in a SQLite database, with each CSV file as a separate table.\n- Query the database using **joins** via command line.\n- Build an interactive dashboard using **Streamlit** or **Dash** to display the insights.\n\n**Projects:**\n1. **Web Scraping Program**: Scrape data from the [Major League Baseball History](https://www.baseball-almanac.com/yearmenu.shtml) website, assemble it into DataFrames, and store the data as several CSV files.\n2. **Database Import Program**: Import the CSV files into a **SQLite** database, with each CSV file as a separate table.\n3. **Database Query Program**: Create a command-line program to query the database using **joins** (e.g., joining player statistics with event data).\n4. **Dashboard Program**: Build a dashboard using **Streamlit** or **Dash** to visualize the data, allowing interactivity for users to explore the data.\n\nAll four programs will be included in the **GitHub** repository, and the dashboard will be deployed for public access.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7394"
        },
        {
          "subsectionOrder": 3,
          "title": "Data Sources",
          "content": "Students will scrape data from the **[Major League Baseball History Site](https://www.baseball-almanac.com/yearmenu.shtml)**, which includes historical baseball data such as notable events, player statistics, and achievements year by year.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7395"
        }
      ]
    },
    {
      "id": "68f812aec22606ecfa5c7398",
      "lessonNumber": 15,
      "title": "Lesson 15 ‚Äî Final Project, Week 2",
      "status": "pending",
      "assignment": {
        "title": "Assignment for Lesson 15",
        "objective": "No objective specified",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [
          {
            "taskNumber": 1,
            "title": "Task 1",
            "description": "### Project Demo Details\n\nIt is good practice to talk about what you have learned and know about project as you will be asked to talk about your experiences in job interviews.  This is why we ask all students to give a short demo of their final projects.  This demo is NOT graded and your performance in the demo will have no bearing on your graduation.  Please record a 3-5 minute presentation that touches on all of the following:\n - [ ] demonstrate the data visualizations you created with your Kaggle project\n - [ ] demonstrate and describe the workings of your Local Data Scraping project done with Dash or Streamlit\n\nRubric details for the Kaggle Project and Scraping Project can be seen below.  Be sure both your projects meet the requirements specified for each.\n\n### How to record your presentation\nYou can record your presentation in any of these three ways: \n 1. Logging into your personal Zoom account and record your personal meeting where only you are in attendance and you are screensharing your work ([this is a link to a video on how to do this](https://www.youtube.com/watch?v=njwbjFYCbGU))\n 2. Use a screen recording program already on your machine\n    * [Mac users can use this link to watch a how-to video](https://www.youtube.com/watch?v=w9Byefp51tY)\n    * [Windows users can use this link to watch a how-to video](https://www.youtube.com/watch?v=PJB7pM5bvNI)\n 3. Use an online option such as loom ([link to a how to video on loom here](https://www.youtube.com/watch?v=oAdLPbfXcQo)).\n\n### How to upload your video so it can be shared\n<details>\n<summary>Click here to expand detailed instructions on how to upload your recording</summary>\n<br>\n<h4>1. Make sure you're logged in to youtube.</h4>\n <p>If you don't have a youtube account, <a href=\"https://support.google.com/youtube/answer/161805?hl=en&co=GENIE.Platform%3DDesktop\">create one by following these instructions</a>.</p>\n <p>You will know you're logged in if you have an initial/icon/other in the top right corner (where the M in the brown circle is on this screenshot):</p>\n \n ![User logged in to youtube account](https://raw.githubusercontent.com/Code-the-Dream-School/intro-to-programming-2025/d2f9b35d7206eeb0af24f85a8e8e5d97d43cbfad/images/Screenshot%202025-01-27%20at%204.01.20%E2%80%AFPM.png?raw=true)\n\n <h4>2. Click `+ Create` in the top right and select `Upload video`</h4>\n \n ![Create menu expanded](https://raw.githubusercontent.com/Code-the-Dream-School/intro-to-programming-2025/d2f9b35d7206eeb0af24f85a8e8e5d97d43cbfad/images/Screenshot%202025-01-27%20at%204.01.27%E2%80%AFPM.png?raw=true)\n\n <h4>3. In the Upload videos window that appears, click the black `Select files` button</h4>\n <p>You'll need to select the file of your recording you have saved on your computer.</p>\n \n ![Upload videos modal](https://raw.githubusercontent.com/Code-the-Dream-School/intro-to-programming-2025/d2f9b35d7206eeb0af24f85a8e8e5d97d43cbfad/images/Screenshot%202025-01-27%20at%204.01.35%E2%80%AFPM.png?raw=true)\n\n<h4>4. The fle title will be the default video title.  You can change this to include your name and \"Python Essentials Final Projects Presentation\"</h4>\n\n![Edit Video Details title](https://raw.githubusercontent.com/Code-the-Dream-School/intro-to-programming-2025/d2f9b35d7206eeb0af24f85a8e8e5d97d43cbfad/images/Screenshot%202025-01-27%20at%204.02.17%E2%80%AFPM.png?raw=true)\n\n<h4>5. Scroll down under the title; select \"No, it's not made for kids\" and click on the `Show more` gray button to make further setting changes</h4>\n\n![Adjust video settings](https://raw.githubusercontent.com/Code-the-Dream-School/intro-to-programming-2025/d2f9b35d7206eeb0af24f85a8e8e5d97d43cbfad/images/Screenshot%202025-01-27%20at%204.02.30%E2%80%AFPM.png?raw=true)\n\n<h4>6. You'll want to be sure the following options for some of the sections that appear after click `Show more` are set to the following:</h4>\n\n - [ ] Altered content: select \"No\" since you have not used AI to alter reality in your video\n \n - [ ] Recording date and location: please select the date you made your final project recording\n      \n - [ ] Shorts remixing: select \"Don't allow remixing\"\n       \n - [ ] Comments and ratings: if you would like classmates to leave comments, leave comments \"On\", otherwise you can turn them off by selecting \"Off\".  _NOTE: we do not and cannot monitor comments.  Please report anything concerning to Code the Dream but have screenshots if needed._  You can also UNcheck the \"Show how many viewers like this video if you wish.\n       \n - [ ] Click the black `Next` button in the bottom right to proceed from the Details section of the upload through the Video elements and Checks portion.\n       \n - [ ] Once you are on the Visibility section of the upload, select \"Unlisted\" as seen here\n\n![Select Unlisted](https://raw.githubusercontent.com/Code-the-Dream-School/intro-to-programming-2025/d2f9b35d7206eeb0af24f85a8e8e5d97d43cbfad/images/Screenshot%202025-01-27%20at%204.04.52%E2%80%AFPM.png?raw=true)\n\n - [ ] Lastly, click `Save` and copy your video link as seen here\n\n![Save and copy](https://raw.githubusercontent.com/Code-the-Dream-School/intro-to-programming-2025/d2f9b35d7206eeb0af24f85a8e8e5d97d43cbfad/images/Screenshot%202025-01-27%20at%204.05.09%E2%80%AFPM.png?raw=true)\n \n</details>\n\n### Submitting your recording\n**Once you have recorded your presentation, please include the youtube link to your presentation in your Assignment Submission Form for this week.  Your assignment submission form this week will have you share the link to your Kaggle Project, the link to your repo that holds your Local Data Scraping Project, and a place for you to share your video recording link.**\n\n### See below for rubrics for your final projects:\n\n<details>\n<summary>Scraping Project Rubric</summary>\n* **Web Scraping**\n\n    * [ ] Uses appropriate libraries (Selenium) to retrieve data from the web\n    * [ ] Handles common scraping challenges like missing tags, pagination, and user-agent headers\n    * [ ] Saves raw data in a structured format such as .csv or .json\n    * [ ] Avoids scraping duplication or redundant requests\n\n* **Data Cleaning & Transformation**\n\n    * [ ] Loads raw data into a Pandas DataFrame or equivalent structure\n    * [ ] Cleans missing, duplicate, or malformed entries effectively\n    * [ ] Applies appropriate transformations, groupings, or filters\n    * [ ] Shows before/after stages of cleaning or reshaping\n\n* **Data Visualization**\n\n    * [ ] Includes at least three visualizations using Plotly, Streamlit, or Dash\n    * [ ] Visuals are relevant, well-labeled, and support the data story\n    * [ ] User interactions such as dropdowns or sliders are implemented\n    * [ ] Visualizations respond correctly to user input or filters\n\n* **Dashboard / App Functionality**\n\n    * [ ] Built with Streamlit or Dash to display data and insights\n    * [ ] Features clean layout and responsive components\n    * [ ] Allows users to explore different aspects of the data\n    * [ ] Provides clear titles, instructions, and descriptions for user guidance\n\n* **Code Quality & Documentation**\n\n    * [ ] Code is well-organized and split into logical sections or functions\n    * [ ] Inline comments or markdown cells explain major steps or choices\n    * [ ] All dependencies are listed and environment setup is reproducible\n    * [ ] Comments or markdown cells explain logic\n    * [ ] `README.md` includes summary, setup steps, and a screenshot  \n</details>\n\n<details>\n<summary>Kaggle Final Project Rubric</summary>\n ## Kaggle Project Rubric\n\n* **General Code Quality**\n\n    * [ ]  Code demonstrates a strong understanding of Python basics. \n    * [ ]  Code is well organized and documented with comments.\n    * [ ]  Functions are used to structure and organize the code.\n\n* **File Handling and Data Loading**\n\n    * [ ]  Data is loaded from appropriate file formats (CSV, JSON, etc.) using Pandas.\n    * [ ]  File paths and loading procedures are clearly defined and handled robustly.\n    * [ ]  Demonstrates effective use of Pandas `read_csv()`, `read_json()`, or similar functions.\n    * [ ]  Uses `head()`, `tail()`, and `info()` effectively to preview and inspect the data.\n\n* **Data Wrangling and Transformation**\n\n    * [ ]  Demonstrates proficiency in using Pandas for data selection, filtering, and transformation.\n    * [ ]  Implements advanced data manipulation techniques, including indexing, slicing, and data type conversion.\n    * [ ]  Handles missing data effectively using `dropna()` or `fillna()` with appropriate strategies.\n    * [ ]  Identifies and removes duplicate records if necessary using Pandas.\n    * [ ]  Code is efficient, well-documented, and follows Pandas best practices.\n    * [ ]  At least three extracted features\n\n* **Data Aggregation**\n\n    * [ ]  Uses Pandas `groupby()` function effectively to aggregate data and gain insights.\n    * [ ]  Applies a variety of aggregation functions (e.g., `sum()`, `mean()`, `count()`, `min()`, `max()`) to analyze grouped data.\n    * [ ]  Clearly presents and interprets the results of data aggregation.\n    * [ ]  At least 3 aggregations\n\n* **Visualization Quality**\n\n    * [ ]  Creates multiple (3+) high-quality, informative, and visually appealing visualizations using appropriate libraries (e.g., Matplotlib, Seaborn, Plotly).\n    * [ ]  Visualizations are clear, concise, and easy to understand, with appropriate titles, labels, legends, and color schemes.\n    * [ ]  Demonstrates strong understanding of design principles.\n    * [ ]  Provides clear explanations of the insights conveyed by each visualization.\n\n* **Chart Types and Interpretation**\n\n    * [ ]  Uses a diverse range of chart types (e.g., scatter plots, bar charts, histograms, box plots, heatmaps) to provide a comprehensive view of the data.\n    * [ ]  Demonstrates a clear understanding of the strengths and weaknesses of each chart type and selects them strategically.\n    * [ ]  Provides insightful interpretations of the visualizations, connecting them to the data analysis and the problem domain.\n\n* **Dataset and Feature Understanding**\n\n    * [ ]  Uses a dataset that is appropriate for the analysis.\n    * [ ]  Demonstrates a clear understanding of the dataset's characteristics, limitations, and potential biases.\n    * [ ]  Selects and uses a sufficient number of relevant features to support a meaningful analysis.\n\n* **Conclusions and Insights**\n\n    * [ ]  Provides a clear, concise, and insightful summary of the project's key findings and conclusions.\n    * [ ]  Connects the findings to the original problem or question and discusses their implications.\n    * [ ]  Identifies potential limitations of the analysis.\n    * [ ]  Demonstrates a strong understanding of the data's story and effectively communicates it.\n    * [ ]  At least 3 conclusions supported by charts and text.\n\n* **Reproducibility**\n\n    * [ ]  Provides a well-organized and clearly documented notebook or script that allows others to easily reproduce the entire analysis.\n    * [ ]  All dependencies are clearly specified.\n\n</details>\n</br>\n\nUse this [link for the final project submission form](https://airtable.com/appoSRJMlXH9KvE6w/shrthD4fozy4UI21I?prefill_Lessons=Python%20100%20v1:%20Lesson%2015%20-%20Project%20Completion%20and%20Presentations) for your Kaggle, Web scraping, and presentation video.",
            "codeExample": "",
            "_id": "68f812aec22606ecfa5c739a"
          }
        ],
        "submissionInstructions": "Please submit on time",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Lesson 15 ‚Äî Final Project, Week 2",
          "content": "# Lesson 15 ‚Äî Final Project, Week 2\n\nCongratulations ‚Äî you've made it to the final week of the course! This week, your goals are:\n1. **Finish and submit your final projects**.\n2. **Record a short project presentation** to share with the rest of the class.\n\nPlease refer to the assignment section below and/or your Week 13 Lesson for Kaggle Project and Week 14 Lesson for your Local Scraping Project to review details and make sure your final projects meet requirements.  Also in the assignment section, you have detailed instructions on how to make and prepare a video of your demonstrated working projects to share with us.   Once you've completed your final projects and video, you'll submit all of these things with your usual Submit Assignment form that you've been using each week.  The form will have additional fields on it to allow you share your Local Scraping Project repo link, your Kaggle link, as well as some information that goes with your video link.  \n\nWhat happens next?\nOnce you've submitted your finals and your video, your reviewer will check your work on your final projects and either declare them needing revisions or being successful, as they have with all your other assignments.  Watch you email for those results.  \n\nTo view your and your classmates' final project presentations, you can visit your class gallery here: [Jamaican Boa Python Essentails Class - Video Demo Gallery](https://airtable.com/appoSRJMlXH9KvE6w/shrqml5HWXby1FzEz/tblTvxLLUlqtgtfbq)\n\nAs always, reach out to a mentor if you need help this week. You're almost there!\n",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c7399"
        }
      ]
    },
    {
      "id": "68f812aec22606ecfa5c739c",
      "lessonNumber": 16,
      "title": "Lesson Name",
      "status": "pending",
      "assignment": {
        "title": "Assignment",
        "objective": "No objective provided",
        "expectedCapabilities": [],
        "instructions": [],
        "tasks": [],
        "submissionInstructions": "Please submit your work by the deadline",
        "checklist": [],
        "checkForUnderstanding": []
      },
      "subsections": [
        {
          "subsectionOrder": 1,
          "title": "Introduction",
          "content": "# Lesson Name",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c739d"
        },
        {
          "subsectionOrder": 2,
          "title": "Lesson Overview",
          "content": "**Learning objective:**\n\nTopics: \n  * TBD\n  * TBD\n  * TBD",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c739e"
        },
        {
          "subsectionOrder": 3,
          "title": "Subsection",
          "content": "### Subsection Video\n\n### Subsection Check for Understanding\n\n---\nThis content was written by Janet Zulu, Reid Russom, and CTD volunteers‚Äîwith special thanks to the brain trust of John McGarvey, Rebecca Callari-Kaczmarczyk, Tom Arns, and Josh Sternfeld. To submit feedback, please fill out the **[CTD Curriculum Feedback Form](https://forms.gle/RZq5mav7wotFxyie6)**.",
          "videoUrl": "",
          "codeExamples": [],
          "externalLinks": [],
          "quizzes": [],
          "_id": "68f812aec22606ecfa5c739f"
        }
      ]
    }
  ]
}